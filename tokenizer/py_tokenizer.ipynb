{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdc5a53-f8a3-41eb-be0d-7a9d5c613f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "\n",
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "pickle_tokenizer = 'tokenizer-01.pkl'\n",
    "\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "GPT4_SPECIAL_TOKENS = {\n",
    "    '<|endoftext|>': 100257,\n",
    "    '<|fim_prefix|>': 100258,\n",
    "    '<|fim_middle|>': 100259,\n",
    "    '<|fim_suffix|>': 100260,\n",
    "    '<|endofprompt|>': 100276\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc75458b-b9ba-4772-8563-d738905b64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT4Tokenizer():\n",
    "    def __init__(self, special_tokens=None, pattern=None):\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)} # idx: bytes\n",
    "        self.merges = {} # (p0, p1): idx\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.special_tokens = GPT4_SPECIAL_TOKENS if special_tokens is None else special_tokens # str: idx\n",
    "        self.inverse_special_tokens = {v: k for k, v in self.special_tokens.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_stats(ids):\n",
    "        \"\"\"returns the number of occurences of each pair of byte codes in our encoded text\"\"\"\n",
    "        counts = {}\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "    \n",
    "        return counts # (p0, p1): number of occurences\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge(ids, pair, idx):\n",
    "        \"\"\"in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\"\"\"\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            # if we are not at the very last position AND the pair matches, replace it\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        \"\"\"adds to the current vocab up to a desired final size, based on merges from a given text\"\"\"\n",
    "        start_vocab = len(self.vocab)\n",
    "        num_merges = vocab_size - start_vocab\n",
    "        \n",
    "        \n",
    "        chunks = re.findall(self.pattern, text)\n",
    "        chunk_ids = []\n",
    "        for chunk in chunks:\n",
    "            chunk_ids.append(self.encode_ordinary(chunk))\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            stats = Counter({})\n",
    "            for chunk in chunk_ids:\n",
    "                stats += Counter(self._get_stats(chunk))\n",
    "            stats = dict(stats)\n",
    "            \n",
    "            if not stats:\n",
    "                print(f\"No more possible chunks, current/max vocab = {len(self.vocab)}\")\n",
    "                break\n",
    "                \n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = start_vocab + i\n",
    "                \n",
    "            for i, chunk in enumerate(chunk_ids):    \n",
    "                chunk_ids[i] = self._merge(chunk, pair, idx)\n",
    "            \n",
    "            self.merges[pair] = idx\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"merging {pair[0], self.decode([pair[0]])} and {pair[1], self.decode([pair[1]])} into a new token {idx, self.decode([idx])}\")\n",
    "        print(f\"Training Complete, chars added:{len(self.vocab)-start_vocab}, new vocab size:{len(self.vocab)}\")\n",
    "\n",
    "    def decode(self, ids, skip_special_tokens=True):\n",
    "        \"\"\"given ids(list of ints), return python string\"\"\"\n",
    "        tokens = []\n",
    "        for idx in ids:\n",
    "            if idx in self.inverse_special_tokens:\n",
    "                if skip_special_tokens:\n",
    "                    continue\n",
    "                else:\n",
    "                    tokens.append(self.inverse_special_tokens[idx].encode('utf-8'))\n",
    "            else:\n",
    "                tokens.append(self.vocab[idx])\n",
    "        tokens = b\"\".join(tokens)\n",
    "        text = tokens.decode('utf-8', errors='replace')\n",
    "        return text\n",
    "\n",
    "    def encode_ordinary(self, text):\n",
    "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = self._get_stats(tokens)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self._merge(tokens, pair, idx)\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text, allowed_special=\"none_raise\"):\n",
    "        \"\"\"return our bytes given a python string\"\"\"\n",
    "        # decode the user desire w.r.t. handling of special tokens\n",
    "        special = None\n",
    "        if allowed_special == \"all\":\n",
    "            special = self.special_tokens\n",
    "        elif allowed_special == \"none\":\n",
    "            special = {}\n",
    "        elif allowed_special == \"none_raise\":\n",
    "            special = {}\n",
    "            assert all(token not in text for token in self.special_tokens)\n",
    "        elif isinstance(allowed_special, set):\n",
    "            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
    "        else:\n",
    "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
    "        if not special:\n",
    "            # shortcut: if no special tokens, just use the ordinary encoding\n",
    "            return self.encode_ordinary(text)\n",
    "            \n",
    "        # we handle special tokens by splitting the text\n",
    "        # based on the occurrence of any exact match with any of the special tokens\n",
    "        # we can use re.split for this. note that surrounding the pattern with ()\n",
    "        # makes it into a capturing group, so the special tokens will be included\n",
    "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        special_chunks = re.split(special_pattern, text)\n",
    "        # now all the special characters are separated from the rest of the text\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for part in special_chunks:\n",
    "            if part in special:\n",
    "                # this is a special token, encode it separately as a special case\n",
    "                ids.append(special[part])\n",
    "            else:\n",
    "                # this is an ordinary sequence, encode it normally\n",
    "                ids.extend(self.encode_ordinary(part))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11406ee6-1d70-42c9-8140-9df83e89c666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print('loading params...')\\nwith open(pickle_tokenizer, 'rb') as f:\\n    t = pickle.load(f)\\nprint('loaded succesfuly')\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = GPT4Tokenizer()\n",
    "\n",
    "\"\"\"print('loading params...')\n",
    "with open(pickle_tokenizer, 'rb') as f:\n",
    "    t = pickle.load(f)\n",
    "print('loaded succesfuly')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9fab2d8-ff19-453c-93b7-b2e07dd908f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (32, ' ') and (116, 't') into a new token (256, ' t')\n",
      "merging (104, 'h') and (101, 'e') into a new token (257, 'he')\n",
      "merging (32, ' ') and (97, 'a') into a new token (258, ' a')\n",
      "merging (256, ' t') and (257, 'he') into a new token (259, ' the')\n",
      "merging (114, 'r') and (101, 'e') into a new token (260, 're')\n",
      "merging (105, 'i') and (110, 'n') into a new token (261, 'in')\n",
      "merging (32, ' ') and (115, 's') into a new token (262, ' s')\n",
      "merging (32, ' ') and (119, 'w') into a new token (263, ' w')\n",
      "merging (110, 'n') and (100, 'd') into a new token (264, 'nd')\n",
      "merging (32, ' ') and (111, 'o') into a new token (265, ' o')\n",
      "merging (101, 'e') and (100, 'd') into a new token (266, 'ed')\n",
      "merging (32, ' ') and (98, 'b') into a new token (267, ' b')\n",
      "merging (111, 'o') and (117, 'u') into a new token (268, 'ou')\n",
      "merging (10, '\\n') and (10, '\\n') into a new token (269, '\\n\\n')\n",
      "merging (104, 'h') and (97, 'a') into a new token (270, 'ha')\n",
      "merging (258, ' a') and (264, 'nd') into a new token (271, ' and')\n",
      "merging (105, 'i') and (116, 't') into a new token (272, 'it')\n",
      "merging (32, ' ') and (99, 'c') into a new token (273, ' c')\n",
      "merging (101, 'e') and (114, 'r') into a new token (274, 'er')\n",
      "merging (256, ' t') and (111, 'o') into a new token (275, ' to')\n",
      "merging (32, ' ') and (102, 'f') into a new token (276, ' f')\n",
      "merging (111, 'o') and (114, 'r') into a new token (277, 'or')\n",
      "merging (32, ' ') and (109, 'm') into a new token (278, ' m')\n",
      "merging (108, 'l') and (101, 'e') into a new token (279, 'le')\n",
      "merging (105, 'i') and (115, 's') into a new token (280, 'is')\n",
      "merging (97, 'a') and (114, 'r') into a new token (281, 'ar')\n",
      "merging (108, 'l') and (108, 'l') into a new token (282, 'll')\n",
      "merging (101, 'e') and (110, 'n') into a new token (283, 'en')\n",
      "merging (111, 'o') and (110, 'n') into a new token (284, 'on')\n",
      "merging (32, ' ') and (32, ' ') into a new token (285, '  ')\n",
      "merging (261, 'in') and (103, 'g') into a new token (286, 'ing')\n",
      "merging (265, ' o') and (102, 'f') into a new token (287, ' of')\n",
      "merging (32, ' ') and (112, 'p') into a new token (288, ' p')\n",
      "merging (32, ' ') and (100, 'd') into a new token (289, ' d')\n",
      "merging (97, 'a') and (110, 'n') into a new token (290, 'an')\n",
      "merging (97, 'a') and (115, 's') into a new token (291, 'as')\n",
      "merging (32, ' ') and (261, 'in') into a new token (292, ' in')\n",
      "merging (101, 'e') and (115, 's') into a new token (293, 'es')\n",
      "merging (32, ' ') and (103, 'g') into a new token (294, ' g')\n",
      "merging (32, ' ') and (257, 'he') into a new token (295, ' he')\n",
      "merging (111, 'o') and (116, 't') into a new token (296, 'ot')\n",
      "merging (97, 'a') and (116, 't') into a new token (297, 'at')\n",
      "merging (32, ' ') and (110, 'n') into a new token (298, ' n')\n",
      "merging (32, ' ') and (108, 'l') into a new token (299, ' l')\n",
      "Training Complete, chars added:44, new vocab size:300\n"
     ]
    }
   ],
   "source": [
    "t.train(text, 300, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1731b9ce-4122-4e96-8ad9-4e777bd38242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode(t.encode(\"hello world!!!? (ì•ˆë…•í•˜<|fim_middle|>ì„¸ìš”!) lol123 ðŸ˜‰<|endoftext|>\", allowed_special=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e8cd66-4cd3-462e-aee6-c2608d32d770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb85d81-6501-42f1-ab3a-67f995c5063a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953831f-6daa-478d-8e6a-85f705942aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with open(pickle_tokenizer, 'wb') as f:\n",
    "    pickle.dump(t, f)\n",
    "print(\"tokenizer saved\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
