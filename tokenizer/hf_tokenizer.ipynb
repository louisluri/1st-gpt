{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989689b7-e793-4217-9a80-699f79ca32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers, Regex\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer_model = \"tokenizer-01.json\"\n",
    "\n",
    "# 1. The Regex Pattern\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4ce123-95dd-4314-8a73-c9761ad16e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. The core model (BPE)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "# 3. Normalization (recommended)\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "\n",
    "# 4. --- The Pre-tokenizer Pipeline ---\n",
    "# This is the key part. We build a Sequence.\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    # Step 1: Split the text using the regex\n",
    "    pre_tokenizers.Split(\n",
    "        pattern=Regex(GPT4_SPLIT_PATTERN),\n",
    "        behavior=\"isolated\"\n",
    "    ),\n",
    "    # Step 2: Convert the resulting chunks to bytes\n",
    "    pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)\n",
    "])\n",
    "\n",
    "# 5. Decoder (to read it back)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# 6. The Trainer (now uses the ByteLevel alphabet)\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=30000,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<|end_of_text|>\", \"<|user|>\", \"<|assistant|>\", \"<|im_end|>\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b39a7b-f8ee-49f2-bbcd-0cb9b0034c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072ddc8d8f964c6386dab8b16bf9ce2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 8013769\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8013769"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved dataset instantly\n",
    "dataset = load_dataset(\"openwebtext\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "train_data = dataset['train']\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2d2132-5925-46ad-a94f-ef8795d91cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAYON_NUM_THREADS is set to: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the number of threads for the Rust backend\n",
    "os.environ['RAYON_NUM_THREADS'] = '2'\n",
    "\n",
    "# Optional: verify it was set\n",
    "print(f\"RAYON_NUM_THREADS is set to: {os.environ.get('RAYON_NUM_THREADS')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6a1b8e-7d96-4fd8-a43d-fba98c5535e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_iterator(data_split, batch_size=1000):\n",
    "    # Only keep the text column to avoid decoding the rest of the columns unnecessarily\n",
    "    tok_dataset = data_split.select_columns(\"text\")\n",
    "    for batch in tok_dataset.iter(batch_size):\n",
    "        yield batch[\"text\"]\n",
    "\n",
    "tokenizer.train_from_iterator(batch_iterator(train_data), trainer=trainer, length=len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff610cb-9864-4a6c-a54a-ee915810e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a post processor\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"$A <|end_of_text|>\",\n",
    "    pair=\"$A <|end_of_text|> $B:1 <|end_of_text|>:1\",\n",
    "    special_tokens=[\n",
    "        (\"<|end_of_text|>\", tokenizer.token_to_id(\"<|end_of_text|>\"))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01a68b9-d73a-4fa9-86e7-5c637ac2c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "# tokenizer = Tokenizer.from_file(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e385d040-d5c6-44bd-bc95-2131896d82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    #text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b377e32-8369-46c3-8612-4cc02c25232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=251, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "# encode\n",
    "output = tokenizer.encode(train_data[1679]['text'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e8234e5-abee-42d5-bfdb-ae24b84e5ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Former', 'ĠRo', 'oster', 'ĠTe', 'eth', 'Ġand', 'ĠAchie', 'vement', 'ĠHunter', 'Ġcontributor', ',', 'ĠDavid', 'Ġ\"', 'Kn', 'uck', 'les', 'ĠDawson', '\"', 'ĠDre', 'ger', 'Ġhas', 'Ġbeen', 'Ġfound', 'Ġdead', 'Ġat', 'ĠAmb', 'les', 'ide', 'ĠPark', 'Ġin', 'ĠWest', 'ĠVancouver', '.', 'ĠHis', 'Ġbody', 'Ġwas', 'Ġnoticed', 'Ġyesterday', ',', 'Ġaccording', 'Ġto', 'Ġa', 'Ġpost', 'Ġby', 'ĠDre', 'ger', \"'s\", 'Ġfamily', 'Ġon', 'Ġthe', 'ĠFacebook', 'Ġpage', 'Ġthat', 'Ġwas', 'Ġsetup', 'Ġto', 'Ġhelp', 'Ġfind', 'Ġhim', '.', 'ĠDavid', \"'s\", 'Ġsister', 'ĠDaniel', 'le', 'Ġwrote', 'Ġon', 'Ġthe', 'Ġpage', ':ĊĊ', 'We', 'Ġare', 'Ġthankful', 'Ġthat', 'Ġthe', 'Ġprayers', 'Ġfor', 'Ġdiscovery', 'Ġmade', 'Ġby', 'Ġeveryone', 'Ġduring', 'Ġthis', 'Ġtime', 'Ġhave', 'Ġbeen', 'Ġanswered', '.', 'ĠIt', 'Ġis', 'Ġwith', 'Ġdeep', 'Ġsadness', 'Ġthat', 'Ġwe', 'Ġmust', 'Ġlet', 'Ġyou', 'Ġall', 'Ġknow', 'Ġthat', 'ĠDavid', \"'s\", 'Ġbody', 'Ġwas', 'Ġdiscovered', 'Ġon', 'ĠJune', 'Ġ', '20', 'Ġnear', 'ĠAmb', 'les', 'ide', 'ĠPark', '.', 'ĠWe', 'Ġwould', 'Ġlike', 'Ġto', 'Ġtake', 'Ġthis', 'Ġopportunity', 'Ġto', 'Ġthank', 'Ġeveryone', 'Ġfor', 'Ġtheir', 'Ġprayers', ',', 'Ġsupport', 'Ġand', 'Ġcompassion', '.', 'ĠDavid', 'Ġwas', 'Ġan', 'Ġexceptional', ',', 'Ġbright', ',', 'Ġcaring', 'Ġand', 'Ġloving', 'Ġyoung', 'Ġman', '.', 'ĠWe', 'Ġconsider', 'Ġourselves', 'Ġblessed', 'Ġto', 'Ġhave', 'Ġhad', 'Ġhim', 'Ġin', 'Ġour', 'Ġlives', ',', 'Ġand', 'Ġare', 'Ġcomfort', 'ed', 'Ġknowing', 'Ġthat', 'Ġhe', 'Ġis', 'Ġnow', 'Ġembraced', 'Ġin', 'Ġthe', 'Ġarms', 'Ġof', 'Ġour', 'ĠLord', '.ĊĊ', 'D', 're', 'ger', ',', 'Ġ', '28', ',', 'Ġwas', 'Ġreported', 'Ġmissing', 'Ġin', 'Ġlate', 'ĠMay', 'Ġafter', 'Ġlast', 'Ġbeing', 'Ġseen', 'Ġon', 'ĠMay', 'Ġ', '26', 'Ġin', 'Ġhis', 'Ġhometown', 'Ġof', 'ĠVancouver', '.', 'ĠHis', 'Ġbike', 'Ġand', 'Ġhelmet', 'Ġwere', 'Ġfound', 'Ġthe', 'Ġday', 'Ġafter', 'Ġin', 'ĠStanley', 'ĠPark', ',', 'Ġand', 'Ġhe', 'Ġleft', 'Ġhis', 'Ġwallet', ',', 'Ġcomputer', 'Ġand', 'Ġphone', 'Ġat', 'Ġhome', ',', 'Ġas', 'Ġwell', 'Ġas', 'Ġclosing', 'Ġhis', 'ĠTwitter', 'Ġand', 'ĠXbox', 'ĠLive', 'Ġaccounts', '.', 'ĠThe', 'Ġcause', 'Ġof', 'Ġhis', 'Ġdeath', 'Ġhasn', \"'t\", 'Ġbeen', 'Ġreleased', 'Ġyet', '.', '<|end_of_text|>']\n",
      "[11093, 5559, 6951, 1823, 3123, 297, 29002, 22549, 12149, 15787, 15, 2894, 492, 29101, 1394, 831, 28330, 5, 22104, 1403, 457, 553, 1029, 2582, 375, 12779, 831, 494, 3098, 290, 2402, 8691, 17, 2381, 2095, 364, 6802, 6192, 15, 1620, 287, 261, 1255, 422, 22104, 1403, 430, 1550, 323, 265, 2647, 2865, 322, 364, 10007, 287, 953, 1096, 698, 17, 2894, 430, 6737, 7132, 300, 2170, 323, 265, 2865, 902, 1022, 388, 22276, 322, 265, 15707, 334, 8360, 906, 422, 2335, 1109, 436, 637, 416, 553, 10011, 17, 639, 326, 357, 2366, 22880, 322, 355, 1492, 1318, 365, 490, 765, 322, 2894, 430, 2095, 364, 4635, 323, 2558, 224, 454, 2013, 12779, 831, 494, 3098, 17, 766, 538, 567, 287, 979, 436, 3219, 287, 5533, 2335, 334, 501, 15707, 15, 1057, 297, 13675, 17, 2894, 364, 285, 14590, 15, 6312, 15, 17040, 297, 14207, 1705, 574, 17, 766, 2490, 6017, 19976, 287, 416, 533, 698, 290, 664, 2811, 15, 297, 388, 4498, 280, 6775, 322, 346, 326, 752, 14936, 290, 265, 5294, 291, 664, 6748, 313, 39, 264, 1403, 15, 224, 2207, 15, 364, 1827, 4796, 290, 2391, 1670, 688, 846, 808, 1681, 323, 1670, 224, 2255, 290, 465, 16765, 291, 8691, 17, 2381, 6398, 297, 15738, 526, 1029, 265, 1097, 688, 290, 11358, 3098, 15, 297, 346, 1347, 465, 13108, 15, 3629, 297, 2783, 375, 1260, 15, 358, 865, 358, 8723, 465, 2441, 297, 8421, 7839, 5128, 17, 398, 2992, 291, 465, 1879, 4793, 699, 553, 2242, 1799, 17, 0]\n"
     ]
    }
   ],
   "source": [
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f2993e8-2935-45cf-8c7b-2d7b14aeeb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former Rooster Teeth and Achievement Hunter contributor, David \"Knuckles Dawson\" Dreger has been found dead at Ambleside Park in West Vancouver. His body was noticed yesterday, according to a post by Dreger's family on the Facebook page that was setup to help find him. David's sister Danielle wrote on the page:\n",
      "\n",
      "We are thankful that the prayers for discovery made by everyone during this time have been answered. It is with deep sadness that we must let you all know that David's body was discovered on June 20 near Ambleside Park. We would like to take this opportunity to thank everyone for their prayers, support and compassion. David was an exceptional, bright, caring and loving young man. We consider ourselves blessed to have had him in our lives, and are comforted knowing that he is now embraced in the arms of our Lord.\n",
      "\n",
      "Dreger, 28, was reported missing in late May after last being seen on May 26 in his hometown of Vancouver. His bike and helmet were found the day after in Stanley Park, and he left his wallet, computer and phone at home, as well as closing his Twitter and Xbox Live accounts. The cause of his death hasn't been released yet.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a781d0d-0493-4443-8301-710d202e6ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a tokens id\n",
    "tokenizer.token_to_id(\"<|end_of_text|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec94f389-f6c7-4142-90df-7419ae0b8e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4aeea1-ea82-417c-9648-ac336d99c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "tokenizer.save(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93484395-2b1f-4264-814d-5279943ea28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
