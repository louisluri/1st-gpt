{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a384fba-6b54-4e42-a296-28e64d437238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tokenizer_file = '/home/user/Desktop/Python_Scripts/gpt_course/gpt_dt/tokenizer/tokenizer-01.json'\n",
    "HF_DATASET = \"openwebtext\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb2c6b3-98fe-4134-8b43-d3f890bf407b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9841c0883f1446aa0418886b49b98ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ba009e6ed3431a8947e2c30e49a388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'text', 'date', 'metadata'],\n",
       "    num_rows: 6315233\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"open-web-math/open-web-math\", split=\"train\", num_proc=12)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1665d82d-da6f-4de9-aab9-cadf15aa0d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['url', 'text', 'date', 'metadata'],\n",
       "        num_rows: 5683709\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['url', 'text', 'date', 'metadata'],\n",
       "        num_rows: 631524\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_value = 42\n",
    "\n",
    "dataset = dataset.train_test_split(\n",
    "    test_size=0.10,\n",
    "    seed=seed_value\n",
    ")\n",
    "dataset.flatten()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c160bb-2c62-4a28-b083-bb063913f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer.encode_batch(examples[\"text\"])\n",
    "    return {\"ids\": [o.ids for o in outputs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dbb131-0e4a-40b3-9db7-a926aba9208f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1489de8e5af41d8a06284a90d62b364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train split (num_proc=12):   0%|          | 0/5683709 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dset in dataset.items():\n",
    "    \n",
    "    tokenized_dset = dset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=12,\n",
    "        batch_size=1000,\n",
    "        keep_in_memory=False, \n",
    "        load_from_cache_file=False,\n",
    "        remove_columns=dset.column_names, \n",
    "        desc=f\"Tokenizing {split} split\"\n",
    "    )\n",
    "\n",
    "    filename = f\"{split}_data.bin\"\n",
    "    \n",
    "    print(f\"Calculating total length for {filename}...\")\n",
    "    total_len = sum(len(x) for x in tokenized_dset[\"ids\"])\n",
    "\n",
    "    # Using uint16 for efficiency (works for vocab < 65,535)\n",
    "    arr = np.memmap(filename, dtype=np.uint16, mode='w+', shape=(total_len,))\n",
    "    \n",
    "    idx = 0\n",
    "    for example in tqdm(tokenized_dset[\"ids\"], desc=f\"Writing {filename}\"):\n",
    "        arr[idx : idx + len(example)] = example\n",
    "        idx += len(example)\n",
    "        \n",
    "    arr.flush()\n",
    "    print(f\"Finished {filename}. Total tokens: {total_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626deb6-9597-40ff-8e4d-2de524f39687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
