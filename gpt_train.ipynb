{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c902b045-7f16-4da0-8502-b6d4b7496358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, Sampler\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from gptmodel import GPTLanguageModel, load_checkpoint, save_checkpoint\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "load = True  # load model from checkpoint?\n",
    "save = True  # save model and data?\n",
    "load_model_number = \"01\"\n",
    "save_model_number = \"01\"\n",
    "resume_step = \"43100\"\n",
    "load_path = f'checkpoints/model{load_model_number}'\n",
    "save_path = f'checkpoints/model{save_model_number}'\n",
    "LOG_FILE = f'train_data/model{save_model_number}_data.csv'\n",
    "\n",
    "start_optim_step = 0  # default start step\n",
    "optim_step = start_optim_step\n",
    "start_epoch = 0\n",
    "start_block = 0\n",
    "block = start_block\n",
    "max_epochs = 1\n",
    "\n",
    "train_dataset_path = \"openwebtext1/train_data.bin\"\n",
    "test_dataset_path = \"openwebtext1/test_data.bin\"\n",
    "train_map_paths = [\"openwebtext1/train_shuffle_map0-128block.bin\"]\n",
    "\n",
    "# evrything here can be changed each training session to optimize learning\n",
    "minibatch_size = 16   # effective batch size is minibatch_size * accumulation_steps\n",
    "accumulation_steps = 64\n",
    "block_size = 128\n",
    "learning_rate = 1e-4\n",
    "save_iters = 100\n",
    "eval_iters = 64\n",
    "\n",
    "# everything below here NEEDS to be identical to load an extistng model\n",
    "vocab_size = 30000\n",
    "n_embed = 1536\n",
    "n_head = 24\n",
    "n_layer = 32\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3881e15d-f540-48f3-838e-5b2a9b35c1bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Datasets and dataloading\n",
    "class FastSeekSampler(Sampler):\n",
    "    def __init__(self, map_path, start_pos):\n",
    "        self.indices_np = np.memmap(map_path, dtype=np.int64, mode='r')\n",
    "        self.start_block = start_pos\n",
    "        self.indices_torch = torch.from_numpy(self.indices_np)\n",
    "\n",
    "    def __iter__(self):\n",
    "        remaining = self.indices_torch[self.start_block:]\n",
    "        \n",
    "        yield from remaining.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices_torch) - self.start_block\n",
    "        \n",
    "class BinDataset(Dataset):\n",
    "    def __init__(self, data_path, block_size):\n",
    "        self.block_size = block_size\n",
    "        # creates a pointer to the file without loading it.\n",
    "        self.data = np.memmap(data_path, dtype=np.uint16, mode='r')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        i = i * block_size\n",
    "        chunk = self.data[i : i + self.block_size + 1]\n",
    "        \n",
    "        # from_numpy is zero-copy It points to the memmap memory.\n",
    "        t = torch.from_numpy(chunk.astype(np.int64))\n",
    "        \n",
    "        x = t[:-1] # Input\n",
    "        y = t[1:]  # Target (shifted right)\n",
    "        return x, y\n",
    "    \n",
    "def get_dataloaders(data_path, batch_size, block_size, map_path, start_pos=0):\n",
    "    dataset=BinDataset(data_path, block_size)\n",
    "\n",
    "    sampler = FastSeekSampler(map_path, start_pos)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=2,      # More than 4 often causes overhead\n",
    "        pin_memory=True,    # Fast transfer to GPU\n",
    "        drop_last=True      # Avoids partial batches that break FSDP shapes\n",
    "    )\n",
    "    \n",
    "    return dataset, loader, sampler  \n",
    "\n",
    "val_data = BinDataset(test_dataset_path, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c41ebf-ab1f-4360-a90a-378cd4477100",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE ESTIMATE LOSS FUNCTION\n",
    "@torch.no_grad()\n",
    "def estimate_loss(step):\n",
    "    out = {}\n",
    "    eval_gen = torch.Generator(device='cpu').manual_seed(step+123)\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        data = train_data.data if split == 'train' else val_data.data\n",
    "        out[split] = 0\n",
    "        for k in range(eval_iters):\n",
    "            ix = torch.randint(len(data), (minibatch_size,), generator=eval_gen)\n",
    "            x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "            y = torch.stack([torch.from_numpy((data[i+1:i+block_size+1]).astype(np.int64)) for i in ix])\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits, loss = m(x, y)\n",
    "            out[split] += loss.item()\n",
    "        out[split] /= eval_iters\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fbbb4d-aacc-4f9b-88d0-a783d71864de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from checkpoints/model01/model01_step27800.pt\n",
      "Loaded succesfuly from (step: 27800, epoch: 0, block: 28467200)\n",
      "Total parameters: 998,554,416\n",
      "Learning rate: 6.19372093e-05\n",
      "Scheduler step: 27800\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE AND LOAD MODEL AND OPTIMIZER\n",
    "\n",
    "# define the model\n",
    "torch.manual_seed(230) \n",
    "torch.cuda.manual_seed(230)\n",
    "m = GPTLanguageModel(vocab_size, n_embed, n_head, n_layer, dropout).to(device)\n",
    "\n",
    "# define a PyTorch optimizer\n",
    "torch.manual_seed(230)\n",
    "#optim = bnb.optim.PagedAdamW8bit(m.parameters(), lr=learning_rate)\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "    \n",
    "scheduler = CosineAnnealingLR(optim, T_max=62500, eta_min=8e-6)\n",
    "\n",
    "if load:\n",
    "    # check for an existing checkpoint and load if necessary\n",
    "    path = f\"{load_path}/model{load_model_number}_step{resume_step}.pt\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Loading checkpoint from {path}\")\n",
    "        start_optim_step, start_epoch, start_block = load_checkpoint(m, optim, scheduler, path)\n",
    "        optim_step = start_optim_step\n",
    "        block = start_block\n",
    "        print(f\"Loaded succesfuly from (step: {optim_step}, epoch: {start_epoch}, block: {start_block})\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {path}.\")\n",
    "        print(f\"New model will be training from (step: {optim_step}, epoch: {start_epoch}, block: {start_block})\")\n",
    "m = torch.compile(m)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in m.parameters()):,}\")\n",
    "print(f\"Learning rate: {optim.param_groups[0]['lr']:.8e}\")\n",
    "print(f\"Scheduler step: {scheduler.last_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c8bfb3-c345-4806-84fb-792608455ca6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5099/2770793083.py:6: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647352509/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  self.indices_torch = torch.from_numpy(self.indices_np)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:45:02\n",
      "(step: 27800, epoch: 0, block: 28467200), Train Loss: 3.3154, Val Loss: 3.3419\n",
      "12:45:24\n",
      "27801,3.3708\n",
      "27802,3.3587\n",
      "27803,3.4006\n",
      "27804,3.3648\n",
      "27805,3.3957\n",
      "27806,3.3732\n",
      "27807,3.4041\n",
      "27808,3.3690\n",
      "27809,3.3883\n",
      "27810,3.4026\n",
      "27811,3.3611\n",
      "27812,3.3974\n",
      "27813,3.3607\n",
      "27814,3.3957\n",
      "27815,3.3985\n",
      "27816,3.3824\n",
      "27817,3.3739\n",
      "27818,3.3689\n",
      "27819,3.3699\n",
      "27820,3.4013\n",
      "27821,3.3788\n",
      "27822,3.3994\n",
      "27823,3.4021\n",
      "27824,3.3538\n",
      "27825,3.3894\n",
      "27826,3.4032\n",
      "27827,3.3818\n",
      "27828,3.4052\n",
      "27829,3.3684\n",
      "27830,3.3675\n",
      "27831,3.4079\n",
      "27832,3.3880\n",
      "27833,3.3725\n",
      "27834,3.3482\n",
      "27835,3.3907\n",
      "27836,3.3952\n",
      "27837,3.3966\n",
      "27838,3.3860\n",
      "27839,3.3744\n",
      "27840,3.4054\n",
      "27841,3.3932\n",
      "27842,3.3893\n",
      "27843,3.3874\n",
      "27844,3.3800\n",
      "27845,3.3904\n",
      "27846,3.3864\n",
      "27847,3.3671\n",
      "27848,3.3775\n",
      "27849,3.3816\n",
      "27850,3.3731\n",
      "27851,3.3505\n",
      "27852,3.3857\n",
      "27853,3.4027\n",
      "27854,3.3721\n",
      "27855,3.3880\n",
      "27856,3.4018\n",
      "27857,3.4063\n",
      "27858,3.3677\n",
      "27859,3.3727\n",
      "27860,3.3855\n",
      "27861,3.3697\n",
      "27862,3.3983\n",
      "27863,3.3746\n",
      "27864,3.3668\n",
      "27865,3.3830\n",
      "27866,3.3687\n",
      "27867,3.3916\n",
      "27868,3.3918\n",
      "27869,3.3848\n",
      "27870,3.3960\n",
      "27871,3.3840\n",
      "27872,3.3724\n",
      "27873,3.3992\n",
      "27874,3.3780\n",
      "27875,3.4016\n",
      "27876,3.3965\n",
      "27877,3.3969\n",
      "27878,3.3941\n",
      "27879,3.3871\n",
      "27880,3.3666\n",
      "27881,3.4025\n",
      "27882,3.3847\n",
      "27883,3.3981\n",
      "27884,3.3666\n",
      "27885,3.3973\n",
      "27886,3.3956\n",
      "27887,3.3793\n",
      "27888,3.3860\n",
      "27889,3.3879\n",
      "27890,3.3618\n",
      "27891,3.3788\n",
      "27892,3.3900\n",
      "27893,3.3913\n",
      "27894,3.3761\n",
      "27895,3.3680\n",
      "27896,3.3971\n",
      "27897,3.4068\n",
      "27898,3.3936\n",
      "27899,3.3676\n",
      "27900,3.3765\n",
      "(step: 27900, epoch: 0, block: 28569600), Train Loss: 3.3239, Val Loss: 3.3054\n",
      "Learning rate: 6.17093569e-05\n",
      "Scheduler step: 27900\n",
      "Checkpoint (step: 27900, epoch: 0, block: 28569600) Saved\n",
      "Step Documented\n",
      "13:12:08\n",
      "27901,3.4007\n",
      "27902,3.3644\n",
      "27903,3.3686\n",
      "27904,3.3698\n",
      "27905,3.3794\n",
      "27906,3.3602\n",
      "27907,3.4159\n",
      "27908,3.4093\n",
      "27909,3.3974\n",
      "27910,3.4037\n",
      "27911,3.3594\n",
      "27912,3.3931\n",
      "27913,3.3566\n",
      "27914,3.3840\n",
      "27915,3.3886\n",
      "27916,3.3735\n",
      "27917,3.3891\n",
      "27918,3.3938\n",
      "27919,3.3837\n",
      "27920,3.3982\n",
      "27921,3.3803\n",
      "27922,3.3983\n",
      "27923,3.3997\n",
      "27924,3.4083\n",
      "27925,3.3925\n",
      "27926,3.4055\n",
      "27927,3.3456\n",
      "27928,3.4099\n",
      "27929,3.4051\n",
      "27930,3.3674\n",
      "27931,3.3803\n",
      "27932,3.4094\n",
      "27933,3.3670\n",
      "27934,3.4048\n",
      "27935,3.4135\n",
      "27936,3.3642\n",
      "27937,3.3755\n",
      "27938,3.3683\n",
      "27939,3.3748\n",
      "27940,3.3823\n",
      "27941,3.3948\n",
      "27942,3.3865\n",
      "27943,3.3818\n",
      "27944,3.3984\n",
      "27945,3.3752\n",
      "27946,3.3956\n",
      "27947,3.4021\n",
      "27948,3.3929\n",
      "27949,3.3482\n",
      "27950,3.4022\n",
      "27951,3.3996\n",
      "27952,3.3917\n",
      "27953,3.3853\n",
      "27954,3.3930\n",
      "27955,3.3977\n",
      "27956,3.3796\n",
      "27957,3.3685\n",
      "27958,3.3911\n",
      "27959,3.3699\n",
      "27960,3.3767\n",
      "27961,3.3823\n",
      "27962,3.3552\n",
      "27963,3.3820\n",
      "27964,3.3687\n",
      "27965,3.4102\n",
      "27966,3.3749\n",
      "27967,3.3893\n",
      "27968,3.3772\n",
      "27969,3.3941\n",
      "27970,3.3451\n",
      "27971,3.3673\n",
      "27972,3.4085\n",
      "27973,3.4110\n",
      "27974,3.3830\n",
      "27975,3.3687\n",
      "27976,3.3998\n",
      "27977,3.3633\n",
      "27978,3.3758\n",
      "27979,3.3858\n",
      "27980,3.3892\n",
      "27981,3.3852\n",
      "27982,3.3784\n",
      "27983,3.3880\n",
      "27984,3.3758\n",
      "27985,3.3793\n",
      "27986,3.3682\n",
      "27987,3.3746\n",
      "27988,3.3976\n",
      "27989,3.4013\n",
      "27990,3.4111\n",
      "27991,3.3647\n",
      "27992,3.3772\n",
      "27993,3.3758\n",
      "27994,3.3769\n",
      "27995,3.3835\n",
      "27996,3.3877\n",
      "27997,3.3935\n",
      "27998,3.4053\n",
      "27999,3.3936\n",
      "28000,3.3875\n",
      "(step: 28000, epoch: 0, block: 28672000), Train Loss: 3.3105, Val Loss: 3.3234\n",
      "Learning rate: 6.14813096e-05\n",
      "Scheduler step: 28000\n",
      "Checkpoint (step: 28000, epoch: 0, block: 28672000) Saved\n",
      "Step Documented\n",
      "13:38:30\n",
      "28001,3.4109\n",
      "28002,3.3854\n",
      "28003,3.3696\n",
      "28004,3.4083\n",
      "28005,3.3584\n",
      "28006,3.3807\n",
      "28007,3.3655\n",
      "28008,3.3911\n",
      "28009,3.4011\n",
      "28010,3.3695\n",
      "28011,3.3913\n",
      "28012,3.3734\n",
      "28013,3.3531\n",
      "28014,3.3662\n",
      "28015,3.3852\n",
      "28016,3.3832\n",
      "28017,3.4176\n",
      "28018,3.4029\n",
      "28019,3.3894\n",
      "28020,3.3797\n",
      "28021,3.3797\n",
      "28022,3.3840\n",
      "28023,3.3821\n",
      "28024,3.3908\n",
      "28025,3.3829\n",
      "28026,3.3781\n",
      "28027,3.3807\n",
      "28028,3.3848\n",
      "28029,3.3653\n",
      "28030,3.3829\n",
      "28031,3.3915\n",
      "28032,3.3858\n",
      "28033,3.3952\n",
      "28034,3.3935\n",
      "28035,3.4072\n",
      "28036,3.4059\n",
      "28037,3.3584\n",
      "28038,3.3590\n",
      "28039,3.3848\n",
      "28040,3.4075\n",
      "28041,3.4102\n",
      "28042,3.3913\n",
      "28043,3.3847\n",
      "28044,3.3759\n",
      "28045,3.4101\n",
      "28046,3.3920\n",
      "28047,3.3877\n",
      "28048,3.3782\n",
      "28049,3.3860\n",
      "28050,3.3962\n",
      "28051,3.3634\n",
      "28052,3.3781\n",
      "28053,3.4047\n",
      "28054,3.3594\n",
      "28055,3.3853\n",
      "28056,3.3725\n",
      "28057,3.3927\n",
      "28058,3.3893\n",
      "28059,3.3922\n",
      "28060,3.3866\n",
      "28061,3.3678\n",
      "28062,3.3739\n",
      "28063,3.3977\n",
      "28064,3.4150\n",
      "28065,3.3730\n",
      "28066,3.3957\n",
      "28067,3.3713\n",
      "28068,3.3841\n",
      "28069,3.3777\n",
      "28070,3.3815\n",
      "28071,3.3809\n",
      "28072,3.3831\n",
      "28073,3.3806\n",
      "28074,3.3698\n",
      "28075,3.3779\n",
      "28076,3.4109\n",
      "28077,3.3979\n",
      "28078,3.3711\n",
      "28079,3.3889\n",
      "28080,3.3756\n",
      "28081,3.3946\n",
      "28082,3.3796\n",
      "28083,3.3836\n",
      "28084,3.3727\n",
      "28085,3.3894\n",
      "28086,3.4038\n",
      "28087,3.4144\n",
      "28088,3.3929\n",
      "28089,3.3830\n",
      "28090,3.4118\n",
      "28091,3.4045\n",
      "28092,3.3585\n",
      "28093,3.3674\n",
      "28094,3.4197\n",
      "28095,3.3792\n",
      "28096,3.3799\n",
      "28097,3.3564\n",
      "28098,3.3833\n",
      "28099,3.3983\n",
      "28100,3.3819\n",
      "(step: 28100, epoch: 0, block: 28774400), Train Loss: 3.2831, Val Loss: 3.3138\n",
      "Learning rate: 6.12530733e-05\n",
      "Scheduler step: 28100\n",
      "Checkpoint (step: 28100, epoch: 0, block: 28774400) Saved\n",
      "Step Documented\n",
      "14:04:52\n",
      "28101,3.3661\n",
      "28102,3.3934\n",
      "28103,3.3736\n",
      "28104,3.3830\n",
      "28105,3.3868\n",
      "28106,3.3564\n",
      "28107,3.3760\n",
      "28108,3.4047\n",
      "28109,3.3716\n",
      "28110,3.3851\n",
      "28111,3.4014\n",
      "28112,3.3931\n",
      "28113,3.3932\n",
      "28114,3.3953\n",
      "28115,3.3802\n",
      "28116,3.3821\n",
      "28117,3.3940\n",
      "28118,3.4141\n",
      "28119,3.3987\n",
      "28120,3.4119\n",
      "28121,3.4086\n",
      "28122,3.3943\n",
      "28123,3.4067\n",
      "28124,3.3826\n",
      "28125,3.3853\n",
      "28126,3.3886\n",
      "28127,3.3648\n",
      "28128,3.3677\n",
      "28129,3.3735\n",
      "28130,3.3791\n",
      "28131,3.3949\n",
      "28132,3.3710\n",
      "28133,3.4121\n",
      "28134,3.3757\n",
      "28135,3.3653\n",
      "28136,3.3629\n",
      "28137,3.3706\n",
      "28138,3.4010\n",
      "28139,3.3848\n",
      "28140,3.3942\n",
      "28141,3.4024\n",
      "28142,3.3422\n",
      "28143,3.4007\n",
      "28144,3.3872\n",
      "28145,3.3886\n",
      "28146,3.3714\n",
      "28147,3.3961\n",
      "28148,3.3657\n",
      "28149,3.3704\n",
      "28150,3.3909\n",
      "28151,3.4154\n",
      "28152,3.3941\n",
      "28153,3.4110\n",
      "28154,3.3920\n",
      "28155,3.3470\n",
      "28156,3.4149\n",
      "28157,3.3722\n",
      "28158,3.4044\n",
      "28159,3.3780\n",
      "28160,3.4028\n",
      "28161,3.3772\n",
      "28162,3.4301\n",
      "28163,3.3840\n",
      "28164,3.3851\n",
      "28165,3.3860\n",
      "28166,3.3872\n",
      "28167,3.3931\n",
      "28168,3.3888\n",
      "28169,3.3740\n",
      "28170,3.3840\n",
      "28171,3.3794\n",
      "28172,3.4127\n",
      "28173,3.4105\n",
      "28174,3.4032\n",
      "28175,3.3852\n",
      "28176,3.3791\n",
      "28177,3.3587\n",
      "28178,3.3887\n",
      "28179,3.3692\n",
      "28180,3.3890\n",
      "28181,3.4011\n",
      "28182,3.3700\n",
      "28183,3.3815\n",
      "28184,3.4006\n",
      "28185,3.3816\n",
      "28186,3.3503\n",
      "28187,3.3712\n",
      "28188,3.3374\n",
      "28189,3.3536\n",
      "28190,3.4139\n",
      "28191,3.3819\n",
      "28192,3.3688\n",
      "28193,3.3740\n",
      "28194,3.3995\n",
      "28195,3.3654\n",
      "28196,3.3719\n",
      "28197,3.4180\n",
      "28198,3.3655\n",
      "28199,3.3855\n",
      "28200,3.3977\n",
      "(step: 28200, epoch: 0, block: 28876800), Train Loss: 3.2700, Val Loss: 3.3289\n",
      "Learning rate: 6.10246538e-05\n",
      "Scheduler step: 28200\n",
      "Checkpoint (step: 28200, epoch: 0, block: 28876800) Saved\n",
      "Step Documented\n",
      "14:31:14\n",
      "28201,3.3840\n",
      "28202,3.3594\n",
      "28203,3.4045\n",
      "28204,3.3912\n",
      "28205,3.3736\n",
      "28206,3.3849\n",
      "28207,3.3705\n",
      "28208,3.3696\n",
      "28209,3.3634\n",
      "28210,3.3764\n",
      "28211,3.3865\n",
      "28212,3.3836\n",
      "28213,3.3493\n",
      "28214,3.3645\n",
      "28215,3.3617\n",
      "28216,3.3952\n",
      "28217,3.3767\n",
      "28218,3.3867\n",
      "28219,3.3742\n",
      "28220,3.3539\n",
      "28221,3.3784\n",
      "28222,3.4025\n",
      "28223,3.3721\n",
      "28224,3.4025\n",
      "28225,3.3843\n",
      "28226,3.3947\n",
      "28227,3.3387\n",
      "28228,3.3748\n",
      "28229,3.3829\n",
      "28230,3.3961\n",
      "28231,3.3817\n",
      "28232,3.4200\n",
      "28233,3.3695\n",
      "28234,3.3606\n",
      "28235,3.3484\n",
      "28236,3.3733\n",
      "28237,3.4017\n",
      "28238,3.3761\n",
      "28239,3.3945\n",
      "28240,3.3654\n",
      "28241,3.3694\n",
      "28242,3.3871\n",
      "28243,3.3715\n",
      "28244,3.3845\n",
      "28245,3.3820\n",
      "28246,3.3915\n",
      "28247,3.3696\n",
      "28248,3.3815\n",
      "28249,3.3821\n",
      "28250,3.4050\n",
      "28251,3.3945\n",
      "28252,3.3724\n",
      "28253,3.3585\n",
      "28254,3.4052\n",
      "28255,3.3843\n",
      "28256,3.3762\n",
      "28257,3.3804\n",
      "28258,3.3516\n",
      "28259,3.3808\n",
      "28260,3.4089\n",
      "28261,3.3822\n",
      "28262,3.3896\n",
      "28263,3.3821\n",
      "28264,3.3494\n",
      "28265,3.3928\n",
      "28266,3.4031\n",
      "28267,3.3952\n",
      "28268,3.3809\n",
      "28269,3.3974\n",
      "28270,3.4074\n",
      "28271,3.3759\n",
      "28272,3.3850\n",
      "28273,3.3862\n",
      "28274,3.3829\n",
      "28275,3.3727\n",
      "28276,3.3829\n",
      "28277,3.3684\n",
      "28278,3.3723\n",
      "28279,3.4082\n",
      "28280,3.3837\n",
      "28281,3.4005\n",
      "28282,3.3957\n",
      "28283,3.3664\n",
      "28284,3.3790\n",
      "28285,3.3812\n",
      "28286,3.3966\n",
      "28287,3.3876\n",
      "28288,3.3797\n",
      "28289,3.3711\n",
      "28290,3.3875\n",
      "28291,3.3813\n",
      "28292,3.3661\n",
      "28293,3.3807\n",
      "28294,3.3961\n",
      "28295,3.3882\n",
      "28296,3.3877\n",
      "28297,3.3766\n",
      "28298,3.3909\n",
      "28299,3.4022\n",
      "28300,3.3532\n",
      "(step: 28300, epoch: 0, block: 28979200), Train Loss: 3.3117, Val Loss: 3.3418\n",
      "Learning rate: 6.07960568e-05\n",
      "Scheduler step: 28300\n",
      "Checkpoint (step: 28300, epoch: 0, block: 28979200) Saved\n",
      "Step Documented\n",
      "14:57:42\n",
      "28301,3.3557\n",
      "28302,3.4148\n",
      "28303,3.4261\n",
      "28304,3.3965\n",
      "28305,3.3817\n",
      "28306,3.3772\n",
      "28307,3.3832\n",
      "28308,3.4074\n",
      "28309,3.3767\n",
      "28310,3.3868\n",
      "28311,3.4089\n",
      "28312,3.3794\n",
      "28313,3.4023\n",
      "28314,3.4087\n",
      "28315,3.4060\n",
      "28316,3.3898\n",
      "28317,3.3673\n",
      "28318,3.3741\n",
      "28319,3.3669\n",
      "28320,3.3794\n",
      "28321,3.3955\n",
      "28322,3.3894\n",
      "28323,3.3465\n",
      "28324,3.3622\n",
      "28325,3.3725\n",
      "28326,3.3993\n",
      "28327,3.3863\n",
      "28328,3.3852\n",
      "28329,3.3859\n",
      "28330,3.3751\n",
      "28331,3.3740\n",
      "28332,3.3855\n",
      "28333,3.3961\n",
      "28334,3.3896\n",
      "28335,3.3786\n",
      "28336,3.3943\n",
      "28337,3.3810\n",
      "28338,3.4312\n",
      "28339,3.3916\n",
      "28340,3.3763\n",
      "28341,3.3651\n",
      "28342,3.4031\n",
      "28343,3.4094\n",
      "28344,3.4071\n",
      "28345,3.3640\n",
      "28346,3.3797\n",
      "28347,3.3803\n",
      "28348,3.3736\n",
      "28349,3.3788\n",
      "28350,3.3449\n",
      "28351,3.3846\n",
      "28352,3.3809\n",
      "28353,3.3708\n",
      "28354,3.3622\n",
      "28355,3.3795\n",
      "28356,3.3618\n",
      "28357,3.3885\n",
      "28358,3.3855\n",
      "28359,3.3712\n",
      "28360,3.4036\n",
      "28361,3.3924\n",
      "28362,3.3724\n",
      "28363,3.3611\n",
      "28364,3.3870\n",
      "28365,3.3780\n",
      "28366,3.3843\n",
      "28367,3.3911\n",
      "28368,3.3808\n",
      "28369,3.3823\n",
      "28370,3.3947\n",
      "28371,3.3697\n",
      "28372,3.3648\n",
      "28373,3.3853\n",
      "28374,3.3649\n",
      "28375,3.4015\n",
      "28376,3.4156\n",
      "28377,3.3828\n",
      "28378,3.4170\n",
      "28379,3.3897\n",
      "28380,3.3823\n",
      "28381,3.3968\n",
      "28382,3.3939\n",
      "28383,3.3706\n",
      "28384,3.3847\n",
      "28385,3.4188\n",
      "28386,3.3998\n",
      "28387,3.3901\n",
      "28388,3.3794\n",
      "28389,3.3640\n",
      "28390,3.4011\n",
      "28391,3.3691\n",
      "28392,3.3573\n",
      "28393,3.3814\n",
      "28394,3.3919\n",
      "28395,3.3696\n",
      "28396,3.3712\n",
      "28397,3.3980\n",
      "28398,3.3854\n",
      "28399,3.3970\n",
      "28400,3.3787\n",
      "(step: 28400, epoch: 0, block: 29081600), Train Loss: 3.3303, Val Loss: 3.3410\n",
      "Learning rate: 6.05672880e-05\n",
      "Scheduler step: 28400\n",
      "Checkpoint (step: 28400, epoch: 0, block: 29081600) Saved\n",
      "Step Documented\n",
      "15:24:04\n",
      "28401,3.3611\n",
      "28402,3.3860\n",
      "28403,3.3860\n",
      "28404,3.3703\n",
      "28405,3.3656\n",
      "28406,3.3913\n",
      "28407,3.3690\n",
      "28408,3.4025\n",
      "28409,3.4073\n",
      "28410,3.3900\n",
      "28411,3.3519\n",
      "28412,3.3748\n",
      "28413,3.3851\n",
      "28414,3.3804\n",
      "28415,3.3912\n",
      "28416,3.3602\n",
      "28417,3.3666\n",
      "28418,3.3635\n",
      "28419,3.3517\n",
      "28420,3.4227\n",
      "28421,3.3906\n",
      "28422,3.3750\n",
      "28423,3.3481\n",
      "28424,3.3728\n",
      "28425,3.3777\n",
      "28426,3.3646\n",
      "28427,3.3845\n",
      "28428,3.3876\n",
      "28429,3.3868\n",
      "28430,3.3805\n",
      "28431,3.3668\n",
      "28432,3.3844\n",
      "28433,3.3825\n",
      "28434,3.3841\n",
      "28435,3.3550\n",
      "28436,3.3991\n",
      "28437,3.3699\n",
      "28438,3.3754\n",
      "28439,3.3751\n",
      "28440,3.3921\n",
      "28441,3.3922\n",
      "28442,3.3824\n",
      "28443,3.3816\n",
      "28444,3.3954\n",
      "28445,3.4015\n",
      "28446,3.3958\n",
      "28447,3.3752\n",
      "28448,3.3949\n",
      "28449,3.3609\n",
      "28450,3.4223\n",
      "28451,3.3961\n",
      "28452,3.3801\n",
      "28453,3.3498\n",
      "28454,3.3827\n",
      "28455,3.4005\n",
      "28456,3.4101\n",
      "28457,3.3801\n",
      "28458,3.3804\n",
      "28459,3.4110\n",
      "28460,3.3845\n",
      "28461,3.3659\n",
      "28462,3.3764\n",
      "28463,3.3752\n",
      "28464,3.3799\n",
      "28465,3.3839\n",
      "28466,3.3790\n",
      "28467,3.3663\n",
      "28468,3.3931\n",
      "28469,3.3641\n",
      "28470,3.3889\n",
      "28471,3.3554\n",
      "28472,3.3894\n",
      "28473,3.3841\n",
      "28474,3.3897\n",
      "28475,3.3784\n",
      "28476,3.3723\n",
      "28477,3.3448\n",
      "28478,3.3895\n",
      "28479,3.3597\n",
      "28480,3.3709\n",
      "28481,3.3954\n",
      "28482,3.3756\n",
      "28483,3.3695\n",
      "28484,3.3901\n",
      "28485,3.3830\n",
      "28486,3.3582\n",
      "28487,3.3596\n",
      "28488,3.3775\n",
      "28489,3.3647\n",
      "28490,3.3975\n",
      "28491,3.3772\n",
      "28492,3.3695\n",
      "28493,3.3833\n",
      "28494,3.3694\n",
      "28495,3.3851\n",
      "28496,3.3921\n",
      "28497,3.3575\n",
      "28498,3.3747\n",
      "28499,3.3965\n",
      "28500,3.3967\n",
      "(step: 28500, epoch: 0, block: 29184000), Train Loss: 3.3321, Val Loss: 3.3333\n",
      "Learning rate: 6.03383534e-05\n",
      "Scheduler step: 28500\n",
      "Checkpoint (step: 28500, epoch: 0, block: 29184000) Saved\n",
      "Step Documented\n",
      "15:50:25\n",
      "28501,3.3916\n",
      "28502,3.3735\n",
      "28503,3.3995\n",
      "28504,3.3870\n",
      "28505,3.3906\n",
      "28506,3.3799\n",
      "28507,3.3857\n",
      "28508,3.3866\n",
      "28509,3.3858\n",
      "28510,3.3755\n",
      "28511,3.3546\n",
      "28512,3.3713\n",
      "28513,3.4115\n",
      "28514,3.3637\n",
      "28515,3.3888\n",
      "28516,3.3815\n",
      "28517,3.4160\n",
      "28518,3.3843\n",
      "28519,3.4008\n",
      "28520,3.3872\n",
      "28521,3.3618\n",
      "28522,3.3682\n",
      "28523,3.3959\n",
      "28524,3.3788\n",
      "28525,3.3973\n",
      "28526,3.3978\n",
      "28527,3.3745\n",
      "28528,3.4022\n",
      "28529,3.3610\n",
      "28530,3.3712\n",
      "28531,3.3961\n",
      "28532,3.3643\n",
      "28533,3.3954\n",
      "28534,3.3811\n",
      "28535,3.3737\n",
      "28536,3.3865\n",
      "28537,3.3636\n",
      "28538,3.3986\n",
      "28539,3.3958\n",
      "28540,3.3679\n",
      "28541,3.3877\n",
      "28542,3.4034\n",
      "28543,3.3594\n",
      "28544,3.3517\n",
      "28545,3.3785\n",
      "28546,3.3807\n",
      "28547,3.3821\n",
      "28548,3.3932\n",
      "28549,3.3320\n",
      "28550,3.3786\n",
      "28551,3.3806\n",
      "28552,3.3709\n",
      "28553,3.3875\n",
      "28554,3.3913\n",
      "28555,3.3988\n",
      "28556,3.3773\n",
      "28557,3.3925\n",
      "28558,3.3956\n",
      "28559,3.3743\n",
      "28560,3.3988\n",
      "28561,3.3887\n",
      "28562,3.3625\n",
      "28563,3.4097\n",
      "28564,3.3872\n",
      "28565,3.3780\n",
      "28566,3.4148\n",
      "28567,3.3742\n",
      "28568,3.3619\n",
      "28569,3.3864\n",
      "28570,3.3785\n",
      "28571,3.3916\n",
      "28572,3.3821\n",
      "28573,3.3971\n",
      "28574,3.4041\n",
      "28575,3.3894\n",
      "28576,3.3580\n",
      "28577,3.3913\n",
      "28578,3.3685\n",
      "28579,3.4112\n",
      "28580,3.3741\n",
      "28581,3.3787\n",
      "28582,3.3828\n",
      "28583,3.3933\n",
      "28584,3.3517\n",
      "28585,3.3832\n",
      "28586,3.3852\n",
      "28587,3.3763\n",
      "28588,3.3855\n",
      "28589,3.3920\n",
      "28590,3.3630\n",
      "28591,3.3749\n",
      "28592,3.3848\n",
      "28593,3.3268\n",
      "28594,3.4002\n",
      "28595,3.3757\n",
      "28596,3.3705\n",
      "28597,3.3909\n",
      "28598,3.3727\n",
      "28599,3.3940\n",
      "28600,3.3624\n",
      "(step: 28600, epoch: 0, block: 29286400), Train Loss: 3.2930, Val Loss: 3.3235\n",
      "Learning rate: 6.01092586e-05\n",
      "Scheduler step: 28600\n",
      "Checkpoint (step: 28600, epoch: 0, block: 29286400) Saved\n",
      "Step Documented\n",
      "16:16:47\n",
      "28601,3.3988\n",
      "28602,3.3751\n",
      "28603,3.3853\n",
      "28604,3.3814\n",
      "28605,3.3917\n",
      "28606,3.4120\n",
      "28607,3.3837\n",
      "28608,3.3848\n",
      "28609,3.3810\n",
      "28610,3.3608\n",
      "28611,3.3743\n",
      "28612,3.3756\n",
      "28613,3.3765\n",
      "28614,3.3665\n",
      "28615,3.3896\n",
      "28616,3.3747\n",
      "28617,3.4124\n",
      "28618,3.4021\n",
      "28619,3.3581\n",
      "28620,3.3805\n",
      "28621,3.3563\n",
      "28622,3.3808\n",
      "28623,3.3791\n",
      "28624,3.3828\n",
      "28625,3.3684\n",
      "28626,3.3672\n",
      "28627,3.3842\n",
      "28628,3.3945\n",
      "28629,3.3833\n",
      "28630,3.4052\n",
      "28631,3.3695\n",
      "28632,3.4025\n",
      "28633,3.3832\n",
      "28634,3.3802\n",
      "28635,3.3662\n",
      "28636,3.3714\n",
      "28637,3.3788\n",
      "28638,3.4260\n",
      "28639,3.3785\n",
      "28640,3.3904\n",
      "28641,3.3824\n",
      "28642,3.3749\n",
      "28643,3.3784\n",
      "28644,3.3754\n",
      "28645,3.3931\n",
      "28646,3.4037\n",
      "28647,3.3557\n",
      "28648,3.3855\n",
      "28649,3.3831\n",
      "28650,3.4154\n",
      "28651,3.3890\n",
      "28652,3.3861\n",
      "28653,3.3902\n",
      "28654,3.3920\n",
      "28655,3.3849\n",
      "28656,3.3753\n",
      "28657,3.3850\n",
      "28658,3.3708\n",
      "28659,3.3924\n",
      "28660,3.3769\n",
      "28661,3.3956\n",
      "28662,3.3918\n",
      "28663,3.3835\n",
      "28664,3.3799\n",
      "28665,3.4038\n",
      "28666,3.3788\n",
      "28667,3.3646\n",
      "28668,3.3614\n",
      "28669,3.3781\n",
      "28670,3.3977\n",
      "28671,3.3755\n",
      "28672,3.3608\n",
      "28673,3.3797\n",
      "28674,3.3799\n",
      "28675,3.3937\n",
      "28676,3.3673\n",
      "28677,3.3850\n",
      "28678,3.4022\n",
      "28679,3.3870\n",
      "28680,3.3806\n",
      "28681,3.3733\n",
      "28682,3.3936\n",
      "28683,3.3892\n",
      "28684,3.3816\n",
      "28685,3.3809\n",
      "28686,3.3652\n",
      "28687,3.3946\n",
      "28688,3.3864\n",
      "28689,3.3879\n",
      "28690,3.3691\n",
      "28691,3.3795\n",
      "28692,3.3991\n",
      "28693,3.4119\n",
      "28694,3.3809\n",
      "28695,3.3785\n",
      "28696,3.3743\n",
      "28697,3.3727\n",
      "28698,3.4071\n",
      "28699,3.3893\n",
      "28700,3.3858\n",
      "(step: 28700, epoch: 0, block: 29388800), Train Loss: 3.3221, Val Loss: 3.3327\n",
      "Learning rate: 5.98800094e-05\n",
      "Scheduler step: 28700\n",
      "Checkpoint (step: 28700, epoch: 0, block: 29388800) Saved\n",
      "Step Documented\n",
      "16:43:09\n",
      "28701,3.3843\n",
      "28702,3.3746\n",
      "28703,3.3807\n",
      "28704,3.3820\n",
      "28705,3.4016\n",
      "28706,3.3869\n",
      "28707,3.3681\n",
      "28708,3.3948\n",
      "28709,3.3734\n",
      "28710,3.3765\n",
      "28711,3.3736\n",
      "28712,3.3717\n",
      "28713,3.4032\n",
      "28714,3.3718\n",
      "28715,3.3819\n",
      "28716,3.3616\n",
      "28717,3.4084\n",
      "28718,3.3837\n",
      "28719,3.3580\n",
      "28720,3.3636\n",
      "28721,3.3603\n",
      "28722,3.3823\n",
      "28723,3.3847\n",
      "28724,3.3715\n",
      "28725,3.3820\n",
      "28726,3.3787\n",
      "28727,3.4088\n",
      "28728,3.3945\n",
      "28729,3.3751\n",
      "28730,3.3726\n",
      "28731,3.3535\n",
      "28732,3.3945\n",
      "28733,3.3758\n",
      "28734,3.4089\n",
      "28735,3.3833\n",
      "28736,3.3863\n",
      "28737,3.3796\n",
      "28738,3.3754\n",
      "28739,3.3692\n",
      "28740,3.4030\n",
      "28741,3.3653\n",
      "28742,3.3994\n",
      "28743,3.4177\n",
      "28744,3.3619\n",
      "28745,3.3589\n",
      "28746,3.3912\n",
      "28747,3.4008\n",
      "28748,3.3775\n",
      "28749,3.3682\n",
      "28750,3.3939\n",
      "28751,3.3691\n",
      "28752,3.4018\n",
      "28753,3.3619\n",
      "28754,3.3664\n",
      "28755,3.3549\n",
      "28756,3.3754\n",
      "28757,3.4016\n",
      "28758,3.4041\n",
      "28759,3.3812\n",
      "28760,3.3761\n",
      "28761,3.4237\n",
      "28762,3.3693\n",
      "28763,3.3716\n",
      "28764,3.3577\n",
      "28765,3.3549\n",
      "28766,3.3811\n",
      "28767,3.3604\n",
      "28768,3.3882\n",
      "28769,3.3621\n",
      "28770,3.3897\n",
      "28771,3.3721\n",
      "28772,3.3670\n",
      "28773,3.3626\n",
      "28774,3.3524\n",
      "28775,3.3727\n",
      "28776,3.3647\n",
      "28777,3.3708\n",
      "28778,3.3983\n",
      "28779,3.3744\n",
      "28780,3.3938\n",
      "28781,3.3759\n",
      "28782,3.3878\n",
      "28783,3.3857\n",
      "28784,3.3900\n",
      "28785,3.3901\n",
      "28786,3.3741\n",
      "28787,3.4061\n",
      "28788,3.3752\n",
      "28789,3.3700\n",
      "28790,3.3978\n",
      "28791,3.3946\n",
      "28792,3.3861\n",
      "28793,3.3616\n",
      "28794,3.3761\n",
      "28795,3.3700\n",
      "28796,3.4166\n",
      "28797,3.3927\n",
      "28798,3.3777\n",
      "28799,3.3842\n",
      "28800,3.3587\n",
      "(step: 28800, epoch: 0, block: 29491200), Train Loss: 3.3143, Val Loss: 3.3068\n",
      "Learning rate: 5.96506117e-05\n",
      "Scheduler step: 28800\n",
      "Checkpoint (step: 28800, epoch: 0, block: 29491200) Saved\n",
      "Step Documented\n",
      "17:09:32\n",
      "28801,3.3935\n",
      "28802,3.3863\n",
      "28803,3.3844\n",
      "28804,3.3725\n",
      "28805,3.3986\n",
      "28806,3.3622\n",
      "28807,3.3834\n",
      "28808,3.3631\n",
      "28809,3.3565\n",
      "28810,3.3836\n",
      "28811,3.3725\n",
      "28812,3.3716\n",
      "28813,3.3573\n",
      "28814,3.3848\n",
      "28815,3.3616\n",
      "28816,3.3666\n",
      "28817,3.3933\n",
      "28818,3.3657\n",
      "28819,3.3884\n",
      "28820,3.3774\n",
      "28821,3.3762\n",
      "28822,3.3816\n",
      "28823,3.3550\n",
      "28824,3.3695\n",
      "28825,3.3768\n",
      "28826,3.3769\n",
      "28827,3.3793\n",
      "28828,3.3874\n",
      "28829,3.3769\n",
      "28830,3.3667\n",
      "28831,3.3884\n",
      "28832,3.3955\n",
      "28833,3.4002\n",
      "28834,3.3889\n",
      "28835,3.3667\n",
      "28836,3.3700\n",
      "28837,3.3676\n",
      "28838,3.3786\n",
      "28839,3.3865\n",
      "28840,3.3725\n",
      "28841,3.3666\n",
      "28842,3.3638\n",
      "28843,3.3879\n",
      "28844,3.3886\n",
      "28845,3.3730\n",
      "28846,3.3796\n",
      "28847,3.4056\n",
      "28848,3.3814\n",
      "28849,3.3810\n",
      "28850,3.3641\n",
      "28851,3.3411\n",
      "28852,3.3853\n",
      "28853,3.3772\n",
      "28854,3.3749\n",
      "28855,3.3804\n",
      "28856,3.3637\n",
      "28857,3.3685\n",
      "28858,3.3836\n",
      "28859,3.3919\n",
      "28860,3.3689\n",
      "28861,3.3722\n",
      "28862,3.3605\n",
      "28863,3.3878\n",
      "28864,3.3942\n",
      "28865,3.3926\n",
      "28866,3.4027\n",
      "28867,3.3747\n",
      "28868,3.3834\n",
      "28869,3.3773\n",
      "28870,3.3962\n",
      "28871,3.3842\n",
      "28872,3.3666\n",
      "28873,3.3814\n",
      "28874,3.3960\n",
      "28875,3.3587\n",
      "28876,3.3736\n",
      "28877,3.3813\n",
      "28878,3.3812\n",
      "28879,3.3547\n",
      "28880,3.4051\n",
      "28881,3.3843\n",
      "28882,3.3643\n",
      "28883,3.3781\n",
      "28884,3.3981\n",
      "28885,3.3965\n",
      "28886,3.3553\n",
      "28887,3.3796\n",
      "28888,3.3858\n",
      "28889,3.3634\n",
      "28890,3.4236\n",
      "28891,3.3899\n",
      "28892,3.3796\n",
      "28893,3.3740\n",
      "28894,3.3837\n",
      "28895,3.3711\n",
      "28896,3.3792\n",
      "28897,3.3818\n",
      "28898,3.4030\n",
      "28899,3.3803\n",
      "28900,3.4005\n",
      "(step: 28900, epoch: 0, block: 29593600), Train Loss: 3.3024, Val Loss: 3.3225\n",
      "Learning rate: 5.94210712e-05\n",
      "Scheduler step: 28900\n",
      "Checkpoint (step: 28900, epoch: 0, block: 29593600) Saved\n",
      "Step Documented\n",
      "17:35:54\n",
      "28901,3.3534\n",
      "28902,3.3461\n",
      "28903,3.3751\n",
      "28904,3.3797\n",
      "28905,3.3848\n",
      "28906,3.3773\n",
      "28907,3.3677\n",
      "28908,3.3787\n",
      "28909,3.3629\n",
      "28910,3.3703\n",
      "28911,3.3825\n",
      "28912,3.3540\n",
      "28913,3.3993\n",
      "28914,3.3713\n",
      "28915,3.3903\n",
      "28916,3.3430\n",
      "28917,3.4000\n",
      "28918,3.3972\n",
      "28919,3.3763\n",
      "28920,3.3776\n",
      "28921,3.3858\n",
      "28922,3.4037\n",
      "28923,3.3835\n",
      "28924,3.3870\n",
      "28925,3.3607\n",
      "28926,3.4028\n",
      "28927,3.4079\n",
      "28928,3.3288\n",
      "28929,3.3910\n",
      "28930,3.3809\n",
      "28931,3.3847\n",
      "28932,3.3842\n",
      "28933,3.3750\n",
      "28934,3.3875\n",
      "28935,3.3892\n",
      "28936,3.3958\n",
      "28937,3.3737\n",
      "28938,3.4028\n",
      "28939,3.3705\n",
      "28940,3.3953\n",
      "28941,3.3426\n",
      "28942,3.3873\n",
      "28943,3.3544\n",
      "28944,3.3541\n",
      "28945,3.3837\n",
      "28946,3.3628\n",
      "28947,3.3537\n",
      "28948,3.3716\n",
      "28949,3.3687\n",
      "28950,3.3836\n",
      "28951,3.3708\n",
      "28952,3.3742\n",
      "28953,3.3822\n",
      "28954,3.3732\n",
      "28955,3.4134\n",
      "28956,3.3617\n",
      "28957,3.3687\n",
      "28958,3.3567\n",
      "28959,3.3850\n",
      "28960,3.3917\n",
      "28961,3.3814\n",
      "28962,3.3624\n",
      "28963,3.3570\n",
      "28964,3.3839\n",
      "28965,3.3784\n",
      "28966,3.3901\n",
      "28967,3.3847\n",
      "28968,3.3679\n",
      "28969,3.3909\n",
      "28970,3.3811\n",
      "28971,3.3524\n",
      "28972,3.3743\n",
      "28973,3.4075\n",
      "28974,3.3731\n",
      "28975,3.3782\n",
      "28976,3.3845\n",
      "28977,3.3490\n",
      "28978,3.3728\n",
      "28979,3.3757\n",
      "28980,3.3941\n",
      "28981,3.3677\n",
      "28982,3.3471\n",
      "28983,3.3828\n",
      "28984,3.4002\n",
      "28985,3.3988\n",
      "28986,3.3714\n",
      "28987,3.3928\n",
      "28988,3.3794\n",
      "28989,3.3606\n",
      "28990,3.3901\n",
      "28991,3.3677\n",
      "28992,3.3563\n",
      "28993,3.3694\n",
      "28994,3.3820\n",
      "28995,3.4067\n",
      "28996,3.4072\n",
      "28997,3.3659\n",
      "28998,3.3695\n",
      "28999,3.3618\n",
      "29000,3.3693\n",
      "(step: 29000, epoch: 0, block: 29696000), Train Loss: 3.3291, Val Loss: 3.3279\n",
      "Learning rate: 5.91913937e-05\n",
      "Scheduler step: 29000\n",
      "Checkpoint (step: 29000, epoch: 0, block: 29696000) Saved\n",
      "Step Documented\n",
      "18:02:16\n",
      "29001,3.4043\n",
      "29002,3.3703\n",
      "29003,3.3927\n",
      "29004,3.3891\n",
      "29005,3.3578\n",
      "29006,3.3869\n",
      "29007,3.3633\n",
      "29008,3.3800\n",
      "29009,3.3796\n",
      "29010,3.3729\n",
      "29011,3.3832\n",
      "29012,3.3807\n",
      "29013,3.3730\n",
      "29014,3.3473\n",
      "29015,3.3596\n",
      "29016,3.3817\n",
      "29017,3.3752\n",
      "29018,3.3576\n",
      "29019,3.3655\n",
      "29020,3.3739\n",
      "29021,3.3707\n",
      "29022,3.3334\n",
      "29023,3.3906\n",
      "29024,3.3762\n",
      "29025,3.3266\n",
      "29026,3.3974\n",
      "29027,3.3915\n",
      "29028,3.3714\n",
      "29029,3.3840\n",
      "29030,3.3899\n",
      "29031,3.3816\n",
      "29032,3.3852\n",
      "29033,3.3980\n",
      "29034,3.3878\n",
      "29035,3.3672\n",
      "29036,3.3571\n",
      "29037,3.3401\n",
      "29038,3.3574\n",
      "29039,3.3637\n",
      "29040,3.3992\n",
      "29041,3.4049\n",
      "29042,3.3657\n",
      "29043,3.3829\n",
      "29044,3.3584\n",
      "29045,3.3659\n",
      "29046,3.3581\n",
      "29047,3.3867\n",
      "29048,3.3489\n",
      "29049,3.3743\n",
      "29050,3.3524\n",
      "29051,3.3538\n",
      "29052,3.3878\n",
      "29053,3.3680\n",
      "29054,3.3805\n",
      "29055,3.3603\n",
      "29056,3.3727\n",
      "29057,3.3592\n",
      "29058,3.3805\n",
      "29059,3.3723\n",
      "29060,3.3755\n",
      "29061,3.3730\n",
      "29062,3.3625\n",
      "29063,3.3661\n",
      "29064,3.3977\n",
      "29065,3.3985\n",
      "29066,3.3598\n",
      "29067,3.3611\n",
      "29068,3.3876\n",
      "29069,3.3842\n",
      "29070,3.3683\n",
      "29071,3.3952\n",
      "29072,3.3688\n",
      "29073,3.3784\n",
      "29074,3.3515\n",
      "29075,3.3776\n",
      "29076,3.3854\n",
      "29077,3.3744\n",
      "29078,3.3817\n",
      "29079,3.3714\n",
      "29080,3.3466\n",
      "29081,3.3992\n",
      "29082,3.3620\n",
      "29083,3.3858\n",
      "29084,3.3601\n",
      "29085,3.3437\n",
      "29086,3.3871\n",
      "29087,3.3737\n",
      "29088,3.4031\n",
      "29089,3.3772\n",
      "29090,3.3786\n",
      "29091,3.3913\n",
      "29092,3.3612\n",
      "29093,3.3747\n",
      "29094,3.4154\n",
      "29095,3.3712\n",
      "29096,3.3934\n",
      "29097,3.3937\n",
      "29098,3.3526\n",
      "29099,3.3778\n",
      "29100,3.4062\n",
      "(step: 29100, epoch: 0, block: 29798400), Train Loss: 3.2898, Val Loss: 3.3073\n",
      "Learning rate: 5.89615851e-05\n",
      "Scheduler step: 29100\n",
      "Checkpoint (step: 29100, epoch: 0, block: 29798400) Saved\n",
      "Step Documented\n",
      "18:28:38\n",
      "29101,3.3786\n",
      "29102,3.3776\n",
      "29103,3.3860\n",
      "29104,3.3713\n",
      "29105,3.3696\n",
      "29106,3.3825\n",
      "29107,3.3778\n",
      "29108,3.3782\n",
      "29109,3.3727\n",
      "29110,3.3981\n",
      "29111,3.3890\n",
      "29112,3.3695\n",
      "29113,3.4008\n",
      "29114,3.4066\n",
      "29115,3.3729\n",
      "29116,3.3716\n",
      "29117,3.3783\n",
      "29118,3.3840\n",
      "29119,3.3915\n",
      "29120,3.3665\n",
      "29121,3.3799\n",
      "29122,3.3621\n",
      "29123,3.3548\n",
      "29124,3.3838\n",
      "29125,3.4171\n",
      "29126,3.3884\n",
      "29127,3.3623\n",
      "29128,3.4005\n",
      "29129,3.3820\n",
      "29130,3.3854\n",
      "29131,3.3904\n",
      "29132,3.3742\n",
      "29133,3.4019\n",
      "29134,3.3634\n",
      "29135,3.3732\n",
      "29136,3.3844\n",
      "29137,3.4010\n",
      "29138,3.3876\n",
      "29139,3.3941\n",
      "29140,3.3919\n",
      "29141,3.3776\n",
      "29142,3.3707\n",
      "29143,3.3869\n",
      "29144,3.3407\n",
      "29145,3.3620\n",
      "29146,3.3522\n",
      "29147,3.3784\n",
      "29148,3.3955\n",
      "29149,3.3807\n",
      "29150,3.3867\n",
      "29151,3.4042\n",
      "29152,3.3633\n",
      "29153,3.3555\n",
      "29154,3.3876\n",
      "29155,3.3712\n",
      "29156,3.3256\n",
      "29157,3.3763\n",
      "29158,3.3937\n",
      "29159,3.3744\n",
      "29160,3.3790\n",
      "29161,3.3808\n",
      "29162,3.3754\n",
      "29163,3.3588\n",
      "29164,3.3930\n",
      "29165,3.3895\n",
      "29166,3.3796\n",
      "29167,3.3758\n",
      "29168,3.3720\n",
      "29169,3.3946\n",
      "29170,3.3605\n",
      "29171,3.3700\n",
      "29172,3.3943\n",
      "29173,3.3888\n",
      "29174,3.3904\n",
      "29175,3.3411\n",
      "29176,3.3832\n",
      "29177,3.3660\n",
      "29178,3.3957\n",
      "29179,3.3643\n",
      "29180,3.3730\n",
      "29181,3.3914\n",
      "29182,3.3732\n",
      "29183,3.3600\n",
      "29184,3.3700\n",
      "29185,3.3773\n",
      "29186,3.3696\n",
      "29187,3.3980\n",
      "29188,3.3858\n",
      "29189,3.3697\n",
      "29190,3.3905\n",
      "29191,3.3758\n",
      "29192,3.3653\n",
      "29193,3.3614\n",
      "29194,3.3629\n",
      "29195,3.3649\n",
      "29196,3.4087\n",
      "29197,3.3643\n",
      "29198,3.3929\n",
      "29199,3.3939\n",
      "29200,3.3839\n",
      "(step: 29200, epoch: 0, block: 29900800), Train Loss: 3.2893, Val Loss: 3.3319\n",
      "Learning rate: 5.87316511e-05\n",
      "Scheduler step: 29200\n",
      "Checkpoint (step: 29200, epoch: 0, block: 29900800) Saved\n",
      "Step Documented\n",
      "18:55:01\n",
      "29201,3.3834\n",
      "29202,3.3637\n",
      "29203,3.3672\n",
      "29204,3.3715\n",
      "29205,3.3677\n",
      "29206,3.3807\n",
      "29207,3.3988\n",
      "29208,3.3669\n",
      "29209,3.3919\n",
      "29210,3.3822\n",
      "29211,3.3900\n",
      "29212,3.3802\n",
      "29213,3.3960\n",
      "29214,3.3670\n",
      "29215,3.3877\n",
      "29216,3.3826\n",
      "29217,3.4012\n",
      "29218,3.4031\n",
      "29219,3.3654\n",
      "29220,3.3997\n",
      "29221,3.4073\n",
      "29222,3.3679\n",
      "29223,3.3609\n",
      "29224,3.3615\n",
      "29225,3.3751\n",
      "29226,3.4148\n",
      "29227,3.3818\n",
      "29228,3.3815\n",
      "29229,3.3945\n",
      "29230,3.3826\n",
      "29231,3.3819\n",
      "29232,3.3695\n",
      "29233,3.3714\n",
      "29234,3.3898\n",
      "29235,3.3841\n",
      "29236,3.3881\n",
      "29237,3.3785\n",
      "29238,3.3710\n",
      "29239,3.4165\n",
      "29240,3.3554\n",
      "29241,3.3412\n",
      "29242,3.4032\n",
      "29243,3.3875\n",
      "29244,3.3642\n",
      "29245,3.4191\n",
      "29246,3.3572\n",
      "29247,3.3513\n",
      "29248,3.3642\n",
      "29249,3.4129\n",
      "29250,3.3941\n",
      "29251,3.3886\n",
      "29252,3.3806\n",
      "29253,3.3744\n",
      "29254,3.3953\n",
      "29255,3.3699\n",
      "29256,3.3543\n",
      "29257,3.3614\n",
      "29258,3.3756\n",
      "29259,3.3651\n",
      "29260,3.3859\n",
      "29261,3.3782\n",
      "29262,3.3856\n",
      "29263,3.3664\n",
      "29264,3.3721\n",
      "29265,3.3672\n",
      "29266,3.3863\n",
      "29267,3.3790\n",
      "29268,3.3575\n",
      "29269,3.3966\n",
      "29270,3.3869\n",
      "29271,3.3857\n",
      "29272,3.3716\n",
      "29273,3.3743\n",
      "29274,3.3662\n",
      "29275,3.3649\n",
      "29276,3.3874\n",
      "29277,3.3602\n",
      "29278,3.3667\n",
      "29279,3.3894\n",
      "29280,3.3828\n",
      "29281,3.3625\n",
      "29282,3.3651\n",
      "29283,3.3943\n",
      "29284,3.3914\n",
      "29285,3.3622\n",
      "29286,3.3887\n",
      "29287,3.4001\n",
      "29288,3.3658\n",
      "29289,3.4013\n",
      "29290,3.3699\n",
      "29291,3.3631\n",
      "29292,3.3335\n",
      "29293,3.3700\n",
      "29294,3.3617\n",
      "29295,3.3650\n",
      "29296,3.3545\n",
      "29297,3.3773\n",
      "29298,3.4022\n",
      "29299,3.3689\n",
      "29300,3.3724\n",
      "(step: 29300, epoch: 0, block: 30003200), Train Loss: 3.2996, Val Loss: 3.3314\n",
      "Learning rate: 5.85015975e-05\n",
      "Scheduler step: 29300\n",
      "Checkpoint (step: 29300, epoch: 0, block: 30003200) Saved\n",
      "Step Documented\n",
      "19:21:22\n",
      "29301,3.3824\n",
      "29302,3.3936\n",
      "29303,3.3623\n",
      "29304,3.3704\n",
      "29305,3.3686\n",
      "29306,3.3844\n",
      "29307,3.3579\n",
      "29308,3.3865\n",
      "29309,3.3638\n",
      "29310,3.3690\n",
      "29311,3.4070\n",
      "29312,3.3444\n",
      "29313,3.3966\n",
      "29314,3.3841\n",
      "29315,3.3695\n",
      "29316,3.3877\n",
      "29317,3.3717\n",
      "29318,3.3678\n",
      "29319,3.3727\n",
      "29320,3.4070\n",
      "29321,3.3854\n",
      "29322,3.4005\n",
      "29323,3.3729\n",
      "29324,3.4147\n",
      "29325,3.3617\n",
      "29326,3.3847\n",
      "29327,3.3987\n",
      "29328,3.3737\n",
      "29329,3.3517\n",
      "29330,3.3694\n",
      "29331,3.3761\n",
      "29332,3.3861\n",
      "29333,3.3910\n",
      "29334,3.3953\n",
      "29335,3.3475\n",
      "29336,3.3695\n",
      "29337,3.3915\n",
      "29338,3.3821\n",
      "29339,3.3633\n",
      "29340,3.3788\n",
      "29341,3.3660\n",
      "29342,3.3666\n",
      "29343,3.3439\n",
      "29344,3.3794\n",
      "29345,3.3950\n",
      "29346,3.3679\n",
      "29347,3.4078\n",
      "29348,3.3658\n",
      "29349,3.3634\n",
      "29350,3.3667\n",
      "29351,3.4029\n",
      "29352,3.3989\n",
      "29353,3.3806\n",
      "29354,3.3833\n",
      "29355,3.3592\n",
      "29356,3.3752\n",
      "29357,3.3804\n",
      "29358,3.3826\n",
      "29359,3.3785\n",
      "29360,3.3736\n",
      "29361,3.3700\n",
      "29362,3.3762\n",
      "29363,3.3743\n",
      "29364,3.3705\n",
      "29365,3.3614\n",
      "29366,3.3966\n",
      "29367,3.3695\n",
      "29368,3.3694\n",
      "29369,3.3691\n",
      "29370,3.3721\n",
      "29371,3.3615\n",
      "29372,3.3786\n",
      "29373,3.3776\n",
      "29374,3.3874\n",
      "29375,3.3450\n",
      "29376,3.3868\n",
      "29377,3.3696\n",
      "29378,3.3847\n",
      "29379,3.4118\n",
      "29380,3.3498\n",
      "29381,3.4019\n",
      "29382,3.3494\n",
      "29383,3.3770\n",
      "29384,3.3811\n",
      "29385,3.3711\n",
      "29386,3.3684\n",
      "29387,3.3813\n",
      "29388,3.3807\n",
      "29389,3.3756\n",
      "29390,3.3885\n",
      "29391,3.3698\n",
      "29392,3.3645\n",
      "29393,3.3708\n",
      "29394,3.3639\n",
      "29395,3.3931\n",
      "29396,3.3706\n",
      "29397,3.3503\n",
      "29398,3.3508\n",
      "29399,3.3597\n",
      "29400,3.4043\n",
      "(step: 29400, epoch: 0, block: 30105600), Train Loss: 3.3124, Val Loss: 3.3289\n",
      "Learning rate: 5.82714302e-05\n",
      "Scheduler step: 29400\n",
      "Checkpoint (step: 29400, epoch: 0, block: 30105600) Saved\n",
      "Step Documented\n",
      "19:47:42\n",
      "29401,3.3765\n",
      "29402,3.3476\n",
      "29403,3.3776\n",
      "29404,3.3742\n",
      "29405,3.3765\n",
      "29406,3.3497\n",
      "29407,3.3568\n",
      "29408,3.3790\n",
      "29409,3.3737\n",
      "29410,3.3804\n",
      "29411,3.3757\n",
      "29412,3.3615\n",
      "29413,3.3776\n",
      "29414,3.3583\n",
      "29415,3.3681\n",
      "29416,3.4015\n",
      "29417,3.3649\n",
      "29418,3.3990\n",
      "29419,3.3858\n",
      "29420,3.3838\n",
      "29421,3.3808\n",
      "29422,3.3667\n",
      "29423,3.3720\n",
      "29424,3.3743\n",
      "29425,3.3569\n",
      "29426,3.3987\n",
      "29427,3.3838\n",
      "29428,3.3650\n",
      "29429,3.3726\n",
      "29430,3.3773\n",
      "29431,3.3698\n",
      "29432,3.3902\n",
      "29433,3.3997\n",
      "29434,3.3838\n",
      "29435,3.3894\n",
      "29436,3.3592\n",
      "29437,3.3795\n",
      "29438,3.3741\n",
      "29439,3.3794\n",
      "29440,3.3918\n",
      "29441,3.3479\n",
      "29442,3.3832\n",
      "29443,3.3620\n",
      "29444,3.3509\n",
      "29445,3.4067\n",
      "29446,3.3938\n",
      "29447,3.3535\n",
      "29448,3.4131\n",
      "29449,3.3705\n",
      "29450,3.3610\n",
      "29451,3.3496\n",
      "29452,3.3618\n",
      "29453,3.3522\n",
      "29454,3.3944\n",
      "29455,3.3510\n",
      "29456,3.3581\n",
      "29457,3.3837\n",
      "29458,3.3653\n",
      "29459,3.3752\n",
      "29460,3.3956\n",
      "29461,3.3615\n",
      "29462,3.3877\n",
      "29463,3.3530\n",
      "29464,3.4078\n",
      "29465,3.3777\n",
      "29466,3.3972\n",
      "29467,3.3589\n",
      "29468,3.3774\n",
      "29469,3.3738\n",
      "29470,3.3707\n",
      "29471,3.3649\n",
      "29472,3.3364\n",
      "29473,3.3779\n",
      "29474,3.3846\n",
      "29475,3.3644\n",
      "29476,3.3818\n",
      "29477,3.3690\n",
      "29478,3.3940\n",
      "29479,3.3629\n",
      "29480,3.3862\n",
      "29481,3.3777\n",
      "29482,3.3750\n",
      "29483,3.3752\n",
      "29484,3.3820\n",
      "29485,3.3614\n",
      "29486,3.3729\n",
      "29487,3.3634\n",
      "29488,3.3699\n",
      "29489,3.3502\n",
      "29490,3.3913\n",
      "29491,3.3520\n",
      "29492,3.3801\n",
      "29493,3.3716\n",
      "29494,3.3761\n",
      "29495,3.3682\n",
      "29496,3.3727\n",
      "29497,3.3912\n",
      "29498,3.3568\n",
      "29499,3.3946\n",
      "29500,3.3761\n",
      "(step: 29500, epoch: 0, block: 30208000), Train Loss: 3.2752, Val Loss: 3.3263\n",
      "Learning rate: 5.80411550e-05\n",
      "Scheduler step: 29500\n",
      "Checkpoint (step: 29500, epoch: 0, block: 30208000) Saved\n",
      "Step Documented\n",
      "20:14:03\n",
      "29501,3.3827\n",
      "29502,3.4131\n",
      "29503,3.3942\n",
      "29504,3.3965\n",
      "29505,3.3953\n",
      "29506,3.3990\n",
      "29507,3.3643\n",
      "29508,3.3669\n",
      "29509,3.3647\n",
      "29510,3.3998\n",
      "29511,3.3832\n",
      "29512,3.3788\n",
      "29513,3.3767\n",
      "29514,3.3623\n",
      "29515,3.3767\n",
      "29516,3.3612\n",
      "29517,3.3860\n",
      "29518,3.3902\n",
      "29519,3.3697\n",
      "29520,3.3745\n",
      "29521,3.3540\n",
      "29522,3.3559\n",
      "29523,3.3979\n",
      "29524,3.3539\n",
      "29525,3.3584\n",
      "29526,3.3553\n",
      "29527,3.3724\n",
      "29528,3.3780\n",
      "29529,3.3861\n",
      "29530,3.4033\n",
      "29531,3.3652\n",
      "29532,3.3753\n",
      "29533,3.3870\n",
      "29534,3.3636\n",
      "29535,3.3753\n",
      "29536,3.3967\n",
      "29537,3.4061\n",
      "29538,3.3871\n",
      "29539,3.3875\n",
      "29540,3.3704\n",
      "29541,3.3489\n",
      "29542,3.3714\n",
      "29543,3.3858\n",
      "29544,3.3812\n",
      "29545,3.3983\n",
      "29546,3.3993\n",
      "29547,3.3814\n",
      "29548,3.3869\n",
      "29549,3.3809\n",
      "29550,3.3932\n",
      "29551,3.3707\n",
      "29552,3.3743\n",
      "29553,3.3641\n",
      "29554,3.3494\n",
      "29555,3.3629\n",
      "29556,3.3730\n",
      "29557,3.3974\n",
      "29558,3.3604\n",
      "29559,3.3827\n",
      "29560,3.3558\n",
      "29561,3.3983\n",
      "29562,3.3739\n",
      "29563,3.3897\n",
      "29564,3.3961\n",
      "29565,3.3890\n",
      "29566,3.3872\n",
      "29567,3.3603\n",
      "29568,3.3918\n",
      "29569,3.3834\n",
      "29570,3.3891\n",
      "29571,3.3595\n",
      "29572,3.3689\n",
      "29573,3.3701\n",
      "29574,3.3393\n",
      "29575,3.3697\n",
      "29576,3.3763\n",
      "29577,3.3769\n",
      "29578,3.3776\n",
      "29579,3.3666\n",
      "29580,3.3705\n",
      "29581,3.3874\n",
      "29582,3.3657\n",
      "29583,3.3681\n",
      "29584,3.3568\n",
      "29585,3.3748\n",
      "29586,3.3920\n",
      "29587,3.3647\n",
      "29588,3.3839\n",
      "29589,3.3478\n",
      "29590,3.3748\n",
      "29591,3.3638\n",
      "29592,3.3782\n",
      "29593,3.3806\n",
      "29594,3.3750\n",
      "29595,3.3494\n",
      "29596,3.3897\n",
      "29597,3.3677\n",
      "29598,3.3710\n",
      "29599,3.3690\n",
      "29600,3.3663\n",
      "(step: 29600, epoch: 0, block: 30310400), Train Loss: 3.3058, Val Loss: 3.3023\n",
      "Learning rate: 5.78107777e-05\n",
      "Scheduler step: 29600\n",
      "Checkpoint (step: 29600, epoch: 0, block: 30310400) Saved\n",
      "Step Documented\n",
      "20:40:22\n",
      "29601,3.3583\n",
      "29602,3.3811\n",
      "29603,3.3719\n",
      "29604,3.3666\n",
      "29605,3.3452\n",
      "29606,3.3644\n",
      "29607,3.3849\n",
      "29608,3.3698\n",
      "29609,3.3613\n",
      "29610,3.3589\n",
      "29611,3.3711\n",
      "29612,3.3798\n",
      "29613,3.3725\n",
      "29614,3.3826\n",
      "29615,3.3289\n",
      "29616,3.3770\n",
      "29617,3.3700\n",
      "29618,3.3586\n",
      "29619,3.3632\n",
      "29620,3.3879\n",
      "29621,3.3614\n",
      "29622,3.3488\n",
      "29623,3.4013\n",
      "29624,3.3846\n",
      "29625,3.3643\n",
      "29626,3.3787\n",
      "29627,3.3747\n",
      "29628,3.3449\n",
      "29629,3.3676\n",
      "29630,3.3819\n",
      "29631,3.3521\n",
      "29632,3.3690\n",
      "29633,3.3862\n",
      "29634,3.3588\n",
      "29635,3.3947\n",
      "29636,3.3921\n",
      "29637,3.3901\n",
      "29638,3.3368\n",
      "29639,3.3762\n",
      "29640,3.3574\n",
      "29641,3.3893\n",
      "29642,3.3413\n",
      "29643,3.3704\n",
      "29644,3.3846\n",
      "29645,3.3815\n",
      "29646,3.4046\n",
      "29647,3.3722\n",
      "29648,3.3915\n",
      "29649,3.3165\n",
      "29650,3.3803\n",
      "29651,3.3674\n",
      "29652,3.3789\n",
      "29653,3.3615\n",
      "29654,3.3803\n",
      "29655,3.3697\n",
      "29656,3.3897\n",
      "29657,3.3554\n",
      "29658,3.3746\n",
      "29659,3.3713\n",
      "29660,3.3744\n",
      "29661,3.3643\n",
      "29662,3.3586\n",
      "29663,3.3906\n",
      "29664,3.3488\n",
      "29665,3.3856\n",
      "29666,3.3533\n",
      "29667,3.3976\n",
      "29668,3.3688\n",
      "29669,3.3843\n",
      "29670,3.3887\n",
      "29671,3.3724\n",
      "29672,3.3854\n",
      "29673,3.3653\n",
      "29674,3.3781\n",
      "29675,3.3373\n",
      "29676,3.3618\n",
      "29677,3.3724\n",
      "29678,3.3678\n",
      "29679,3.3783\n",
      "29680,3.3801\n",
      "29681,3.3721\n",
      "29682,3.3749\n",
      "29683,3.3890\n",
      "29684,3.3890\n",
      "29685,3.3873\n",
      "29686,3.3728\n",
      "29687,3.3768\n",
      "29688,3.4074\n",
      "29689,3.3877\n",
      "29690,3.3542\n",
      "29691,3.3784\n",
      "29692,3.3714\n",
      "29693,3.3437\n",
      "29694,3.3911\n",
      "29695,3.3885\n",
      "29696,3.3726\n",
      "29697,3.3705\n",
      "29698,3.3713\n",
      "29699,3.3685\n",
      "29700,3.3563\n",
      "(step: 29700, epoch: 0, block: 30412800), Train Loss: 3.2660, Val Loss: 3.3195\n",
      "Learning rate: 5.75803041e-05\n",
      "Scheduler step: 29700\n",
      "Checkpoint (step: 29700, epoch: 0, block: 30412800) Saved\n",
      "Step Documented\n",
      "21:06:41\n",
      "29701,3.3759\n",
      "29702,3.3691\n",
      "29703,3.3756\n",
      "29704,3.3801\n",
      "29705,3.4000\n",
      "29706,3.3610\n",
      "29707,3.3757\n",
      "29708,3.3576\n",
      "29709,3.4005\n",
      "29710,3.3588\n",
      "29711,3.3595\n",
      "29712,3.3440\n",
      "29713,3.3613\n",
      "29714,3.3867\n",
      "29715,3.3601\n",
      "29716,3.3882\n",
      "29717,3.3593\n",
      "29718,3.3841\n",
      "29719,3.3791\n",
      "29720,3.3788\n",
      "29721,3.3671\n",
      "29722,3.3744\n",
      "29723,3.3709\n",
      "29724,3.3707\n",
      "29725,3.3983\n",
      "29726,3.3744\n",
      "29727,3.3938\n",
      "29728,3.3967\n",
      "29729,3.3798\n",
      "29730,3.3390\n",
      "29731,3.3341\n",
      "29732,3.3999\n",
      "29733,3.3312\n",
      "29734,3.3646\n",
      "29735,3.3838\n",
      "29736,3.3685\n",
      "29737,3.3941\n",
      "29738,3.3616\n",
      "29739,3.3772\n",
      "29740,3.3553\n",
      "29741,3.3550\n",
      "29742,3.3690\n",
      "29743,3.3899\n",
      "29744,3.3508\n",
      "29745,3.3522\n",
      "29746,3.3555\n",
      "29747,3.3565\n",
      "29748,3.3652\n",
      "29749,3.3884\n",
      "29750,3.3973\n",
      "29751,3.3670\n",
      "29752,3.3718\n",
      "29753,3.3773\n",
      "29754,3.3835\n",
      "29755,3.3689\n",
      "29756,3.3675\n",
      "29757,3.4006\n",
      "29758,3.3820\n",
      "29759,3.3766\n",
      "29760,3.3693\n",
      "29761,3.3790\n",
      "29762,3.3356\n",
      "29763,3.3756\n",
      "29764,3.3780\n",
      "29765,3.3735\n",
      "29766,3.3876\n",
      "29767,3.3746\n",
      "29768,3.3658\n",
      "29769,3.3651\n",
      "29770,3.3876\n",
      "29771,3.3678\n",
      "29772,3.3632\n",
      "29773,3.3636\n",
      "29774,3.3767\n",
      "29775,3.3634\n",
      "29776,3.3942\n",
      "29777,3.3629\n",
      "29778,3.3797\n",
      "29779,3.3647\n",
      "29780,3.3692\n",
      "29781,3.3644\n",
      "29782,3.4200\n",
      "29783,3.3727\n",
      "29784,3.3533\n",
      "29785,3.3646\n",
      "29786,3.3603\n",
      "29787,3.3748\n",
      "29788,3.3743\n",
      "29789,3.3832\n",
      "29790,3.3744\n",
      "29791,3.3726\n",
      "29792,3.3517\n",
      "29793,3.3602\n",
      "29794,3.3630\n",
      "29795,3.3843\n",
      "29796,3.3962\n",
      "29797,3.3776\n",
      "29798,3.3853\n",
      "29799,3.3471\n",
      "29800,3.3711\n",
      "(step: 29800, epoch: 0, block: 30515200), Train Loss: 3.2848, Val Loss: 3.3095\n",
      "Learning rate: 5.73497401e-05\n",
      "Scheduler step: 29800\n",
      "Checkpoint (step: 29800, epoch: 0, block: 30515200) Saved\n",
      "Step Documented\n",
      "21:32:59\n",
      "29801,3.3666\n",
      "29802,3.3533\n",
      "29803,3.4033\n",
      "29804,3.3827\n",
      "29805,3.3787\n",
      "29806,3.3818\n",
      "29807,3.3488\n",
      "29808,3.3806\n",
      "29809,3.3760\n",
      "29810,3.3829\n",
      "29811,3.3438\n",
      "29812,3.3505\n",
      "29813,3.3761\n",
      "29814,3.3842\n",
      "29815,3.3875\n",
      "29816,3.3957\n",
      "29817,3.3861\n",
      "29818,3.3704\n",
      "29819,3.3781\n",
      "29820,3.3592\n",
      "29821,3.3523\n",
      "29822,3.3636\n",
      "29823,3.3496\n",
      "29824,3.3615\n",
      "29825,3.3862\n",
      "29826,3.3566\n",
      "29827,3.3763\n",
      "29828,3.4097\n",
      "29829,3.3556\n",
      "29830,3.3956\n",
      "29831,3.3872\n",
      "29832,3.3736\n",
      "29833,3.3743\n",
      "29834,3.3772\n",
      "29835,3.3870\n",
      "29836,3.3772\n",
      "29837,3.3605\n",
      "29838,3.3645\n",
      "29839,3.3812\n",
      "29840,3.3538\n",
      "29841,3.3660\n",
      "29842,3.3963\n",
      "29843,3.3633\n",
      "29844,3.3946\n",
      "29845,3.3854\n",
      "29846,3.4029\n",
      "29847,3.3542\n",
      "29848,3.3709\n",
      "29849,3.3665\n",
      "29850,3.3715\n",
      "29851,3.3569\n",
      "29852,3.3671\n",
      "29853,3.3664\n",
      "29854,3.3748\n",
      "29855,3.3597\n",
      "29856,3.3715\n",
      "29857,3.3612\n",
      "29858,3.3966\n",
      "29859,3.3967\n",
      "29860,3.3952\n",
      "29861,3.3922\n",
      "29862,3.3703\n",
      "29863,3.3377\n",
      "29864,3.3959\n",
      "29865,3.3916\n",
      "29866,3.3824\n",
      "29867,3.3815\n",
      "29868,3.3675\n",
      "29869,3.3664\n",
      "29870,3.3907\n",
      "29871,3.3787\n",
      "29872,3.3951\n",
      "29873,3.3724\n",
      "29874,3.3675\n",
      "29875,3.3704\n",
      "29876,3.3854\n",
      "29877,3.3693\n",
      "29878,3.3760\n",
      "29879,3.3614\n",
      "29880,3.3674\n",
      "29881,3.3802\n",
      "29882,3.3596\n",
      "29883,3.3875\n",
      "29884,3.3681\n",
      "29885,3.3853\n",
      "29886,3.3688\n",
      "29887,3.3569\n",
      "29888,3.3643\n",
      "29889,3.3560\n",
      "29890,3.3638\n",
      "29891,3.3665\n",
      "29892,3.3932\n",
      "29893,3.3662\n",
      "29894,3.3602\n",
      "29895,3.3887\n",
      "29896,3.3605\n",
      "29897,3.3739\n",
      "29898,3.3638\n",
      "29899,3.3631\n",
      "29900,3.3672\n",
      "(step: 29900, epoch: 0, block: 30617600), Train Loss: 3.2981, Val Loss: 3.2981\n",
      "Learning rate: 5.71190914e-05\n",
      "Scheduler step: 29900\n",
      "Checkpoint (step: 29900, epoch: 0, block: 30617600) Saved\n",
      "Step Documented\n",
      "21:59:21\n",
      "29901,3.3605\n",
      "29902,3.3650\n",
      "29903,3.3816\n",
      "29904,3.3593\n",
      "29905,3.3862\n",
      "29906,3.3441\n",
      "29907,3.3684\n",
      "29908,3.3411\n",
      "29909,3.3706\n",
      "29910,3.3744\n",
      "29911,3.3644\n",
      "29912,3.3618\n",
      "29913,3.3365\n",
      "29914,3.3626\n",
      "29915,3.3660\n",
      "29916,3.3700\n",
      "29917,3.3353\n",
      "29918,3.3792\n",
      "29919,3.3718\n",
      "29920,3.3857\n",
      "29921,3.3670\n",
      "29922,3.4191\n",
      "29923,3.3699\n",
      "29924,3.3622\n",
      "29925,3.3458\n",
      "29926,3.3622\n",
      "29927,3.3648\n",
      "29928,3.3682\n",
      "29929,3.3629\n",
      "29930,3.3692\n",
      "29931,3.3806\n",
      "29932,3.3648\n",
      "29933,3.3771\n",
      "29934,3.3731\n",
      "29935,3.3896\n",
      "29936,3.3515\n",
      "29937,3.4108\n",
      "29938,3.3815\n",
      "29939,3.3971\n",
      "29940,3.3953\n",
      "29941,3.4121\n",
      "29942,3.3977\n",
      "29943,3.3982\n",
      "29944,3.3628\n",
      "29945,3.3767\n",
      "29946,3.3978\n",
      "29947,3.3868\n",
      "29948,3.3507\n",
      "29949,3.3798\n",
      "29950,3.3893\n",
      "29951,3.3702\n",
      "29952,3.3745\n",
      "29953,3.3750\n",
      "29954,3.3865\n",
      "29955,3.4053\n",
      "29956,3.3824\n",
      "29957,3.3513\n",
      "29958,3.3780\n",
      "29959,3.3867\n",
      "29960,3.3884\n",
      "29961,3.3596\n",
      "29962,3.3657\n",
      "29963,3.3790\n",
      "29964,3.3753\n",
      "29965,3.3813\n",
      "29966,3.3447\n",
      "29967,3.3753\n",
      "29968,3.3803\n",
      "29969,3.3554\n",
      "29970,3.3672\n",
      "29971,3.3582\n",
      "29972,3.3766\n",
      "29973,3.3969\n",
      "29974,3.4002\n",
      "29975,3.3604\n",
      "29976,3.3630\n",
      "29977,3.4000\n",
      "29978,3.3617\n",
      "29979,3.3661\n",
      "29980,3.3605\n",
      "29981,3.3549\n",
      "29982,3.3621\n",
      "29983,3.3798\n",
      "29984,3.3385\n",
      "29985,3.3915\n",
      "29986,3.3753\n",
      "29987,3.4123\n",
      "29988,3.3516\n",
      "29989,3.3504\n",
      "29990,3.3907\n",
      "29991,3.3779\n",
      "29992,3.3605\n",
      "29993,3.3512\n",
      "29994,3.3657\n",
      "29995,3.3650\n",
      "29996,3.3597\n",
      "29997,3.4050\n",
      "29998,3.3424\n",
      "29999,3.3520\n",
      "30000,3.3808\n",
      "(step: 30000, epoch: 0, block: 30720000), Train Loss: 3.2996, Val Loss: 3.3415\n",
      "Learning rate: 5.68883639e-05\n",
      "Scheduler step: 30000\n",
      "Checkpoint (step: 30000, epoch: 0, block: 30720000) Saved\n",
      "Step Documented\n",
      "22:25:41\n",
      "30001,3.3592\n",
      "30002,3.3792\n",
      "30003,3.3769\n",
      "30004,3.3742\n",
      "30005,3.3764\n",
      "30006,3.3587\n",
      "30007,3.3596\n",
      "30008,3.3880\n",
      "30009,3.3671\n",
      "30010,3.3846\n",
      "30011,3.3847\n",
      "30012,3.3556\n",
      "30013,3.3725\n",
      "30014,3.3723\n",
      "30015,3.3580\n",
      "30016,3.3992\n",
      "30017,3.3769\n",
      "30018,3.3798\n",
      "30019,3.3564\n",
      "30020,3.3787\n",
      "30021,3.3848\n",
      "30022,3.3949\n",
      "30023,3.4036\n",
      "30024,3.3717\n",
      "30025,3.3566\n",
      "30026,3.3685\n",
      "30027,3.3834\n",
      "30028,3.3701\n",
      "30029,3.3902\n",
      "30030,3.3857\n",
      "30031,3.3721\n",
      "30032,3.3268\n",
      "30033,3.3555\n",
      "30034,3.3739\n",
      "30035,3.3532\n",
      "30036,3.3584\n",
      "30037,3.3885\n",
      "30038,3.3514\n",
      "30039,3.3734\n",
      "30040,3.3741\n",
      "30041,3.3569\n",
      "30042,3.3678\n",
      "30043,3.3608\n",
      "30044,3.3649\n",
      "30045,3.3706\n",
      "30046,3.3887\n",
      "30047,3.3341\n",
      "30048,3.3550\n",
      "30049,3.3869\n",
      "30050,3.3934\n",
      "30051,3.3799\n",
      "30052,3.3653\n",
      "30053,3.3895\n",
      "30054,3.3552\n",
      "30055,3.3716\n",
      "30056,3.4129\n",
      "30057,3.3634\n",
      "30058,3.3866\n",
      "30059,3.3889\n",
      "30060,3.3800\n",
      "30061,3.3850\n",
      "30062,3.3793\n",
      "30063,3.3860\n",
      "30064,3.3871\n",
      "30065,3.4052\n",
      "30066,3.3774\n",
      "30067,3.3750\n",
      "30068,3.3810\n",
      "30069,3.3605\n",
      "30070,3.3755\n",
      "30071,3.3885\n",
      "30072,3.3706\n",
      "30073,3.3955\n",
      "30074,3.3840\n",
      "30075,3.3733\n",
      "30076,3.3973\n",
      "30077,3.3695\n",
      "30078,3.3662\n",
      "30079,3.4033\n",
      "30080,3.3654\n",
      "30081,3.3704\n",
      "30082,3.3544\n",
      "30083,3.3593\n",
      "30084,3.3759\n",
      "30085,3.3951\n",
      "30086,3.3852\n",
      "30087,3.3864\n",
      "30088,3.3731\n",
      "30089,3.3731\n",
      "30090,3.3640\n",
      "30091,3.3732\n",
      "30092,3.3915\n",
      "30093,3.3597\n",
      "30094,3.3566\n",
      "30095,3.4028\n",
      "30096,3.3492\n",
      "30097,3.3541\n",
      "30098,3.3653\n",
      "30099,3.3607\n",
      "30100,3.3798\n",
      "(step: 30100, epoch: 0, block: 30822400), Train Loss: 3.3209, Val Loss: 3.2949\n",
      "Learning rate: 5.66575634e-05\n",
      "Scheduler step: 30100\n",
      "Checkpoint (step: 30100, epoch: 0, block: 30822400) Saved\n",
      "Step Documented\n",
      "22:52:00\n",
      "30101,3.3824\n",
      "30102,3.3824\n",
      "30103,3.3629\n",
      "30104,3.3480\n",
      "30105,3.4269\n",
      "30106,3.3596\n",
      "30107,3.3761\n",
      "30108,3.3698\n",
      "30109,3.3580\n",
      "30110,3.3631\n",
      "30111,3.3786\n",
      "30112,3.3765\n",
      "30113,3.3849\n",
      "30114,3.3763\n",
      "30115,3.3723\n",
      "30116,3.3614\n",
      "30117,3.3882\n",
      "30118,3.3531\n",
      "30119,3.3756\n",
      "30120,3.3708\n",
      "30121,3.3806\n",
      "30122,3.3612\n",
      "30123,3.3841\n",
      "30124,3.3462\n",
      "30125,3.3650\n",
      "30126,3.4000\n",
      "30127,3.3547\n",
      "30128,3.3909\n",
      "30129,3.3953\n",
      "30130,3.3815\n",
      "30131,3.3743\n",
      "30132,3.3717\n",
      "30133,3.3625\n",
      "30134,3.3821\n",
      "30135,3.3491\n",
      "30136,3.3848\n",
      "30137,3.3485\n",
      "30138,3.4137\n",
      "30139,3.3683\n",
      "30140,3.3663\n",
      "30141,3.3533\n",
      "30142,3.3652\n",
      "30143,3.3886\n",
      "30144,3.3547\n",
      "30145,3.3587\n",
      "30146,3.3724\n",
      "30147,3.3471\n",
      "30148,3.4014\n",
      "30149,3.3579\n",
      "30150,3.3568\n",
      "30151,3.3507\n",
      "30152,3.3769\n",
      "30153,3.3431\n",
      "30154,3.3823\n",
      "30155,3.3445\n",
      "30156,3.3852\n",
      "30157,3.4094\n",
      "30158,3.3277\n",
      "30159,3.3623\n",
      "30160,3.3759\n",
      "30161,3.3831\n",
      "30162,3.3575\n",
      "30163,3.3541\n",
      "30164,3.3606\n",
      "30165,3.3930\n",
      "30166,3.3839\n",
      "30167,3.3325\n",
      "30168,3.3567\n",
      "30169,3.3375\n",
      "30170,3.3886\n",
      "30171,3.3528\n",
      "30172,3.3635\n",
      "30173,3.3656\n",
      "30174,3.3697\n",
      "30175,3.3571\n",
      "30176,3.3696\n",
      "30177,3.3568\n",
      "30178,3.3821\n",
      "30179,3.3574\n",
      "30180,3.3663\n",
      "30181,3.3824\n",
      "30182,3.3809\n",
      "30183,3.3767\n",
      "30184,3.3950\n",
      "30185,3.3799\n",
      "30186,3.3757\n",
      "30187,3.3746\n",
      "30188,3.3954\n",
      "30189,3.3778\n",
      "30190,3.3947\n",
      "30191,3.4118\n",
      "30192,3.3710\n",
      "30193,3.3688\n",
      "30194,3.4065\n",
      "30195,3.3630\n",
      "30196,3.3809\n",
      "30197,3.3760\n",
      "30198,3.3728\n",
      "30199,3.3494\n",
      "30200,3.3625\n",
      "(step: 30200, epoch: 0, block: 30924800), Train Loss: 3.3231, Val Loss: 3.3014\n",
      "Learning rate: 5.64266958e-05\n",
      "Scheduler step: 30200\n",
      "Checkpoint (step: 30200, epoch: 0, block: 30924800) Saved\n",
      "Step Documented\n",
      "23:18:20\n",
      "30201,3.3658\n",
      "30202,3.3770\n",
      "30203,3.3533\n",
      "30204,3.3560\n",
      "30205,3.3785\n",
      "30206,3.3702\n",
      "30207,3.3905\n",
      "30208,3.3902\n",
      "30209,3.3537\n",
      "30210,3.3999\n",
      "30211,3.3923\n",
      "30212,3.3726\n",
      "30213,3.3674\n",
      "30214,3.3751\n",
      "30215,3.3489\n",
      "30216,3.3779\n",
      "30217,3.3665\n",
      "30218,3.3708\n",
      "30219,3.3766\n",
      "30220,3.3744\n",
      "30221,3.3623\n",
      "30222,3.3533\n",
      "30223,3.3918\n",
      "30224,3.3492\n",
      "30225,3.3531\n",
      "30226,3.3784\n",
      "30227,3.3953\n",
      "30228,3.3452\n",
      "30229,3.3701\n",
      "30230,3.3633\n",
      "30231,3.3824\n",
      "30232,3.3710\n",
      "30233,3.3964\n",
      "30234,3.4038\n",
      "30235,3.3681\n",
      "30236,3.3929\n",
      "30237,3.4081\n",
      "30238,3.3960\n",
      "30239,3.3735\n",
      "30240,3.3738\n",
      "30241,3.3604\n",
      "30242,3.3708\n",
      "30243,3.3902\n",
      "30244,3.3928\n",
      "30245,3.3884\n",
      "30246,3.3585\n",
      "30247,3.3757\n",
      "30248,3.3867\n",
      "30249,3.3566\n",
      "30250,3.3821\n",
      "30251,3.3617\n",
      "30252,3.3825\n",
      "30253,3.3863\n",
      "30254,3.3490\n",
      "30255,3.3551\n",
      "30256,3.3421\n",
      "30257,3.3760\n",
      "30258,3.3737\n",
      "30259,3.3662\n",
      "30260,3.3334\n",
      "30261,3.3608\n",
      "30262,3.3531\n",
      "30263,3.4015\n",
      "30264,3.3846\n",
      "30265,3.3593\n",
      "30266,3.3949\n",
      "30267,3.3962\n",
      "30268,3.3684\n",
      "30269,3.3414\n",
      "30270,3.3902\n",
      "30271,3.3946\n",
      "30272,3.3662\n",
      "30273,3.3572\n",
      "30274,3.3449\n",
      "30275,3.3598\n",
      "30276,3.3815\n",
      "30277,3.3701\n",
      "30278,3.3537\n",
      "30279,3.3538\n",
      "30280,3.3886\n",
      "30281,3.3509\n",
      "30282,3.3863\n",
      "30283,3.3797\n",
      "30284,3.3774\n",
      "30285,3.3696\n",
      "30286,3.3586\n",
      "30287,3.3590\n",
      "30288,3.3820\n",
      "30289,3.3815\n",
      "30290,3.3778\n",
      "30291,3.3748\n",
      "30292,3.3511\n",
      "30293,3.3755\n",
      "30294,3.3955\n",
      "30295,3.3766\n",
      "30296,3.3932\n",
      "30297,3.3688\n",
      "30298,3.3892\n",
      "30299,3.3636\n",
      "30300,3.3452\n",
      "(step: 30300, epoch: 0, block: 31027200), Train Loss: 3.2909, Val Loss: 3.3377\n",
      "Learning rate: 5.61957669e-05\n",
      "Scheduler step: 30300\n",
      "Checkpoint (step: 30300, epoch: 0, block: 31027200) Saved\n",
      "Step Documented\n",
      "23:44:39\n",
      "30301,3.3559\n",
      "30302,3.3315\n",
      "30303,3.4164\n",
      "30304,3.3960\n",
      "30305,3.3643\n",
      "30306,3.3421\n",
      "30307,3.3571\n",
      "30308,3.3705\n",
      "30309,3.3878\n",
      "30310,3.3493\n",
      "30311,3.3783\n",
      "30312,3.3970\n",
      "30313,3.3844\n",
      "30314,3.3747\n",
      "30315,3.3948\n",
      "30316,3.3595\n",
      "30317,3.3998\n",
      "30318,3.3834\n",
      "30319,3.3770\n",
      "30320,3.3485\n",
      "30321,3.3812\n",
      "30322,3.4058\n",
      "30323,3.3918\n",
      "30324,3.3933\n",
      "30325,3.3787\n",
      "30326,3.3876\n",
      "30327,3.3516\n",
      "30328,3.3735\n",
      "30329,3.3826\n",
      "30330,3.3710\n",
      "30331,3.3540\n",
      "30332,3.3839\n",
      "30333,3.3587\n",
      "30334,3.3495\n",
      "30335,3.3689\n",
      "30336,3.3669\n",
      "30337,3.3489\n",
      "30338,3.3531\n",
      "30339,3.3613\n",
      "30340,3.3702\n",
      "30341,3.3895\n",
      "30342,3.3603\n",
      "30343,3.3768\n",
      "30344,3.3659\n",
      "30345,3.3455\n",
      "30346,3.3952\n",
      "30347,3.3563\n",
      "30348,3.3416\n",
      "30349,3.3430\n",
      "30350,3.3873\n",
      "30351,3.3756\n",
      "30352,3.3921\n",
      "30353,3.3802\n",
      "30354,3.3953\n",
      "30355,3.3777\n",
      "30356,3.3858\n",
      "30357,3.3522\n",
      "30358,3.3492\n",
      "30359,3.3809\n",
      "30360,3.3691\n",
      "30361,3.3423\n",
      "30362,3.3893\n",
      "30363,3.3732\n",
      "30364,3.3855\n",
      "30365,3.3602\n",
      "30366,3.3946\n",
      "30367,3.3643\n",
      "30368,3.3652\n",
      "30369,3.3802\n",
      "30370,3.3808\n",
      "30371,3.3677\n",
      "30372,3.3495\n",
      "30373,3.3975\n",
      "30374,3.3477\n",
      "30375,3.3927\n",
      "30376,3.3916\n",
      "30377,3.3740\n",
      "30378,3.3713\n",
      "30379,3.3835\n",
      "30380,3.3620\n",
      "30381,3.3642\n",
      "30382,3.3653\n",
      "30383,3.3832\n",
      "30384,3.3543\n",
      "30385,3.3703\n",
      "30386,3.3886\n",
      "30387,3.3382\n",
      "30388,3.3620\n",
      "30389,3.3307\n",
      "30390,3.3773\n",
      "30391,3.3656\n",
      "30392,3.3862\n",
      "30393,3.3747\n",
      "30394,3.3750\n",
      "30395,3.3576\n",
      "30396,3.3574\n",
      "30397,3.3942\n",
      "30398,3.3727\n",
      "30399,3.3774\n",
      "30400,3.3723\n",
      "(step: 30400, epoch: 0, block: 31129600), Train Loss: 3.2971, Val Loss: 3.3034\n",
      "Learning rate: 5.59647825e-05\n",
      "Scheduler step: 30400\n",
      "Checkpoint (step: 30400, epoch: 0, block: 31129600) Saved\n",
      "Step Documented\n",
      "00:11:00\n",
      "30401,3.3733\n",
      "30402,3.3825\n",
      "30403,3.3563\n",
      "30404,3.3621\n",
      "30405,3.3787\n",
      "30406,3.3852\n",
      "30407,3.3848\n",
      "30408,3.3878\n",
      "30409,3.3618\n",
      "30410,3.3650\n",
      "30411,3.3960\n",
      "30412,3.3715\n",
      "30413,3.4098\n",
      "30414,3.3709\n",
      "30415,3.3943\n",
      "30416,3.3573\n",
      "30417,3.3490\n",
      "30418,3.3560\n",
      "30419,3.3967\n",
      "30420,3.3626\n",
      "30421,3.3712\n",
      "30422,3.3775\n",
      "30423,3.3566\n",
      "30424,3.3691\n",
      "30425,3.3882\n",
      "30426,3.4104\n",
      "30427,3.3824\n",
      "30428,3.3554\n",
      "30429,3.3635\n",
      "30430,3.3650\n",
      "30431,3.3661\n",
      "30432,3.3722\n",
      "30433,3.3802\n",
      "30434,3.3327\n",
      "30435,3.3477\n",
      "30436,3.3444\n",
      "30437,3.3682\n",
      "30438,3.4113\n",
      "30439,3.3834\n",
      "30440,3.3808\n",
      "30441,3.3502\n",
      "30442,3.3627\n",
      "30443,3.3554\n",
      "30444,3.4059\n",
      "30445,3.3500\n",
      "30446,3.3605\n",
      "30447,3.3727\n",
      "30448,3.3735\n",
      "30449,3.3499\n",
      "30450,3.3792\n",
      "30451,3.3916\n",
      "30452,3.3647\n",
      "30453,3.3715\n",
      "30454,3.3689\n",
      "30455,3.3593\n",
      "30456,3.3632\n",
      "30457,3.3825\n",
      "30458,3.3767\n",
      "30459,3.3667\n",
      "30460,3.3730\n",
      "30461,3.3766\n",
      "30462,3.3634\n",
      "30463,3.3940\n",
      "30464,3.3823\n",
      "30465,3.3592\n",
      "30466,3.3259\n",
      "30467,3.3928\n",
      "30468,3.3685\n",
      "30469,3.3581\n",
      "30470,3.3654\n",
      "30471,3.4087\n",
      "30472,3.3842\n",
      "30473,3.3770\n",
      "30474,3.3935\n",
      "30475,3.3594\n",
      "30476,3.3562\n",
      "30477,3.3529\n",
      "30478,3.3863\n",
      "30479,3.3825\n",
      "30480,3.3575\n",
      "30481,3.4042\n",
      "30482,3.3858\n",
      "30483,3.3929\n",
      "30484,3.3791\n",
      "30485,3.3690\n",
      "30486,3.3791\n",
      "30487,3.3732\n",
      "30488,3.3953\n",
      "30489,3.3743\n",
      "30490,3.3833\n",
      "30491,3.3505\n",
      "30492,3.3912\n",
      "30493,3.3796\n",
      "30494,3.3475\n",
      "30495,3.3617\n",
      "30496,3.3823\n",
      "30497,3.3605\n",
      "30498,3.3633\n",
      "30499,3.3298\n",
      "30500,3.3475\n",
      "(step: 30500, epoch: 0, block: 31232000), Train Loss: 3.2869, Val Loss: 3.3163\n",
      "Learning rate: 5.57337484e-05\n",
      "Scheduler step: 30500\n",
      "Checkpoint (step: 30500, epoch: 0, block: 31232000) Saved\n",
      "Step Documented\n",
      "00:37:19\n",
      "30501,3.3584\n",
      "30502,3.3578\n",
      "30503,3.3940\n",
      "30504,3.3537\n",
      "30505,3.3639\n",
      "30506,3.3971\n",
      "30507,3.3666\n",
      "30508,3.3688\n",
      "30509,3.3806\n",
      "30510,3.3841\n",
      "30511,3.3539\n",
      "30512,3.4153\n",
      "30513,3.3789\n",
      "30514,3.3869\n",
      "30515,3.3428\n",
      "30516,3.3795\n",
      "30517,3.3510\n",
      "30518,3.3745\n",
      "30519,3.3857\n",
      "30520,3.3703\n",
      "30521,3.3741\n",
      "30522,3.3696\n",
      "30523,3.3828\n",
      "30524,3.3775\n",
      "30525,3.3625\n",
      "30526,3.3845\n",
      "30527,3.3433\n",
      "30528,3.3898\n",
      "30529,3.3478\n",
      "30530,3.3638\n",
      "30531,3.3654\n",
      "30532,3.4022\n",
      "30533,3.3536\n",
      "30534,3.3642\n",
      "30535,3.3863\n",
      "30536,3.3718\n",
      "30537,3.3656\n",
      "30538,3.3688\n",
      "30539,3.3644\n",
      "30540,3.3771\n",
      "30541,3.3503\n",
      "30542,3.3806\n",
      "30543,3.3751\n",
      "30544,3.3610\n",
      "30545,3.3690\n",
      "30546,3.3507\n",
      "30547,3.3760\n",
      "30548,3.3719\n",
      "30549,3.3706\n",
      "30550,3.3512\n",
      "30551,3.3725\n",
      "30552,3.3601\n",
      "30553,3.3403\n",
      "30554,3.4290\n",
      "30555,3.3744\n",
      "30556,3.3645\n",
      "30557,3.3778\n",
      "30558,3.3854\n",
      "30559,3.3548\n",
      "30560,3.3616\n",
      "30561,3.3963\n",
      "30562,3.3626\n",
      "30563,3.3940\n",
      "30564,3.3680\n",
      "30565,3.3578\n",
      "30566,3.3758\n",
      "30567,3.3770\n",
      "30568,3.3750\n",
      "30569,3.3743\n",
      "30570,3.3850\n",
      "30571,3.3615\n",
      "30572,3.3616\n",
      "30573,3.3775\n",
      "30574,3.3339\n",
      "30575,3.3841\n",
      "30576,3.3682\n",
      "30577,3.3693\n",
      "30578,3.3829\n",
      "30579,3.3980\n",
      "30580,3.3726\n",
      "30581,3.3764\n",
      "30582,3.3564\n",
      "30583,3.3948\n",
      "30584,3.3807\n",
      "30585,3.3887\n",
      "30586,3.3681\n",
      "30587,3.3830\n",
      "30588,3.3583\n",
      "30589,3.3741\n",
      "30590,3.3796\n",
      "30591,3.3827\n",
      "30592,3.3686\n",
      "30593,3.3965\n",
      "30594,3.3653\n",
      "30595,3.3572\n",
      "30596,3.3632\n",
      "30597,3.3859\n",
      "30598,3.3783\n",
      "30599,3.3746\n",
      "30600,3.3765\n",
      "(step: 30600, epoch: 0, block: 31334400), Train Loss: 3.2760, Val Loss: 3.3174\n",
      "Learning rate: 5.55026705e-05\n",
      "Scheduler step: 30600\n",
      "Checkpoint (step: 30600, epoch: 0, block: 31334400) Saved\n",
      "Step Documented\n",
      "01:03:38\n",
      "30601,3.3623\n",
      "30602,3.3876\n",
      "30603,3.3687\n",
      "30604,3.3815\n",
      "30605,3.3693\n",
      "30606,3.3662\n",
      "30607,3.3435\n",
      "30608,3.3888\n",
      "30609,3.3577\n",
      "30610,3.3468\n",
      "30611,3.4024\n",
      "30612,3.3767\n",
      "30613,3.3534\n",
      "30614,3.3672\n",
      "30615,3.3587\n",
      "30616,3.3713\n",
      "30617,3.3652\n",
      "30618,3.3615\n",
      "30619,3.3765\n",
      "30620,3.3603\n",
      "30621,3.4016\n",
      "30622,3.3457\n",
      "30623,3.3599\n",
      "30624,3.3520\n",
      "30625,3.3487\n",
      "30626,3.4091\n",
      "30627,3.3410\n",
      "30628,3.3848\n",
      "30629,3.3789\n",
      "30630,3.3762\n",
      "30631,3.3634\n",
      "30632,3.3594\n",
      "30633,3.3830\n",
      "30634,3.3664\n",
      "30635,3.3403\n",
      "30636,3.3802\n",
      "30637,3.3778\n",
      "30638,3.3833\n",
      "30639,3.3939\n",
      "30640,3.3782\n",
      "30641,3.3762\n",
      "30642,3.3685\n",
      "30643,3.3751\n",
      "30644,3.3698\n",
      "30645,3.3486\n",
      "30646,3.3749\n",
      "30647,3.3768\n",
      "30648,3.3666\n",
      "30649,3.3716\n",
      "30650,3.3629\n",
      "30651,3.3984\n",
      "30652,3.3522\n",
      "30653,3.3498\n",
      "30654,3.3818\n",
      "30655,3.3663\n",
      "30656,3.3870\n",
      "30657,3.3878\n",
      "30658,3.3556\n",
      "30659,3.3485\n",
      "30660,3.3692\n",
      "30661,3.3811\n",
      "30662,3.3699\n",
      "30663,3.3480\n",
      "30664,3.3491\n",
      "30665,3.3833\n",
      "30666,3.3363\n",
      "30667,3.4063\n",
      "30668,3.3763\n",
      "30669,3.3800\n",
      "30670,3.3296\n",
      "30671,3.3638\n",
      "30672,3.3973\n",
      "30673,3.3841\n",
      "30674,3.3619\n",
      "30675,3.3467\n",
      "30676,3.3595\n",
      "30677,3.3744\n",
      "30678,3.3811\n",
      "30679,3.3752\n",
      "30680,3.3741\n",
      "30681,3.3664\n",
      "30682,3.3621\n",
      "30683,3.4016\n",
      "30684,3.3962\n",
      "30685,3.3590\n",
      "30686,3.3579\n",
      "30687,3.3517\n",
      "30688,3.3870\n",
      "30689,3.3789\n",
      "30690,3.3932\n",
      "30691,3.3410\n",
      "30692,3.3555\n",
      "30693,3.3595\n",
      "30694,3.3590\n",
      "30695,3.3645\n",
      "30696,3.3538\n",
      "30697,3.3743\n",
      "30698,3.3771\n",
      "30699,3.3925\n",
      "30700,3.3590\n",
      "(step: 30700, epoch: 0, block: 31436800), Train Loss: 3.2977, Val Loss: 3.2843\n",
      "Learning rate: 5.52715547e-05\n",
      "Scheduler step: 30700\n",
      "Checkpoint (step: 30700, epoch: 0, block: 31436800) Saved\n",
      "Step Documented\n",
      "01:29:52\n",
      "30701,3.3661\n",
      "30702,3.3674\n",
      "30703,3.3755\n",
      "30704,3.3860\n",
      "30705,3.3563\n",
      "30706,3.3796\n",
      "30707,3.3566\n",
      "30708,3.3663\n",
      "30709,3.3623\n",
      "30710,3.3349\n",
      "30711,3.3693\n",
      "30712,3.3600\n",
      "30713,3.3486\n",
      "30714,3.3340\n",
      "30715,3.3823\n",
      "30716,3.3789\n",
      "30717,3.3777\n",
      "30718,3.3642\n",
      "30719,3.3546\n",
      "30720,3.3530\n",
      "30721,3.3920\n",
      "30722,3.3856\n",
      "30723,3.4162\n",
      "30724,3.3499\n",
      "30725,3.3944\n",
      "30726,3.3678\n",
      "30727,3.4083\n",
      "30728,3.3944\n",
      "30729,3.3741\n",
      "30730,3.3549\n",
      "30731,3.3781\n",
      "30732,3.3686\n",
      "30733,3.3748\n",
      "30734,3.3574\n",
      "30735,3.3850\n",
      "30736,3.3644\n",
      "30737,3.3541\n",
      "30738,3.3625\n",
      "30739,3.3750\n",
      "30740,3.3533\n",
      "30741,3.3600\n",
      "30742,3.3480\n",
      "30743,3.3573\n",
      "30744,3.3509\n",
      "30745,3.3887\n",
      "30746,3.3505\n",
      "30747,3.3526\n",
      "30748,3.3835\n",
      "30749,3.3638\n",
      "30750,3.3500\n",
      "30751,3.3799\n",
      "30752,3.3821\n",
      "30753,3.3357\n",
      "30754,3.3879\n",
      "30755,3.3814\n",
      "30756,3.3701\n",
      "30757,3.3805\n",
      "30758,3.3379\n",
      "30759,3.3708\n",
      "30760,3.3762\n",
      "30761,3.3785\n",
      "30762,3.3728\n",
      "30763,3.3772\n",
      "30764,3.3681\n",
      "30765,3.3524\n",
      "30766,3.3933\n",
      "30767,3.3848\n",
      "30768,3.4032\n",
      "30769,3.3582\n",
      "30770,3.3834\n",
      "30771,3.3503\n",
      "30772,3.3504\n",
      "30773,3.3873\n",
      "30774,3.3754\n",
      "30775,3.3831\n",
      "30776,3.4028\n",
      "30777,3.3911\n",
      "30778,3.3639\n",
      "30779,3.3682\n",
      "30780,3.4055\n",
      "30781,3.3815\n",
      "30782,3.3826\n",
      "30783,3.3582\n",
      "30784,3.3680\n",
      "30785,3.3580\n",
      "30786,3.3683\n",
      "30787,3.4020\n",
      "30788,3.3566\n",
      "30789,3.4054\n",
      "30790,3.3758\n",
      "30791,3.3741\n",
      "30792,3.3441\n",
      "30793,3.3858\n",
      "30794,3.3800\n",
      "30795,3.3696\n",
      "30796,3.3752\n",
      "30797,3.3545\n",
      "30798,3.3513\n",
      "30799,3.3620\n",
      "30800,3.3780\n",
      "(step: 30800, epoch: 0, block: 31539200), Train Loss: 3.3243, Val Loss: 3.3193\n",
      "Learning rate: 5.50404068e-05\n",
      "Scheduler step: 30800\n",
      "Checkpoint (step: 30800, epoch: 0, block: 31539200) Saved\n",
      "Step Documented\n",
      "01:56:06\n",
      "30801,3.3397\n",
      "30802,3.3583\n",
      "30803,3.3924\n",
      "30804,3.3414\n",
      "30805,3.3833\n",
      "30806,3.3735\n",
      "30807,3.3618\n",
      "30808,3.3645\n",
      "30809,3.3335\n",
      "30810,3.4053\n",
      "30811,3.3668\n",
      "30812,3.3488\n",
      "30813,3.3664\n",
      "30814,3.3862\n",
      "30815,3.3805\n",
      "30816,3.4033\n",
      "30817,3.3380\n",
      "30818,3.3605\n",
      "30819,3.3884\n",
      "30820,3.3686\n",
      "30821,3.3754\n",
      "30822,3.3612\n",
      "30823,3.3573\n",
      "30824,3.3607\n",
      "30825,3.3845\n",
      "30826,3.3552\n",
      "30827,3.3695\n",
      "30828,3.3712\n",
      "30829,3.3492\n",
      "30830,3.3838\n",
      "30831,3.3690\n",
      "30832,3.3559\n",
      "30833,3.3705\n",
      "30834,3.3906\n",
      "30835,3.3807\n",
      "30836,3.3756\n",
      "30837,3.3743\n",
      "30838,3.3710\n",
      "30839,3.3733\n",
      "30840,3.3529\n",
      "30841,3.3900\n",
      "30842,3.3373\n",
      "30843,3.3691\n",
      "30844,3.3616\n",
      "30845,3.3650\n",
      "30846,3.3532\n",
      "30847,3.3524\n",
      "30848,3.3607\n",
      "30849,3.3510\n",
      "30850,3.3745\n",
      "30851,3.3553\n",
      "30852,3.3658\n",
      "30853,3.3881\n",
      "30854,3.3991\n",
      "30855,3.3783\n",
      "30856,3.3568\n",
      "30857,3.3622\n",
      "30858,3.3839\n",
      "30859,3.3394\n",
      "30860,3.3519\n",
      "30861,3.3881\n",
      "30862,3.3638\n",
      "30863,3.3669\n",
      "30864,3.3524\n",
      "30865,3.3706\n",
      "30866,3.3617\n",
      "30867,3.3665\n",
      "30868,3.3716\n",
      "30869,3.3616\n",
      "30870,3.3750\n",
      "30871,3.3791\n",
      "30872,3.3947\n",
      "30873,3.3507\n",
      "30874,3.3523\n",
      "30875,3.3807\n",
      "30876,3.3826\n",
      "30877,3.3342\n",
      "30878,3.3587\n",
      "30879,3.3686\n",
      "30880,3.3715\n",
      "30881,3.3734\n",
      "30882,3.3722\n",
      "30883,3.3646\n",
      "30884,3.3877\n",
      "30885,3.3879\n",
      "30886,3.3578\n",
      "30887,3.3799\n",
      "30888,3.3611\n",
      "30889,3.3512\n",
      "30890,3.3746\n",
      "30891,3.3762\n",
      "30892,3.3618\n",
      "30893,3.3746\n",
      "30894,3.3615\n",
      "30895,3.3681\n",
      "30896,3.3664\n",
      "30897,3.3591\n",
      "30898,3.3408\n",
      "30899,3.3547\n",
      "30900,3.3840\n",
      "(step: 30900, epoch: 0, block: 31641600), Train Loss: 3.2775, Val Loss: 3.3182\n",
      "Learning rate: 5.48092325e-05\n",
      "Scheduler step: 30900\n",
      "Checkpoint (step: 30900, epoch: 0, block: 31641600) Saved\n",
      "Step Documented\n",
      "02:22:26\n",
      "30901,3.3367\n",
      "30902,3.3585\n",
      "30903,3.3731\n",
      "30904,3.3884\n",
      "30905,3.3675\n",
      "30906,3.3322\n",
      "30907,3.3690\n",
      "30908,3.3937\n",
      "30909,3.3600\n",
      "30910,3.3812\n",
      "30911,3.3779\n",
      "30912,3.3571\n",
      "30913,3.3709\n",
      "30914,3.3504\n",
      "30915,3.3418\n",
      "30916,3.3748\n",
      "30917,3.3562\n",
      "30918,3.3631\n",
      "30919,3.3495\n",
      "30920,3.3498\n",
      "30921,3.3472\n",
      "30922,3.3797\n",
      "30923,3.3647\n",
      "30924,3.3708\n",
      "30925,3.3704\n",
      "30926,3.3690\n",
      "30927,3.3652\n",
      "30928,3.3935\n",
      "30929,3.3490\n",
      "30930,3.3549\n",
      "30931,3.3739\n",
      "30932,3.3899\n",
      "30933,3.3764\n",
      "30934,3.3410\n",
      "30935,3.3585\n",
      "30936,3.3774\n",
      "30937,3.3499\n",
      "30938,3.3869\n",
      "30939,3.3926\n",
      "30940,3.3588\n",
      "30941,3.3571\n",
      "30942,3.3580\n",
      "30943,3.3659\n",
      "30944,3.3803\n",
      "30945,3.4085\n",
      "30946,3.3759\n",
      "30947,3.3769\n",
      "30948,3.3439\n",
      "30949,3.3665\n",
      "30950,3.3498\n",
      "30951,3.3690\n",
      "30952,3.3367\n",
      "30953,3.3808\n",
      "30954,3.3392\n",
      "30955,3.3759\n",
      "30956,3.3723\n",
      "30957,3.3527\n",
      "30958,3.3517\n",
      "30959,3.3690\n",
      "30960,3.3918\n",
      "30961,3.3605\n",
      "30962,3.3915\n",
      "30963,3.3827\n",
      "30964,3.3852\n",
      "30965,3.3762\n",
      "30966,3.3896\n",
      "30967,3.3370\n",
      "30968,3.3768\n",
      "30969,3.3837\n",
      "30970,3.3933\n",
      "30971,3.3988\n",
      "30972,3.3737\n",
      "30973,3.3694\n",
      "30974,3.3664\n",
      "30975,3.3648\n",
      "30976,3.3890\n",
      "30977,3.3812\n",
      "30978,3.3840\n",
      "30979,3.3802\n",
      "30980,3.3686\n",
      "30981,3.3516\n",
      "30982,3.3615\n",
      "30983,3.3607\n",
      "30984,3.3823\n",
      "30985,3.3699\n",
      "30986,3.3630\n",
      "30987,3.3488\n",
      "30988,3.3792\n",
      "30989,3.3477\n",
      "30990,3.3499\n",
      "30991,3.3882\n",
      "30992,3.3801\n",
      "30993,3.3591\n",
      "30994,3.3746\n",
      "30995,3.3870\n",
      "30996,3.3538\n",
      "30997,3.3552\n",
      "30998,3.3665\n",
      "30999,3.3830\n",
      "31000,3.3900\n",
      "(step: 31000, epoch: 0, block: 31744000), Train Loss: 3.2772, Val Loss: 3.3090\n",
      "Learning rate: 5.45780378e-05\n",
      "Scheduler step: 31000\n",
      "Checkpoint (step: 31000, epoch: 0, block: 31744000) Saved\n",
      "Step Documented\n",
      "02:48:40\n",
      "31001,3.3665\n",
      "31002,3.3746\n",
      "31003,3.3809\n",
      "31004,3.3619\n",
      "31005,3.3757\n",
      "31006,3.3469\n",
      "31007,3.3718\n",
      "31008,3.3770\n",
      "31009,3.3999\n",
      "31010,3.3391\n",
      "31011,3.3454\n",
      "31012,3.3960\n",
      "31013,3.3785\n",
      "31014,3.3625\n",
      "31015,3.3477\n",
      "31016,3.3344\n",
      "31017,3.3889\n",
      "31018,3.3729\n",
      "31019,3.3639\n",
      "31020,3.3856\n",
      "31021,3.3552\n",
      "31022,3.3909\n",
      "31023,3.3437\n",
      "31024,3.3572\n",
      "31025,3.3787\n",
      "31026,3.3790\n",
      "31027,3.3551\n",
      "31028,3.3708\n",
      "31029,3.3634\n",
      "31030,3.3643\n",
      "31031,3.3947\n",
      "31032,3.3414\n",
      "31033,3.3413\n",
      "31034,3.3536\n",
      "31035,3.3524\n",
      "31036,3.3811\n",
      "31037,3.3726\n",
      "31038,3.4091\n",
      "31039,3.3918\n",
      "31040,3.3551\n",
      "31041,3.3787\n",
      "31042,3.3666\n",
      "31043,3.3771\n",
      "31044,3.3639\n",
      "31045,3.3677\n",
      "31046,3.3976\n",
      "31047,3.3432\n",
      "31048,3.3749\n",
      "31049,3.3537\n",
      "31050,3.3486\n",
      "31051,3.3676\n",
      "31052,3.3486\n",
      "31053,3.3765\n",
      "31054,3.3766\n",
      "31055,3.3787\n",
      "31056,3.3472\n",
      "31057,3.4010\n",
      "31058,3.3793\n",
      "31059,3.3715\n",
      "31060,3.3682\n",
      "31061,3.3723\n",
      "31062,3.3482\n",
      "31063,3.3757\n",
      "31064,3.3836\n",
      "31065,3.3568\n",
      "31066,3.3580\n",
      "31067,3.3610\n",
      "31068,3.3656\n",
      "31069,3.3519\n",
      "31070,3.4005\n",
      "31071,3.3585\n",
      "31072,3.3810\n",
      "31073,3.3393\n",
      "31074,3.3697\n",
      "31075,3.3853\n",
      "31076,3.3838\n",
      "31077,3.3644\n",
      "31078,3.3573\n",
      "31079,3.3601\n",
      "31080,3.3630\n",
      "31081,3.4020\n",
      "31082,3.3641\n",
      "31083,3.3724\n",
      "31084,3.3661\n",
      "31085,3.3714\n",
      "31086,3.3659\n",
      "31087,3.3657\n",
      "31088,3.3702\n",
      "31089,3.3562\n",
      "31090,3.3636\n",
      "31091,3.3699\n",
      "31092,3.3707\n",
      "31093,3.3624\n",
      "31094,3.3638\n",
      "31095,3.3721\n",
      "31096,3.3668\n",
      "31097,3.3723\n",
      "31098,3.3575\n",
      "31099,3.4014\n",
      "31100,3.4082\n",
      "(step: 31100, epoch: 0, block: 31846400), Train Loss: 3.3125, Val Loss: 3.3168\n",
      "Learning rate: 5.43468285e-05\n",
      "Scheduler step: 31100\n",
      "Checkpoint (step: 31100, epoch: 0, block: 31846400) Saved\n",
      "Step Documented\n",
      "03:15:01\n",
      "31101,3.3951\n",
      "31102,3.3664\n",
      "31103,3.3556\n",
      "31104,3.3566\n",
      "31105,3.3729\n",
      "31106,3.3725\n",
      "31107,3.3750\n",
      "31108,3.3879\n",
      "31109,3.3923\n",
      "31110,3.3828\n",
      "31111,3.3627\n",
      "31112,3.3974\n",
      "31113,3.3803\n",
      "31114,3.4043\n",
      "31115,3.3597\n",
      "31116,3.3795\n",
      "31117,3.3964\n",
      "31118,3.3997\n",
      "31119,3.3793\n",
      "31120,3.3800\n",
      "31121,3.3683\n",
      "31122,3.3721\n",
      "31123,3.3729\n",
      "31124,3.3647\n",
      "31125,3.3836\n",
      "31126,3.3744\n",
      "31127,3.3491\n",
      "31128,3.3499\n",
      "31129,3.3749\n",
      "31130,3.3483\n",
      "31131,3.3803\n",
      "31132,3.3579\n",
      "31133,3.3366\n",
      "31134,3.3757\n",
      "31135,3.3652\n",
      "31136,3.3853\n",
      "31137,3.3611\n",
      "31138,3.3670\n",
      "31139,3.3515\n",
      "31140,3.3829\n",
      "31141,3.3642\n",
      "31142,3.3581\n",
      "31143,3.3426\n",
      "31144,3.3890\n",
      "31145,3.3594\n",
      "31146,3.3755\n",
      "31147,3.3737\n",
      "31148,3.3471\n",
      "31149,3.3540\n",
      "31150,3.3372\n",
      "31151,3.3612\n",
      "31152,3.4003\n",
      "31153,3.3936\n",
      "31154,3.3897\n",
      "31155,3.3745\n",
      "31156,3.3598\n",
      "31157,3.3958\n",
      "31158,3.3493\n",
      "31159,3.3551\n",
      "31160,3.3570\n",
      "31161,3.3852\n",
      "31162,3.3743\n",
      "31163,3.3660\n",
      "31164,3.3692\n",
      "31165,3.3876\n",
      "31166,3.3340\n",
      "31167,3.3895\n",
      "31168,3.3735\n",
      "31169,3.3541\n",
      "31170,3.3790\n",
      "31171,3.3677\n",
      "31172,3.3645\n",
      "31173,3.3737\n",
      "31174,3.3976\n",
      "31175,3.3784\n",
      "31176,3.3667\n",
      "31177,3.3625\n",
      "31178,3.3574\n",
      "31179,3.3637\n",
      "31180,3.3731\n",
      "31181,3.3492\n",
      "31182,3.3651\n",
      "31183,3.3690\n",
      "31184,3.3693\n",
      "31185,3.3753\n",
      "31186,3.3809\n",
      "31187,3.3494\n",
      "31188,3.3484\n",
      "31189,3.3486\n",
      "31190,3.3543\n",
      "31191,3.3546\n",
      "31192,3.3931\n",
      "31193,3.3594\n",
      "31194,3.3598\n",
      "31195,3.3545\n",
      "31196,3.3634\n",
      "31197,3.3967\n",
      "31198,3.3426\n",
      "31199,3.3721\n",
      "31200,3.3538\n",
      "(step: 31200, epoch: 0, block: 31948800), Train Loss: 3.2772, Val Loss: 3.2903\n",
      "Learning rate: 5.41156105e-05\n",
      "Scheduler step: 31200\n",
      "Checkpoint (step: 31200, epoch: 0, block: 31948800) Saved\n",
      "Step Documented\n",
      "03:41:20\n",
      "31201,3.3553\n",
      "31202,3.3771\n",
      "31203,3.3832\n",
      "31204,3.3553\n",
      "31205,3.3476\n",
      "31206,3.3838\n",
      "31207,3.3467\n",
      "31208,3.3333\n",
      "31209,3.3649\n",
      "31210,3.3860\n",
      "31211,3.3472\n",
      "31212,3.3270\n",
      "31213,3.3687\n",
      "31214,3.4033\n",
      "31215,3.3576\n",
      "31216,3.3761\n",
      "31217,3.3620\n",
      "31218,3.3732\n",
      "31219,3.3773\n",
      "31220,3.3827\n",
      "31221,3.3524\n",
      "31222,3.3641\n",
      "31223,3.3973\n",
      "31224,3.3624\n",
      "31225,3.3762\n",
      "31226,3.3864\n",
      "31227,3.3782\n",
      "31228,3.3853\n",
      "31229,3.3322\n",
      "31230,3.3876\n",
      "31231,3.3689\n",
      "31232,3.3838\n",
      "31233,3.3973\n",
      "31234,3.3507\n",
      "31235,3.3563\n",
      "31236,3.4077\n",
      "31237,3.3501\n",
      "31238,3.3613\n",
      "31239,3.3853\n",
      "31240,3.3940\n",
      "31241,3.3620\n",
      "31242,3.3787\n",
      "31243,3.3708\n",
      "31244,3.3872\n",
      "31245,3.3853\n",
      "31246,3.3664\n",
      "31247,3.3726\n",
      "31248,3.3444\n",
      "31249,3.3702\n",
      "31250,3.4022\n",
      "31251,3.3625\n",
      "31252,3.3811\n",
      "31253,3.3427\n",
      "31254,3.3662\n",
      "31255,3.3819\n",
      "31256,3.3958\n",
      "31257,3.3434\n",
      "31258,3.3693\n",
      "31259,3.3667\n",
      "31260,3.3592\n",
      "31261,3.3533\n",
      "31262,3.3775\n",
      "31263,3.3524\n",
      "31264,3.3899\n",
      "31265,3.3946\n",
      "31266,3.3506\n",
      "31267,3.3343\n",
      "31268,3.3657\n",
      "31269,3.3740\n",
      "31270,3.3633\n",
      "31271,3.3694\n",
      "31272,3.3953\n",
      "31273,3.3520\n",
      "31274,3.3708\n",
      "31275,3.3757\n",
      "31276,3.3690\n",
      "31277,3.3745\n",
      "31278,3.3596\n",
      "31279,3.3728\n",
      "31280,3.3708\n",
      "31281,3.3572\n",
      "31282,3.3580\n",
      "31283,3.3793\n",
      "31284,3.3754\n",
      "31285,3.3866\n",
      "31286,3.3889\n",
      "31287,3.3717\n",
      "31288,3.3648\n",
      "31289,3.3534\n",
      "31290,3.3486\n",
      "31291,3.3885\n",
      "31292,3.3978\n",
      "31293,3.3689\n",
      "31294,3.3672\n",
      "31295,3.3722\n",
      "31296,3.3551\n",
      "31297,3.3584\n",
      "31298,3.3673\n",
      "31299,3.3464\n",
      "31300,3.3837\n",
      "(step: 31300, epoch: 0, block: 32051200), Train Loss: 3.2810, Val Loss: 3.2958\n",
      "Learning rate: 5.38843895e-05\n",
      "Scheduler step: 31300\n",
      "Checkpoint (step: 31300, epoch: 0, block: 32051200) Saved\n",
      "Step Documented\n",
      "04:07:41\n",
      "31301,3.3684\n",
      "31302,3.3781\n",
      "31303,3.3714\n",
      "31304,3.3838\n",
      "31305,3.3717\n",
      "31306,3.3680\n",
      "31307,3.3744\n",
      "31308,3.3440\n",
      "31309,3.3913\n",
      "31310,3.3458\n",
      "31311,3.3666\n",
      "31312,3.3360\n",
      "31313,3.3717\n",
      "31314,3.3551\n",
      "31315,3.3613\n",
      "31316,3.3823\n",
      "31317,3.3851\n",
      "31318,3.3697\n",
      "31319,3.3675\n",
      "31320,3.3857\n",
      "31321,3.3672\n",
      "31322,3.3642\n",
      "31323,3.3378\n",
      "31324,3.3599\n",
      "31325,3.3497\n",
      "31326,3.3753\n",
      "31327,3.3906\n",
      "31328,3.3795\n",
      "31329,3.3786\n",
      "31330,3.3483\n",
      "31331,3.3849\n",
      "31332,3.3905\n",
      "31333,3.3580\n",
      "31334,3.3562\n",
      "31335,3.3521\n",
      "31336,3.3622\n",
      "31337,3.3419\n",
      "31338,3.3627\n",
      "31339,3.3551\n",
      "31340,3.3220\n",
      "31341,3.3904\n",
      "31342,3.3789\n",
      "31343,3.3530\n",
      "31344,3.3543\n",
      "31345,3.3799\n",
      "31346,3.3450\n",
      "31347,3.3423\n",
      "31348,3.3750\n",
      "31349,3.3645\n",
      "31350,3.3462\n",
      "31351,3.3953\n",
      "31352,3.3739\n",
      "31353,3.3622\n",
      "31354,3.4021\n",
      "31355,3.3667\n",
      "31356,3.3467\n",
      "31357,3.3472\n",
      "31358,3.3451\n",
      "31359,3.3698\n",
      "31360,3.3568\n",
      "31361,3.3518\n",
      "31362,3.3650\n",
      "31363,3.3629\n",
      "31364,3.3773\n",
      "31365,3.3670\n",
      "31366,3.3614\n",
      "31367,3.3780\n",
      "31368,3.3381\n",
      "31369,3.3619\n",
      "31370,3.3611\n",
      "31371,3.3850\n",
      "31372,3.3455\n",
      "31373,3.4007\n",
      "31374,3.3551\n",
      "31375,3.3764\n",
      "31376,3.3571\n",
      "31377,3.3777\n",
      "31378,3.3443\n",
      "31379,3.3783\n",
      "31380,3.3771\n",
      "31381,3.3626\n",
      "31382,3.3813\n",
      "31383,3.3824\n",
      "31384,3.3699\n",
      "31385,3.3802\n",
      "31386,3.3595\n",
      "31387,3.3654\n",
      "31388,3.3645\n",
      "31389,3.3716\n",
      "31390,3.3858\n",
      "31391,3.3972\n",
      "31392,3.3709\n",
      "31393,3.3632\n",
      "31394,3.3648\n",
      "31395,3.3384\n",
      "31396,3.3636\n",
      "31397,3.3782\n",
      "31398,3.3633\n",
      "31399,3.3621\n",
      "31400,3.3849\n",
      "(step: 31400, epoch: 0, block: 32153600), Train Loss: 3.3022, Val Loss: 3.3227\n",
      "Learning rate: 5.36531715e-05\n",
      "Scheduler step: 31400\n",
      "Checkpoint (step: 31400, epoch: 0, block: 32153600) Saved\n",
      "Step Documented\n",
      "04:34:01\n",
      "31401,3.3625\n",
      "31402,3.4040\n",
      "31403,3.4059\n",
      "31404,3.3454\n",
      "31405,3.3490\n",
      "31406,3.3587\n",
      "31407,3.3590\n",
      "31408,3.3524\n",
      "31409,3.3700\n",
      "31410,3.3511\n",
      "31411,3.3677\n",
      "31412,3.3915\n",
      "31413,3.3638\n",
      "31414,3.3767\n",
      "31415,3.3513\n",
      "31416,3.3338\n",
      "31417,3.3416\n",
      "31418,3.3570\n",
      "31419,3.3746\n",
      "31420,3.3719\n",
      "31421,3.3667\n",
      "31422,3.3718\n",
      "31423,3.3643\n",
      "31424,3.3609\n",
      "31425,3.3821\n",
      "31426,3.3798\n",
      "31427,3.3322\n",
      "31428,3.3687\n",
      "31429,3.3544\n",
      "31430,3.3760\n",
      "31431,3.3628\n",
      "31432,3.3829\n",
      "31433,3.3543\n",
      "31434,3.3704\n",
      "31435,3.3499\n",
      "31436,3.3803\n",
      "31437,3.3752\n",
      "31438,3.3443\n",
      "31439,3.3733\n",
      "31440,3.3958\n",
      "31441,3.3628\n",
      "31442,3.3526\n",
      "31443,3.3570\n",
      "31444,3.3638\n",
      "31445,3.4023\n",
      "31446,3.3681\n",
      "31447,3.3553\n",
      "31448,3.3775\n",
      "31449,3.3479\n",
      "31450,3.3757\n",
      "31451,3.3782\n",
      "31452,3.3530\n",
      "31453,3.3697\n",
      "31454,3.3860\n",
      "31455,3.3767\n",
      "31456,3.3571\n",
      "31457,3.3869\n",
      "31458,3.3666\n",
      "31459,3.3875\n",
      "31460,3.3557\n",
      "31461,3.3831\n",
      "31462,3.3852\n",
      "31463,3.3620\n",
      "31464,3.3311\n",
      "31465,3.3924\n",
      "31466,3.3449\n",
      "31467,3.3655\n",
      "31468,3.3675\n",
      "31469,3.3545\n",
      "31470,3.3609\n",
      "31471,3.3707\n",
      "31472,3.3540\n",
      "31473,3.4139\n",
      "31474,3.3661\n",
      "31475,3.3675\n",
      "31476,3.3913\n",
      "31477,3.3601\n",
      "31478,3.3738\n",
      "31479,3.3560\n",
      "31480,3.3687\n",
      "31481,3.3913\n",
      "31482,3.3603\n",
      "31483,3.3685\n",
      "31484,3.3824\n",
      "31485,3.3879\n",
      "31486,3.3562\n",
      "31487,3.3615\n",
      "31488,3.3705\n",
      "31489,3.3612\n",
      "31490,3.3516\n",
      "31491,3.3341\n",
      "31492,3.3742\n",
      "31493,3.3842\n",
      "31494,3.3813\n",
      "31495,3.3688\n",
      "31496,3.3705\n",
      "31497,3.3788\n",
      "31498,3.3513\n",
      "31499,3.3402\n",
      "31500,3.3531\n",
      "(step: 31500, epoch: 0, block: 32256000), Train Loss: 3.3028, Val Loss: 3.2850\n",
      "Learning rate: 5.34219622e-05\n",
      "Scheduler step: 31500\n",
      "Checkpoint (step: 31500, epoch: 0, block: 32256000) Saved\n",
      "Step Documented\n",
      "05:00:22\n",
      "31501,3.3705\n",
      "31502,3.3494\n",
      "31503,3.3745\n",
      "31504,3.3720\n",
      "31505,3.3584\n",
      "31506,3.3854\n",
      "31507,3.3803\n",
      "31508,3.3694\n",
      "31509,3.3584\n",
      "31510,3.3860\n",
      "31511,3.3703\n",
      "31512,3.3603\n",
      "31513,3.3887\n",
      "31514,3.3305\n",
      "31515,3.3673\n",
      "31516,3.3961\n",
      "31517,3.3624\n",
      "31518,3.3797\n",
      "31519,3.3863\n",
      "31520,3.3646\n",
      "31521,3.3580\n",
      "31522,3.3633\n",
      "31523,3.3900\n",
      "31524,3.3728\n",
      "31525,3.3607\n",
      "31526,3.3439\n",
      "31527,3.4107\n",
      "31528,3.3588\n",
      "31529,3.3699\n",
      "31530,3.3837\n",
      "31531,3.3762\n",
      "31532,3.3706\n",
      "31533,3.3831\n",
      "31534,3.3648\n",
      "31535,3.3873\n",
      "31536,3.3829\n",
      "31537,3.3595\n",
      "31538,3.3627\n",
      "31539,3.3881\n",
      "31540,3.3250\n",
      "31541,3.3643\n",
      "31542,3.3843\n",
      "31543,3.3971\n",
      "31544,3.3786\n",
      "31545,3.3696\n",
      "31546,3.3545\n",
      "31547,3.3676\n",
      "31548,3.3478\n",
      "31549,3.3656\n",
      "31550,3.3710\n",
      "31551,3.3840\n",
      "31552,3.3716\n",
      "31553,3.3598\n",
      "31554,3.3621\n",
      "31555,3.3658\n",
      "31556,3.3626\n",
      "31557,3.3805\n",
      "31558,3.3623\n",
      "31559,3.3755\n",
      "31560,3.3517\n",
      "31561,3.3592\n",
      "31562,3.3688\n",
      "31563,3.3505\n",
      "31564,3.3811\n",
      "31565,3.3710\n",
      "31566,3.3822\n",
      "31567,3.3923\n",
      "31568,3.3725\n",
      "31569,3.3769\n",
      "31570,3.3546\n",
      "31571,3.3662\n",
      "31572,3.3723\n",
      "31573,3.4141\n",
      "31574,3.3614\n",
      "31575,3.3580\n",
      "31576,3.3588\n",
      "31577,3.3738\n",
      "31578,3.3226\n",
      "31579,3.3774\n",
      "31580,3.3559\n",
      "31581,3.3716\n",
      "31582,3.3545\n",
      "31583,3.3640\n",
      "31584,3.3902\n",
      "31585,3.3840\n",
      "31586,3.3599\n",
      "31587,3.3600\n",
      "31588,3.3543\n",
      "31589,3.3789\n",
      "31590,3.3709\n",
      "31591,3.3751\n",
      "31592,3.3754\n",
      "31593,3.3834\n",
      "31594,3.3534\n",
      "31595,3.3705\n",
      "31596,3.3966\n",
      "31597,3.3683\n",
      "31598,3.3736\n",
      "31599,3.3802\n",
      "31600,3.3652\n",
      "(step: 31600, epoch: 0, block: 32358400), Train Loss: 3.2933, Val Loss: 3.2971\n",
      "Learning rate: 5.31907675e-05\n",
      "Scheduler step: 31600\n",
      "Checkpoint (step: 31600, epoch: 0, block: 32358400) Saved\n",
      "Step Documented\n",
      "05:26:42\n",
      "31601,3.3798\n",
      "31602,3.3325\n",
      "31603,3.3392\n",
      "31604,3.3555\n",
      "31605,3.3650\n",
      "31606,3.3830\n",
      "31607,3.4171\n",
      "31608,3.3981\n",
      "31609,3.3940\n",
      "31610,3.3504\n",
      "31611,3.3901\n",
      "31612,3.3525\n",
      "31613,3.3776\n",
      "31614,3.3434\n",
      "31615,3.3672\n",
      "31616,3.3501\n",
      "31617,3.3857\n",
      "31618,3.3628\n",
      "31619,3.3590\n",
      "31620,3.3576\n",
      "31621,3.3709\n",
      "31622,3.3674\n",
      "31623,3.3480\n",
      "31624,3.3652\n",
      "31625,3.3644\n",
      "31626,3.3771\n",
      "31627,3.3966\n",
      "31628,3.3753\n",
      "31629,3.3744\n",
      "31630,3.3609\n",
      "31631,3.3624\n",
      "31632,3.3958\n",
      "31633,3.3573\n",
      "31634,3.3611\n",
      "31635,3.3666\n",
      "31636,3.3471\n",
      "31637,3.3420\n",
      "31638,3.3699\n",
      "31639,3.3870\n",
      "31640,3.3811\n",
      "31641,3.3491\n",
      "31642,3.3793\n",
      "31643,3.3805\n",
      "31644,3.3555\n",
      "31645,3.3293\n",
      "31646,3.4013\n",
      "31647,3.3605\n",
      "31648,3.3645\n",
      "31649,3.3669\n",
      "31650,3.3582\n",
      "31651,3.3580\n",
      "31652,3.3424\n",
      "31653,3.3701\n",
      "31654,3.3550\n",
      "31655,3.3514\n",
      "31656,3.3503\n",
      "31657,3.3412\n",
      "31658,3.3824\n",
      "31659,3.3873\n",
      "31660,3.3748\n",
      "31661,3.3607\n",
      "31662,3.4030\n",
      "31663,3.3415\n",
      "31664,3.3549\n",
      "31665,3.3661\n",
      "31666,3.3717\n",
      "31667,3.3645\n",
      "31668,3.3663\n",
      "31669,3.3671\n",
      "31670,3.3524\n",
      "31671,3.3760\n",
      "31672,3.3727\n",
      "31673,3.3377\n",
      "31674,3.4165\n",
      "31675,3.3574\n",
      "31676,3.3465\n",
      "31677,3.3557\n",
      "31678,3.3753\n",
      "31679,3.3562\n",
      "31680,3.3513\n",
      "31681,3.3683\n",
      "31682,3.3825\n",
      "31683,3.3755\n",
      "31684,3.3853\n",
      "31685,3.3812\n",
      "31686,3.3756\n",
      "31687,3.3695\n",
      "31688,3.3988\n",
      "31689,3.3841\n",
      "31690,3.3711\n",
      "31691,3.3739\n",
      "31692,3.3770\n",
      "31693,3.3929\n",
      "31694,3.3740\n",
      "31695,3.3814\n",
      "31696,3.3487\n",
      "31697,3.3503\n",
      "31698,3.3579\n",
      "31699,3.3770\n",
      "31700,3.3939\n",
      "(step: 31700, epoch: 0, block: 32460800), Train Loss: 3.2818, Val Loss: 3.3063\n",
      "Learning rate: 5.29595932e-05\n",
      "Scheduler step: 31700\n",
      "Checkpoint (step: 31700, epoch: 0, block: 32460800) Saved\n",
      "Step Documented\n",
      "05:53:03\n",
      "31701,3.4061\n",
      "31702,3.3613\n",
      "31703,3.3550\n",
      "31704,3.3764\n",
      "31705,3.3780\n",
      "31706,3.3511\n",
      "31707,3.3839\n",
      "31708,3.3639\n",
      "31709,3.3617\n",
      "31710,3.3553\n",
      "31711,3.3458\n",
      "31712,3.3679\n",
      "31713,3.3717\n",
      "31714,3.3547\n",
      "31715,3.3700\n",
      "31716,3.3639\n",
      "31717,3.3754\n",
      "31718,3.3746\n",
      "31719,3.3749\n",
      "31720,3.3697\n",
      "31721,3.3518\n",
      "31722,3.3555\n",
      "31723,3.3917\n",
      "31724,3.3616\n",
      "31725,3.3878\n",
      "31726,3.3473\n",
      "31727,3.3531\n",
      "31728,3.3330\n",
      "31729,3.3639\n",
      "31730,3.3613\n",
      "31731,3.3596\n",
      "31732,3.3701\n",
      "31733,3.3796\n",
      "31734,3.3827\n",
      "31735,3.3595\n",
      "31736,3.3694\n",
      "31737,3.3816\n",
      "31738,3.3667\n",
      "31739,3.3665\n",
      "31740,3.3733\n",
      "31741,3.3744\n",
      "31742,3.3676\n",
      "31743,3.3767\n",
      "31744,3.3815\n",
      "31745,3.3507\n",
      "31746,3.3799\n",
      "31747,3.3806\n",
      "31748,3.3914\n",
      "31749,3.3685\n",
      "31750,3.3816\n",
      "31751,3.3409\n",
      "31752,3.3610\n",
      "31753,3.4026\n",
      "31754,3.3872\n",
      "31755,3.3826\n",
      "31756,3.3905\n",
      "31757,3.3699\n",
      "31758,3.3500\n",
      "31759,3.3697\n",
      "31760,3.3543\n",
      "31761,3.3685\n",
      "31762,3.3619\n",
      "31763,3.3813\n",
      "31764,3.3656\n",
      "31765,3.3657\n",
      "31766,3.3500\n",
      "31767,3.3745\n",
      "31768,3.3851\n",
      "31769,3.3877\n",
      "31770,3.3879\n",
      "31771,3.3642\n",
      "31772,3.3661\n",
      "31773,3.3607\n",
      "31774,3.3825\n",
      "31775,3.3670\n",
      "31776,3.3922\n",
      "31777,3.3436\n",
      "31778,3.3798\n",
      "31779,3.3860\n",
      "31780,3.3735\n",
      "31781,3.3860\n",
      "31782,3.3682\n",
      "31783,3.3453\n",
      "31784,3.3764\n",
      "31785,3.3587\n",
      "31786,3.3753\n",
      "31787,3.3665\n",
      "31788,3.4015\n",
      "31789,3.3625\n",
      "31790,3.3595\n",
      "31791,3.3731\n",
      "31792,3.3662\n",
      "31793,3.3696\n",
      "31794,3.3778\n",
      "31795,3.3760\n",
      "31796,3.3773\n",
      "31797,3.3831\n",
      "31798,3.3763\n",
      "31799,3.3551\n",
      "31800,3.3707\n",
      "(step: 31800, epoch: 0, block: 32563200), Train Loss: 3.2849, Val Loss: 3.3097\n",
      "Learning rate: 5.27284453e-05\n",
      "Scheduler step: 31800\n",
      "Checkpoint (step: 31800, epoch: 0, block: 32563200) Saved\n",
      "Step Documented\n",
      "06:19:24\n",
      "31801,3.3453\n",
      "31802,3.3575\n",
      "31803,3.3687\n",
      "31804,3.3722\n",
      "31805,3.3240\n",
      "31806,3.3870\n",
      "31807,3.3418\n",
      "31808,3.3631\n",
      "31809,3.3650\n",
      "31810,3.3263\n",
      "31811,3.3600\n",
      "31812,3.3317\n",
      "31813,3.3873\n",
      "31814,3.3597\n",
      "31815,3.3799\n",
      "31816,3.3876\n",
      "31817,3.3890\n",
      "31818,3.3543\n",
      "31819,3.3790\n",
      "31820,3.3468\n",
      "31821,3.3731\n",
      "31822,3.4017\n",
      "31823,3.3880\n",
      "31824,3.3737\n",
      "31825,3.3795\n",
      "31826,3.3689\n",
      "31827,3.3829\n",
      "31828,3.3785\n",
      "31829,3.3591\n",
      "31830,3.3540\n",
      "31831,3.3302\n",
      "31832,3.3646\n",
      "31833,3.3770\n",
      "31834,3.3576\n",
      "31835,3.4023\n",
      "31836,3.3574\n",
      "31837,3.3565\n",
      "31838,3.3440\n",
      "31839,3.3788\n",
      "31840,3.3586\n",
      "31841,3.3874\n",
      "31842,3.3391\n",
      "31843,3.3666\n",
      "31844,3.3555\n",
      "31845,3.3756\n",
      "31846,3.3269\n",
      "31847,3.3804\n",
      "31848,3.3891\n",
      "31849,3.3662\n",
      "31850,3.3546\n",
      "31851,3.3796\n",
      "31852,3.3346\n",
      "31853,3.3629\n",
      "31854,3.3547\n",
      "31855,3.3401\n",
      "31856,3.3623\n",
      "31857,3.3567\n",
      "31858,3.3601\n",
      "31859,3.3579\n",
      "31860,3.3643\n",
      "31861,3.3507\n",
      "31862,3.3585\n",
      "31863,3.3506\n",
      "31864,3.3691\n",
      "31865,3.3744\n",
      "31866,3.3908\n",
      "31867,3.3540\n",
      "31868,3.3908\n",
      "31869,3.3725\n",
      "31870,3.3860\n",
      "31871,3.3757\n",
      "31872,3.3769\n",
      "31873,3.3482\n",
      "31874,3.3621\n",
      "31875,3.3592\n",
      "31876,3.3498\n",
      "31877,3.3753\n",
      "31878,3.3562\n",
      "31879,3.3402\n",
      "31880,3.3449\n",
      "31881,3.3453\n",
      "31882,3.3801\n",
      "31883,3.3461\n",
      "31884,3.3459\n",
      "31885,3.3649\n",
      "31886,3.3749\n",
      "31887,3.3476\n",
      "31888,3.3748\n",
      "31889,3.3611\n",
      "31890,3.3690\n",
      "31891,3.3679\n",
      "31892,3.3486\n",
      "31893,3.3519\n",
      "31894,3.3592\n",
      "31895,3.3459\n",
      "31896,3.3953\n",
      "31897,3.3672\n",
      "31898,3.3544\n",
      "31899,3.3649\n",
      "31900,3.3713\n",
      "(step: 31900, epoch: 0, block: 32665600), Train Loss: 3.2847, Val Loss: 3.3139\n",
      "Learning rate: 5.24973295e-05\n",
      "Scheduler step: 31900\n",
      "Checkpoint (step: 31900, epoch: 0, block: 32665600) Saved\n",
      "Step Documented\n",
      "06:45:44\n",
      "31901,3.3539\n",
      "31902,3.3662\n",
      "31903,3.3536\n",
      "31904,3.3709\n",
      "31905,3.3678\n",
      "31906,3.3516\n",
      "31907,3.3788\n",
      "31908,3.3931\n",
      "31909,3.3566\n",
      "31910,3.3693\n",
      "31911,3.3570\n",
      "31912,3.3594\n",
      "31913,3.3669\n",
      "31914,3.3528\n",
      "31915,3.3524\n",
      "31916,3.3623\n",
      "31917,3.3557\n",
      "31918,3.3572\n",
      "31919,3.3804\n",
      "31920,3.3690\n",
      "31921,3.3526\n",
      "31922,3.3701\n",
      "31923,3.3517\n",
      "31924,3.3332\n",
      "31925,3.3609\n",
      "31926,3.3419\n",
      "31927,3.3605\n",
      "31928,3.3852\n",
      "31929,3.3533\n",
      "31930,3.3807\n",
      "31931,3.3764\n",
      "31932,3.3835\n",
      "31933,3.3576\n",
      "31934,3.3542\n",
      "31935,3.3443\n",
      "31936,3.3698\n",
      "31937,3.3503\n",
      "31938,3.3640\n",
      "31939,3.4012\n",
      "31940,3.3527\n",
      "31941,3.3562\n",
      "31942,3.3774\n",
      "31943,3.3534\n",
      "31944,3.3819\n",
      "31945,3.3409\n",
      "31946,3.3183\n",
      "31947,3.3573\n",
      "31948,3.3595\n",
      "31949,3.3800\n",
      "31950,3.3684\n",
      "31951,3.3359\n",
      "31952,3.3444\n",
      "31953,3.3627\n",
      "31954,3.3802\n",
      "31955,3.3691\n",
      "31956,3.3584\n",
      "31957,3.3499\n",
      "31958,3.3808\n",
      "31959,3.3846\n",
      "31960,3.3821\n",
      "31961,3.3857\n",
      "31962,3.3733\n",
      "31963,3.3594\n",
      "31964,3.3440\n",
      "31965,3.3389\n",
      "31966,3.3814\n",
      "31967,3.3437\n",
      "31968,3.3645\n",
      "31969,3.3741\n",
      "31970,3.3658\n",
      "31971,3.3659\n",
      "31972,3.3714\n",
      "31973,3.3753\n",
      "31974,3.3522\n",
      "31975,3.3459\n",
      "31976,3.3873\n",
      "31977,3.3292\n",
      "31978,3.3584\n",
      "31979,3.3821\n",
      "31980,3.3820\n",
      "31981,3.3518\n",
      "31982,3.3281\n",
      "31983,3.3495\n",
      "31984,3.3659\n",
      "31985,3.3401\n",
      "31986,3.3713\n",
      "31987,3.3652\n",
      "31988,3.3862\n",
      "31989,3.3655\n",
      "31990,3.3796\n",
      "31991,3.3594\n",
      "31992,3.3676\n",
      "31993,3.3597\n",
      "31994,3.3528\n",
      "31995,3.3381\n",
      "31996,3.3546\n",
      "31997,3.3637\n",
      "31998,3.3629\n",
      "31999,3.3755\n",
      "32000,3.3519\n",
      "(step: 32000, epoch: 0, block: 32768000), Train Loss: 3.3186, Val Loss: 3.3235\n",
      "Learning rate: 5.22662516e-05\n",
      "Scheduler step: 32000\n",
      "Checkpoint (step: 32000, epoch: 0, block: 32768000) Saved\n",
      "Step Documented\n",
      "07:12:05\n",
      "32001,3.3595\n",
      "32002,3.3672\n",
      "32003,3.3724\n",
      "32004,3.3517\n",
      "32005,3.3570\n",
      "32006,3.3767\n",
      "32007,3.3809\n",
      "32008,3.3794\n",
      "32009,3.3687\n",
      "32010,3.3767\n",
      "32011,3.3788\n",
      "32012,3.3955\n",
      "32013,3.3628\n",
      "32014,3.3752\n",
      "32015,3.3693\n",
      "32016,3.3661\n",
      "32017,3.3836\n",
      "32018,3.3578\n",
      "32019,3.3743\n",
      "32020,3.3522\n",
      "32021,3.3869\n",
      "32022,3.3511\n",
      "32023,3.3804\n",
      "32024,3.3866\n",
      "32025,3.3716\n",
      "32026,3.3818\n",
      "32027,3.3687\n",
      "32028,3.3742\n",
      "32029,3.3598\n",
      "32030,3.3659\n",
      "32031,3.3509\n",
      "32032,3.3741\n",
      "32033,3.3503\n",
      "32034,3.3703\n",
      "32035,3.3742\n",
      "32036,3.3696\n",
      "32037,3.3897\n",
      "32038,3.3710\n",
      "32039,3.3763\n",
      "32040,3.3501\n",
      "32041,3.3220\n",
      "32042,3.3692\n",
      "32043,3.3542\n",
      "32044,3.3413\n",
      "32045,3.3766\n",
      "32046,3.3507\n",
      "32047,3.3632\n",
      "32048,3.3891\n",
      "32049,3.3866\n",
      "32050,3.3980\n",
      "32051,3.3599\n",
      "32052,3.3766\n",
      "32053,3.4025\n",
      "32054,3.3557\n",
      "32055,3.3826\n",
      "32056,3.3589\n",
      "32057,3.3401\n",
      "32058,3.3745\n",
      "32059,3.3754\n",
      "32060,3.3517\n",
      "32061,3.3674\n",
      "32062,3.3511\n",
      "32063,3.3639\n",
      "32064,3.3624\n",
      "32065,3.3873\n",
      "32066,3.3624\n",
      "32067,3.3929\n",
      "32068,3.3908\n",
      "32069,3.3661\n",
      "32070,3.3539\n",
      "32071,3.3560\n",
      "32072,3.3701\n",
      "32073,3.4044\n",
      "32074,3.3391\n",
      "32075,3.3603\n",
      "32076,3.3683\n",
      "32077,3.3430\n",
      "32078,3.4055\n",
      "32079,3.3526\n",
      "32080,3.3686\n",
      "32081,3.3767\n",
      "32082,3.3858\n",
      "32083,3.3634\n",
      "32084,3.3660\n",
      "32085,3.3364\n",
      "32086,3.3520\n",
      "32087,3.3659\n",
      "32088,3.3851\n",
      "32089,3.3840\n",
      "32090,3.3699\n",
      "32091,3.3314\n",
      "32092,3.3369\n",
      "32093,3.3908\n",
      "32094,3.3360\n",
      "32095,3.3579\n",
      "32096,3.3842\n",
      "32097,3.3658\n",
      "32098,3.3803\n",
      "32099,3.3512\n",
      "32100,3.3777\n",
      "(step: 32100, epoch: 0, block: 32870400), Train Loss: 3.3039, Val Loss: 3.3449\n",
      "Learning rate: 5.20352175e-05\n",
      "Scheduler step: 32100\n",
      "Checkpoint (step: 32100, epoch: 0, block: 32870400) Saved\n",
      "Step Documented\n",
      "07:38:25\n",
      "32101,3.3570\n",
      "32102,3.3650\n",
      "32103,3.3569\n",
      "32104,3.3557\n",
      "32105,3.3800\n",
      "32106,3.3549\n",
      "32107,3.3787\n",
      "32108,3.3611\n",
      "32109,3.3553\n",
      "32110,3.3785\n",
      "32111,3.3692\n",
      "32112,3.3588\n",
      "32113,3.3563\n",
      "32114,3.3462\n",
      "32115,3.3596\n",
      "32116,3.3796\n",
      "32117,3.3555\n",
      "32118,3.3882\n",
      "32119,3.3781\n",
      "32120,3.3397\n",
      "32121,3.3496\n",
      "32122,3.3928\n",
      "32123,3.3584\n",
      "32124,3.3559\n",
      "32125,3.3609\n",
      "32126,3.3776\n",
      "32127,3.3765\n",
      "32128,3.3339\n",
      "32129,3.3617\n",
      "32130,3.3492\n",
      "32131,3.3677\n",
      "32132,3.3642\n",
      "32133,3.4140\n",
      "32134,3.3712\n",
      "32135,3.4007\n",
      "32136,3.3628\n",
      "32137,3.3468\n",
      "32138,3.3637\n",
      "32139,3.3744\n",
      "32140,3.3157\n",
      "32141,3.3601\n",
      "32142,3.3872\n",
      "32143,3.3497\n",
      "32144,3.3368\n",
      "32145,3.3941\n",
      "32146,3.3560\n",
      "32147,3.3303\n",
      "32148,3.3359\n",
      "32149,3.3496\n",
      "32150,3.3310\n",
      "32151,3.3603\n",
      "32152,3.3757\n",
      "32153,3.3618\n",
      "32154,3.3540\n",
      "32155,3.3378\n",
      "32156,3.3720\n",
      "32157,3.3895\n",
      "32158,3.3619\n",
      "32159,3.3582\n",
      "32160,3.3669\n",
      "32161,3.3539\n",
      "32162,3.3602\n",
      "32163,3.3762\n",
      "32164,3.3727\n",
      "32165,3.3822\n",
      "32166,3.3516\n",
      "32167,3.3702\n",
      "32168,3.3570\n",
      "32169,3.3776\n",
      "32170,3.3720\n",
      "32171,3.3595\n",
      "32172,3.4015\n",
      "32173,3.3490\n",
      "32174,3.3645\n",
      "32175,3.3669\n",
      "32176,3.3534\n",
      "32177,3.3515\n",
      "32178,3.3254\n",
      "32179,3.3624\n",
      "32180,3.3610\n",
      "32181,3.3669\n",
      "32182,3.3541\n",
      "32183,3.3699\n",
      "32184,3.3465\n",
      "32185,3.3708\n",
      "32186,3.3551\n",
      "32187,3.3409\n",
      "32188,3.3747\n",
      "32189,3.3738\n",
      "32190,3.3304\n",
      "32191,3.3541\n",
      "32192,3.3688\n",
      "32193,3.3705\n",
      "32194,3.3718\n",
      "32195,3.3394\n",
      "32196,3.3593\n",
      "32197,3.3752\n",
      "32198,3.3432\n",
      "32199,3.3953\n",
      "32200,3.3732\n",
      "(step: 32200, epoch: 0, block: 32972800), Train Loss: 3.2855, Val Loss: 3.2965\n",
      "Learning rate: 5.18042331e-05\n",
      "Scheduler step: 32200\n",
      "Checkpoint (step: 32200, epoch: 0, block: 32972800) Saved\n",
      "Step Documented\n",
      "08:04:45\n",
      "32201,3.3805\n",
      "32202,3.3562\n",
      "32203,3.3582\n",
      "32204,3.3823\n",
      "32205,3.4037\n",
      "32206,3.3893\n",
      "32207,3.3506\n",
      "32208,3.4030\n",
      "32209,3.3904\n",
      "32210,3.3657\n",
      "32211,3.3730\n",
      "32212,3.3581\n",
      "32213,3.3453\n",
      "32214,3.3338\n",
      "32215,3.3640\n",
      "32216,3.3633\n",
      "32217,3.3907\n",
      "32218,3.3705\n",
      "32219,3.3750\n",
      "32220,3.3700\n",
      "32221,3.3488\n",
      "32222,3.3699\n",
      "32223,3.3711\n",
      "32224,3.3832\n",
      "32225,3.3775\n",
      "32226,3.3480\n",
      "32227,3.3822\n",
      "32228,3.3507\n",
      "32229,3.3656\n",
      "32230,3.3584\n",
      "32231,3.3622\n",
      "32232,3.3704\n",
      "32233,3.3440\n",
      "32234,3.3691\n",
      "32235,3.3666\n",
      "32236,3.3704\n",
      "32237,3.3615\n",
      "32238,3.3731\n",
      "32239,3.3569\n",
      "32240,3.3569\n",
      "32241,3.3596\n",
      "32242,3.3505\n",
      "32243,3.3768\n",
      "32244,3.3586\n",
      "32245,3.3736\n",
      "32246,3.4048\n",
      "32247,3.3731\n",
      "32248,3.3514\n",
      "32249,3.3876\n",
      "32250,3.3390\n",
      "32251,3.3591\n",
      "32252,3.3460\n",
      "32253,3.3648\n",
      "32254,3.3578\n",
      "32255,3.3446\n",
      "32256,3.3693\n",
      "32257,3.3807\n",
      "32258,3.3496\n",
      "32259,3.3724\n",
      "32260,3.3689\n",
      "32261,3.3628\n",
      "32262,3.3982\n",
      "32263,3.3797\n",
      "32264,3.3839\n",
      "32265,3.3858\n",
      "32266,3.3915\n",
      "32267,3.3634\n",
      "32268,3.3437\n",
      "32269,3.3716\n",
      "32270,3.3484\n",
      "32271,3.3638\n",
      "32272,3.3558\n",
      "32273,3.3467\n",
      "32274,3.4002\n",
      "32275,3.3650\n",
      "32276,3.3733\n",
      "32277,3.3650\n",
      "32278,3.3547\n",
      "32279,3.3679\n",
      "32280,3.3851\n",
      "32281,3.3523\n",
      "32282,3.3768\n",
      "32283,3.3696\n",
      "32284,3.3423\n",
      "32285,3.3615\n",
      "32286,3.3557\n",
      "32287,3.3799\n",
      "32288,3.3669\n",
      "32289,3.3838\n",
      "32290,3.4047\n",
      "32291,3.3449\n",
      "32292,3.4088\n",
      "32293,3.3909\n",
      "32294,3.3577\n",
      "32295,3.3546\n",
      "32296,3.3543\n",
      "32297,3.3495\n",
      "32298,3.3766\n",
      "32299,3.3631\n",
      "32300,3.3757\n",
      "(step: 32300, epoch: 0, block: 33075200), Train Loss: 3.2811, Val Loss: 3.3156\n",
      "Learning rate: 5.15733042e-05\n",
      "Scheduler step: 32300\n",
      "Checkpoint (step: 32300, epoch: 0, block: 33075200) Saved\n",
      "Step Documented\n",
      "08:31:06\n",
      "32301,3.3517\n",
      "32302,3.3467\n",
      "32303,3.3610\n",
      "32304,3.3664\n",
      "32305,3.3697\n",
      "32306,3.3650\n",
      "32307,3.3672\n",
      "32308,3.3756\n",
      "32309,3.3639\n",
      "32310,3.3639\n",
      "32311,3.3462\n",
      "32312,3.3756\n",
      "32313,3.3896\n",
      "32314,3.3420\n",
      "32315,3.3817\n",
      "32316,3.3871\n",
      "32317,3.3399\n",
      "32318,3.3598\n",
      "32319,3.3444\n",
      "32320,3.3485\n",
      "32321,3.3595\n",
      "32322,3.3316\n",
      "32323,3.3426\n",
      "32324,3.3743\n",
      "32325,3.3675\n",
      "32326,3.3364\n",
      "32327,3.3630\n",
      "32328,3.3407\n",
      "32329,3.4048\n",
      "32330,3.3740\n",
      "32331,3.3899\n",
      "32332,3.3769\n",
      "32333,3.3376\n",
      "32334,3.3644\n",
      "32335,3.3870\n",
      "32336,3.3655\n",
      "32337,3.3744\n",
      "32338,3.3485\n",
      "32339,3.3704\n",
      "32340,3.3501\n",
      "32341,3.3913\n",
      "32342,3.3665\n",
      "32343,3.3441\n",
      "32344,3.3620\n",
      "32345,3.3681\n",
      "32346,3.3204\n",
      "32347,3.3666\n",
      "32348,3.3667\n",
      "32349,3.3823\n",
      "32350,3.3622\n",
      "32351,3.3486\n",
      "32352,3.3337\n",
      "32353,3.3517\n",
      "32354,3.3600\n",
      "32355,3.3429\n",
      "32356,3.3561\n",
      "32357,3.3612\n",
      "32358,3.3577\n",
      "32359,3.3979\n",
      "32360,3.3656\n",
      "32361,3.3520\n",
      "32362,3.3602\n",
      "32363,3.3637\n",
      "32364,3.3555\n",
      "32365,3.3588\n",
      "32366,3.3777\n",
      "32367,3.3431\n",
      "32368,3.3676\n",
      "32369,3.3679\n",
      "32370,3.3583\n",
      "32371,3.3532\n",
      "32372,3.3401\n",
      "32373,3.3648\n",
      "32374,3.3879\n",
      "32375,3.3522\n",
      "32376,3.3533\n",
      "32377,3.3589\n",
      "32378,3.3733\n",
      "32379,3.3684\n",
      "32380,3.3508\n",
      "32381,3.3564\n",
      "32382,3.3723\n",
      "32383,3.3613\n",
      "32384,3.3679\n",
      "32385,3.3439\n",
      "32386,3.3782\n",
      "32387,3.3729\n",
      "32388,3.3359\n",
      "32389,3.3559\n",
      "32390,3.3625\n",
      "32391,3.3714\n",
      "32392,3.3594\n",
      "32393,3.3581\n",
      "32394,3.3567\n",
      "32395,3.3775\n",
      "32396,3.3764\n",
      "32397,3.3761\n",
      "32398,3.3636\n",
      "32399,3.3855\n",
      "32400,3.3758\n",
      "(step: 32400, epoch: 0, block: 33177600), Train Loss: 3.2943, Val Loss: 3.2754\n",
      "Learning rate: 5.13424366e-05\n",
      "Scheduler step: 32400\n",
      "Checkpoint (step: 32400, epoch: 0, block: 33177600) Saved\n",
      "Step Documented\n",
      "08:57:27\n",
      "32401,3.3451\n",
      "32402,3.4049\n",
      "32403,3.3684\n",
      "32404,3.3744\n",
      "32405,3.3680\n",
      "32406,3.3496\n",
      "32407,3.3592\n",
      "32408,3.3607\n",
      "32409,3.3763\n",
      "32410,3.3798\n",
      "32411,3.3619\n",
      "32412,3.3641\n",
      "32413,3.3671\n",
      "32414,3.3516\n",
      "32415,3.3569\n",
      "32416,3.3627\n",
      "32417,3.3658\n",
      "32418,3.3586\n",
      "32419,3.3126\n",
      "32420,3.3390\n",
      "32421,3.3688\n",
      "32422,3.3579\n",
      "32423,3.3667\n",
      "32424,3.3338\n",
      "32425,3.3539\n",
      "32426,3.3950\n",
      "32427,3.3749\n",
      "32428,3.3795\n",
      "32429,3.3359\n",
      "32430,3.3668\n",
      "32431,3.3486\n",
      "32432,3.3784\n",
      "32433,3.3742\n",
      "32434,3.3740\n",
      "32435,3.3908\n",
      "32436,3.3674\n",
      "32437,3.3734\n",
      "32438,3.3883\n",
      "32439,3.3669\n",
      "32440,3.3835\n",
      "32441,3.3616\n",
      "32442,3.3715\n",
      "32443,3.3872\n",
      "32444,3.3504\n",
      "32445,3.3944\n",
      "32446,3.3626\n",
      "32447,3.3411\n",
      "32448,3.3673\n",
      "32449,3.3515\n",
      "32450,3.3774\n",
      "32451,3.3736\n",
      "32452,3.3617\n",
      "32453,3.3463\n",
      "32454,3.3467\n",
      "32455,3.3468\n",
      "32456,3.3560\n",
      "32457,3.3198\n",
      "32458,3.3526\n",
      "32459,3.3506\n",
      "32460,3.3857\n",
      "32461,3.3648\n",
      "32462,3.3644\n",
      "32463,3.3764\n",
      "32464,3.3423\n",
      "32465,3.3330\n",
      "32466,3.3515\n",
      "32467,3.3568\n",
      "32468,3.3767\n",
      "32469,3.3625\n",
      "32470,3.3583\n",
      "32471,3.3646\n",
      "32472,3.3621\n",
      "32473,3.3294\n",
      "32474,3.3918\n",
      "32475,3.3776\n",
      "32476,3.3580\n",
      "32477,3.4011\n",
      "32478,3.3486\n",
      "32479,3.3681\n",
      "32480,3.3260\n",
      "32481,3.3426\n",
      "32482,3.3698\n",
      "32483,3.3565\n",
      "32484,3.3704\n",
      "32485,3.3610\n",
      "32486,3.3676\n",
      "32487,3.3767\n",
      "32488,3.3241\n",
      "32489,3.3697\n",
      "32490,3.3554\n",
      "32491,3.3476\n",
      "32492,3.3519\n",
      "32493,3.3714\n",
      "32494,3.3957\n",
      "32495,3.3743\n",
      "32496,3.3730\n",
      "32497,3.3692\n",
      "32498,3.3636\n",
      "32499,3.3707\n",
      "32500,3.3692\n",
      "(step: 32500, epoch: 0, block: 33280000), Train Loss: 3.2792, Val Loss: 3.3012\n",
      "Learning rate: 5.11116361e-05\n",
      "Scheduler step: 32500\n",
      "Checkpoint (step: 32500, epoch: 0, block: 33280000) Saved\n",
      "Step Documented\n",
      "09:23:49\n",
      "32501,3.3897\n",
      "32502,3.3687\n",
      "32503,3.3326\n",
      "32504,3.4005\n",
      "32505,3.3743\n",
      "32506,3.3911\n",
      "32507,3.3425\n",
      "32508,3.3465\n",
      "32509,3.3843\n",
      "32510,3.3473\n",
      "32511,3.3522\n",
      "32512,3.3464\n",
      "32513,3.3695\n",
      "32514,3.3561\n",
      "32515,3.3573\n",
      "32516,3.3676\n",
      "32517,3.3551\n",
      "32518,3.3511\n",
      "32519,3.3680\n",
      "32520,3.3475\n",
      "32521,3.3300\n",
      "32522,3.3383\n",
      "32523,3.3322\n",
      "32524,3.3556\n",
      "32525,3.3406\n",
      "32526,3.3425\n",
      "32527,3.3607\n",
      "32528,3.3946\n",
      "32529,3.3720\n",
      "32530,3.3611\n",
      "32531,3.3486\n",
      "32532,3.3779\n",
      "32533,3.3625\n",
      "32534,3.3454\n",
      "32535,3.3895\n",
      "32536,3.3812\n",
      "32537,3.3785\n",
      "32538,3.3678\n",
      "32539,3.3879\n",
      "32540,3.3683\n",
      "32541,3.3643\n",
      "32542,3.3542\n",
      "32543,3.3537\n",
      "32544,3.3566\n",
      "32545,3.3870\n",
      "32546,3.3611\n",
      "32547,3.3599\n",
      "32548,3.3809\n",
      "32549,3.3553\n",
      "32550,3.3715\n",
      "32551,3.3572\n",
      "32552,3.3665\n",
      "32553,3.3391\n",
      "32554,3.3809\n",
      "32555,3.3666\n",
      "32556,3.3601\n",
      "32557,3.3735\n",
      "32558,3.3494\n",
      "32559,3.3739\n",
      "32560,3.3735\n",
      "32561,3.3843\n",
      "32562,3.3269\n",
      "32563,3.3730\n",
      "32564,3.3675\n",
      "32565,3.3678\n",
      "32566,3.3625\n",
      "32567,3.3364\n",
      "32568,3.3733\n",
      "32569,3.3738\n",
      "32570,3.3544\n",
      "32571,3.3362\n",
      "32572,3.3571\n",
      "32573,3.3728\n",
      "32574,3.3459\n",
      "32575,3.3535\n",
      "32576,3.3513\n",
      "32577,3.3539\n",
      "32578,3.3490\n",
      "32579,3.3531\n",
      "32580,3.3937\n",
      "32581,3.3550\n",
      "32582,3.3489\n",
      "32583,3.3392\n",
      "32584,3.3577\n",
      "32585,3.3941\n",
      "32586,3.3657\n",
      "32587,3.3664\n",
      "32588,3.3622\n",
      "32589,3.4013\n",
      "32590,3.3368\n",
      "32591,3.3855\n",
      "32592,3.3512\n",
      "32593,3.3535\n",
      "32594,3.3585\n",
      "32595,3.3658\n",
      "32596,3.3732\n",
      "32597,3.3409\n",
      "32598,3.3693\n",
      "32599,3.3807\n",
      "32600,3.3529\n",
      "(step: 32600, epoch: 0, block: 33382400), Train Loss: 3.2729, Val Loss: 3.3019\n",
      "Learning rate: 5.08809086e-05\n",
      "Scheduler step: 32600\n",
      "Checkpoint (step: 32600, epoch: 0, block: 33382400) Saved\n",
      "Step Documented\n",
      "09:50:09\n",
      "32601,3.3640\n",
      "32602,3.3525\n",
      "32603,3.3581\n",
      "32604,3.3850\n",
      "32605,3.3743\n",
      "32606,3.3460\n",
      "32607,3.3255\n",
      "32608,3.3585\n",
      "32609,3.3340\n",
      "32610,3.3483\n",
      "32611,3.3383\n",
      "32612,3.3606\n",
      "32613,3.3650\n",
      "32614,3.3487\n",
      "32615,3.3556\n",
      "32616,3.3686\n",
      "32617,3.3560\n",
      "32618,3.3382\n",
      "32619,3.3676\n",
      "32620,3.3356\n",
      "32621,3.3589\n",
      "32622,3.3526\n",
      "32623,3.3712\n",
      "32624,3.3483\n",
      "32625,3.3746\n",
      "32626,3.3658\n",
      "32627,3.3562\n",
      "32628,3.3785\n",
      "32629,3.3735\n",
      "32630,3.3658\n",
      "32631,3.3540\n",
      "32632,3.3432\n",
      "32633,3.3601\n",
      "32634,3.3617\n",
      "32635,3.3631\n",
      "32636,3.3504\n",
      "32637,3.3748\n",
      "32638,3.3653\n",
      "32639,3.3382\n",
      "32640,3.3594\n",
      "32641,3.3603\n",
      "32642,3.3779\n",
      "32643,3.3277\n",
      "32644,3.3615\n",
      "32645,3.3365\n",
      "32646,3.3527\n",
      "32647,3.3693\n",
      "32648,3.3715\n",
      "32649,3.3638\n",
      "32650,3.3388\n",
      "32651,3.3329\n",
      "32652,3.3550\n",
      "32653,3.3476\n",
      "32654,3.3907\n",
      "32655,3.3864\n",
      "32656,3.3879\n",
      "32657,3.3830\n",
      "32658,3.3339\n",
      "32659,3.3605\n",
      "32660,3.3728\n",
      "32661,3.3759\n",
      "32662,3.3704\n",
      "32663,3.3809\n",
      "32664,3.3699\n",
      "32665,3.3762\n",
      "32666,3.3515\n",
      "32667,3.3672\n",
      "32668,3.3611\n",
      "32669,3.3813\n",
      "32670,3.3586\n",
      "32671,3.3713\n",
      "32672,3.3739\n",
      "32673,3.3662\n",
      "32674,3.3747\n",
      "32675,3.3907\n",
      "32676,3.3672\n",
      "32677,3.3338\n",
      "32678,3.3983\n",
      "32679,3.3762\n",
      "32680,3.3585\n",
      "32681,3.3687\n",
      "32682,3.3520\n",
      "32683,3.3555\n",
      "32684,3.3755\n",
      "32685,3.3687\n",
      "32686,3.3522\n",
      "32687,3.3553\n",
      "32688,3.3938\n",
      "32689,3.3816\n",
      "32690,3.3556\n",
      "32691,3.3758\n",
      "32692,3.3737\n",
      "32693,3.3758\n",
      "32694,3.3625\n",
      "32695,3.3800\n",
      "32696,3.3472\n",
      "32697,3.3521\n",
      "32698,3.3712\n",
      "32699,3.3756\n",
      "32700,3.3586\n",
      "(step: 32700, epoch: 0, block: 33484800), Train Loss: 3.2849, Val Loss: 3.3041\n",
      "Learning rate: 5.06502599e-05\n",
      "Scheduler step: 32700\n",
      "Checkpoint (step: 32700, epoch: 0, block: 33484800) Saved\n",
      "Step Documented\n",
      "10:16:30\n",
      "32701,3.3421\n",
      "32702,3.3948\n",
      "32703,3.3714\n",
      "32704,3.3349\n",
      "32705,3.3388\n",
      "32706,3.3550\n",
      "32707,3.3183\n",
      "32708,3.3870\n",
      "32709,3.3881\n",
      "32710,3.3674\n",
      "32711,3.3474\n",
      "32712,3.3678\n",
      "32713,3.3992\n",
      "32714,3.3340\n",
      "32715,3.3355\n",
      "32716,3.3742\n",
      "32717,3.3655\n",
      "32718,3.3526\n",
      "32719,3.3375\n",
      "32720,3.3545\n",
      "32721,3.3715\n",
      "32722,3.3750\n",
      "32723,3.3632\n",
      "32724,3.3681\n",
      "32725,3.3491\n",
      "32726,3.3686\n",
      "32727,3.3683\n",
      "32728,3.3639\n",
      "32729,3.3849\n",
      "32730,3.3721\n",
      "32731,3.3642\n",
      "32732,3.3518\n",
      "32733,3.3381\n",
      "32734,3.3538\n",
      "32735,3.3621\n",
      "32736,3.3348\n",
      "32737,3.3494\n",
      "32738,3.3662\n",
      "32739,3.3453\n",
      "32740,3.3809\n",
      "32741,3.3680\n",
      "32742,3.3378\n",
      "32743,3.3785\n",
      "32744,3.3479\n",
      "32745,3.3435\n",
      "32746,3.3590\n",
      "32747,3.3901\n",
      "32748,3.3963\n",
      "32749,3.3512\n",
      "32750,3.3578\n",
      "32751,3.3655\n",
      "32752,3.3239\n",
      "32753,3.3618\n",
      "32754,3.3819\n",
      "32755,3.3587\n",
      "32756,3.3955\n",
      "32757,3.3774\n",
      "32758,3.3805\n",
      "32759,3.3587\n",
      "32760,3.3543\n",
      "32761,3.3683\n",
      "32762,3.3459\n",
      "32763,3.3567\n",
      "32764,3.3569\n",
      "32765,3.3727\n",
      "32766,3.3690\n",
      "32767,3.3335\n",
      "32768,3.3913\n",
      "32769,3.3256\n",
      "32770,3.3534\n",
      "32771,3.3715\n",
      "32772,3.3812\n",
      "32773,3.3429\n",
      "32774,3.3491\n",
      "32775,3.3900\n",
      "32776,3.3805\n",
      "32777,3.3753\n",
      "32778,3.3484\n",
      "32779,3.3562\n",
      "32780,3.3762\n",
      "32781,3.3570\n",
      "32782,3.3783\n",
      "32783,3.3540\n",
      "32784,3.3478\n",
      "32785,3.3521\n",
      "32786,3.3376\n",
      "32787,3.3652\n",
      "32788,3.3542\n",
      "32789,3.3527\n",
      "32790,3.3690\n",
      "32791,3.3509\n",
      "32792,3.3671\n",
      "32793,3.3321\n",
      "32794,3.3490\n",
      "32795,3.3705\n",
      "32796,3.3636\n",
      "32797,3.3521\n",
      "32798,3.3632\n",
      "32799,3.3589\n",
      "32800,3.3715\n",
      "(step: 32800, epoch: 0, block: 33587200), Train Loss: 3.2912, Val Loss: 3.2652\n",
      "Learning rate: 5.04196959e-05\n",
      "Scheduler step: 32800\n",
      "Checkpoint (step: 32800, epoch: 0, block: 33587200) Saved\n",
      "Step Documented\n",
      "10:42:52\n",
      "32801,3.3669\n",
      "32802,3.3385\n",
      "32803,3.3554\n",
      "32804,3.3488\n",
      "32805,3.3817\n",
      "32806,3.3826\n",
      "32807,3.3844\n",
      "32808,3.3748\n",
      "32809,3.3650\n",
      "32810,3.3466\n",
      "32811,3.3568\n",
      "32812,3.3427\n",
      "32813,3.3967\n",
      "32814,3.3583\n",
      "32815,3.3853\n",
      "32816,3.3709\n",
      "32817,3.3344\n",
      "32818,3.3642\n",
      "32819,3.3408\n",
      "32820,3.3580\n",
      "32821,3.3577\n",
      "32822,3.3462\n",
      "32823,3.3738\n",
      "32824,3.4068\n",
      "32825,3.3590\n",
      "32826,3.3263\n",
      "32827,3.3528\n",
      "32828,3.3627\n",
      "32829,3.3599\n",
      "32830,3.3181\n",
      "32831,3.3662\n",
      "32832,3.3730\n",
      "32833,3.3684\n",
      "32834,3.3923\n",
      "32835,3.3603\n",
      "32836,3.3532\n",
      "32837,3.3686\n",
      "32838,3.3469\n",
      "32839,3.3905\n",
      "32840,3.3792\n",
      "32841,3.3637\n",
      "32842,3.3890\n",
      "32843,3.3621\n",
      "32844,3.3891\n",
      "32845,3.3418\n",
      "32846,3.3565\n",
      "32847,3.3651\n",
      "32848,3.3402\n",
      "32849,3.3638\n",
      "32850,3.3642\n",
      "32851,3.3479\n",
      "32852,3.3816\n",
      "32853,3.3929\n",
      "32854,3.3910\n",
      "32855,3.3690\n",
      "32856,3.3321\n",
      "32857,3.3826\n",
      "32858,3.3774\n",
      "32859,3.3777\n",
      "32860,3.3554\n",
      "32861,3.3558\n",
      "32862,3.3724\n",
      "32863,3.3318\n",
      "32864,3.3375\n",
      "32865,3.3509\n",
      "32866,3.3824\n",
      "32867,3.3752\n",
      "32868,3.3719\n",
      "32869,3.3646\n",
      "32870,3.3546\n",
      "32871,3.3601\n",
      "32872,3.3717\n",
      "32873,3.3485\n",
      "32874,3.3607\n",
      "32875,3.3833\n",
      "32876,3.3729\n",
      "32877,3.3482\n",
      "32878,3.3546\n",
      "32879,3.3762\n",
      "32880,3.3661\n",
      "32881,3.3825\n",
      "32882,3.3633\n",
      "32883,3.3581\n",
      "32884,3.3453\n",
      "32885,3.3384\n",
      "32886,3.3291\n",
      "32887,3.3829\n",
      "32888,3.3368\n",
      "32889,3.3484\n",
      "32890,3.3525\n",
      "32891,3.3499\n",
      "32892,3.3690\n",
      "32893,3.3407\n",
      "32894,3.3805\n",
      "32895,3.3748\n",
      "32896,3.3369\n",
      "32897,3.3698\n",
      "32898,3.3873\n",
      "32899,3.3629\n",
      "32900,3.3740\n",
      "(step: 32900, epoch: 0, block: 33689600), Train Loss: 3.2471, Val Loss: 3.3062\n",
      "Learning rate: 5.01892223e-05\n",
      "Scheduler step: 32900\n",
      "Checkpoint (step: 32900, epoch: 0, block: 33689600) Saved\n",
      "Step Documented\n",
      "11:09:14\n",
      "32901,3.3733\n",
      "32902,3.3463\n",
      "32903,3.3504\n",
      "32904,3.3602\n",
      "32905,3.3707\n",
      "32906,3.3431\n",
      "32907,3.3797\n",
      "32908,3.3439\n",
      "32909,3.3512\n",
      "32910,3.3821\n",
      "32911,3.3786\n",
      "32912,3.3652\n",
      "32913,3.3833\n",
      "32914,3.3450\n",
      "32915,3.3443\n",
      "32916,3.3393\n",
      "32917,3.3403\n",
      "32918,3.3403\n",
      "32919,3.3772\n",
      "32920,3.3730\n",
      "32921,3.3804\n",
      "32922,3.3416\n",
      "32923,3.3765\n",
      "32924,3.3452\n",
      "32925,3.3727\n",
      "32926,3.3553\n",
      "32927,3.4001\n",
      "32928,3.3474\n",
      "32929,3.3503\n",
      "32930,3.3476\n",
      "32931,3.3651\n",
      "32932,3.3835\n",
      "32933,3.3350\n",
      "32934,3.3571\n",
      "32935,3.3758\n",
      "32936,3.3947\n",
      "32937,3.3366\n",
      "32938,3.3513\n",
      "32939,3.3608\n",
      "32940,3.3853\n",
      "32941,3.3445\n",
      "32942,3.3567\n",
      "32943,3.3721\n",
      "32944,3.3644\n",
      "32945,3.3400\n",
      "32946,3.3674\n",
      "32947,3.3854\n",
      "32948,3.3682\n",
      "32949,3.3454\n",
      "32950,3.3638\n",
      "32951,3.3420\n",
      "32952,3.3772\n",
      "32953,3.3698\n",
      "32954,3.3741\n",
      "32955,3.3744\n",
      "32956,3.3584\n",
      "32957,3.3583\n",
      "32958,3.3753\n",
      "32959,3.3566\n",
      "32960,3.3575\n",
      "32961,3.3437\n",
      "32962,3.3613\n",
      "32963,3.3650\n",
      "32964,3.3555\n",
      "32965,3.3266\n",
      "32966,3.3693\n",
      "32967,3.3788\n",
      "32968,3.3777\n",
      "32969,3.3690\n",
      "32970,3.3384\n",
      "32971,3.3623\n",
      "32972,3.3613\n",
      "32973,3.3950\n",
      "32974,3.3368\n",
      "32975,3.3575\n",
      "32976,3.3687\n",
      "32977,3.3452\n",
      "32978,3.3807\n",
      "32979,3.3509\n",
      "32980,3.3891\n",
      "32981,3.3690\n",
      "32982,3.3711\n",
      "32983,3.3443\n",
      "32984,3.3892\n",
      "32985,3.3743\n",
      "32986,3.3420\n",
      "32987,3.3567\n",
      "32988,3.3788\n",
      "32989,3.3703\n",
      "32990,3.3511\n",
      "32991,3.3721\n",
      "32992,3.3274\n",
      "32993,3.3636\n",
      "32994,3.3740\n",
      "32995,3.3764\n",
      "32996,3.3205\n",
      "32997,3.3462\n",
      "32998,3.3871\n",
      "32999,3.3239\n",
      "33000,3.3732\n",
      "(step: 33000, epoch: 0, block: 33792000), Train Loss: 3.2369, Val Loss: 3.3162\n",
      "Learning rate: 4.99588450e-05\n",
      "Scheduler step: 33000\n",
      "Checkpoint (step: 33000, epoch: 0, block: 33792000) Saved\n",
      "Step Documented\n",
      "11:35:35\n",
      "33001,3.3948\n",
      "33002,3.3573\n",
      "33003,3.3542\n",
      "33004,3.3651\n",
      "33005,3.3850\n",
      "33006,3.3409\n",
      "33007,3.3542\n",
      "33008,3.3650\n",
      "33009,3.3724\n",
      "33010,3.3706\n",
      "33011,3.3676\n",
      "33012,3.3835\n",
      "33013,3.3787\n",
      "33014,3.3625\n",
      "33015,3.3851\n",
      "33016,3.3761\n",
      "33017,3.3907\n",
      "33018,3.3528\n",
      "33019,3.3390\n",
      "33020,3.3534\n",
      "33021,3.3229\n",
      "33022,3.3957\n",
      "33023,3.3628\n",
      "33024,3.3719\n",
      "33025,3.3787\n",
      "33026,3.3614\n",
      "33027,3.3232\n",
      "33028,3.3535\n",
      "33029,3.3407\n",
      "33030,3.3579\n",
      "33031,3.3482\n",
      "33032,3.3807\n",
      "33033,3.3731\n",
      "33034,3.3520\n",
      "33035,3.3543\n",
      "33036,3.3493\n",
      "33037,3.3708\n",
      "33038,3.3768\n",
      "33039,3.3542\n",
      "33040,3.3752\n",
      "33041,3.3733\n",
      "33042,3.3551\n",
      "33043,3.3578\n",
      "33044,3.3560\n",
      "33045,3.3605\n",
      "33046,3.3427\n",
      "33047,3.3395\n",
      "33048,3.3683\n",
      "33049,3.3699\n",
      "33050,3.3554\n",
      "33051,3.3676\n",
      "33052,3.3472\n",
      "33053,3.3543\n",
      "33054,3.3723\n",
      "33055,3.3202\n",
      "33056,3.3717\n",
      "33057,3.3430\n",
      "33058,3.3418\n",
      "33059,3.3906\n",
      "33060,3.3673\n",
      "33061,3.3730\n",
      "33062,3.3648\n",
      "33063,3.3592\n",
      "33064,3.3796\n",
      "33065,3.3441\n",
      "33066,3.3498\n",
      "33067,3.3627\n",
      "33068,3.3647\n",
      "33069,3.3312\n",
      "33070,3.3511\n",
      "33071,3.3449\n",
      "33072,3.3600\n",
      "33073,3.3688\n",
      "33074,3.3555\n",
      "33075,3.3381\n",
      "33076,3.3750\n",
      "33077,3.3738\n",
      "33078,3.3610\n",
      "33079,3.3896\n",
      "33080,3.3518\n",
      "33081,3.3565\n",
      "33082,3.3408\n",
      "33083,3.3680\n",
      "33084,3.3738\n",
      "33085,3.3571\n",
      "33086,3.3635\n",
      "33087,3.3643\n",
      "33088,3.3648\n",
      "33089,3.3518\n",
      "33090,3.3604\n",
      "33091,3.3791\n",
      "33092,3.3758\n",
      "33093,3.4046\n",
      "33094,3.3503\n",
      "33095,3.3489\n",
      "33096,3.3556\n",
      "33097,3.3619\n",
      "33098,3.3842\n",
      "33099,3.3601\n",
      "33100,3.3735\n",
      "(step: 33100, epoch: 0, block: 33894400), Train Loss: 3.2849, Val Loss: 3.3126\n",
      "Learning rate: 4.97285698e-05\n",
      "Scheduler step: 33100\n",
      "Checkpoint (step: 33100, epoch: 0, block: 33894400) Saved\n",
      "Step Documented\n",
      "12:02:03\n",
      "33101,3.3460\n",
      "33102,3.3596\n",
      "33103,3.4022\n",
      "33104,3.3525\n",
      "33105,3.3847\n",
      "33106,3.3464\n",
      "33107,3.3730\n",
      "33108,3.3509\n",
      "33109,3.3782\n",
      "33110,3.3538\n",
      "33111,3.3459\n",
      "33112,3.3662\n",
      "33113,3.3427\n",
      "33114,3.3381\n",
      "33115,3.3578\n",
      "33116,3.3807\n",
      "33117,3.3368\n",
      "33118,3.3513\n",
      "33119,3.3640\n",
      "33120,3.3591\n",
      "33121,3.3487\n",
      "33122,3.3697\n",
      "33123,3.3692\n",
      "33124,3.3903\n",
      "33125,3.3512\n",
      "33126,3.3606\n",
      "33127,3.3533\n",
      "33128,3.3680\n",
      "33129,3.3672\n",
      "33130,3.3708\n",
      "33131,3.3802\n",
      "33132,3.3535\n",
      "33133,3.3548\n",
      "33134,3.3536\n",
      "33135,3.3638\n",
      "33136,3.3935\n",
      "33137,3.3777\n",
      "33138,3.3271\n",
      "33139,3.3655\n",
      "33140,3.3574\n",
      "33141,3.3593\n",
      "33142,3.3513\n",
      "33143,3.4039\n",
      "33144,3.3636\n",
      "33145,3.3483\n",
      "33146,3.3381\n",
      "33147,3.3805\n",
      "33148,3.3651\n",
      "33149,3.3121\n",
      "33150,3.3578\n",
      "33151,3.3565\n",
      "33152,3.3516\n",
      "33153,3.3507\n",
      "33154,3.3283\n",
      "33155,3.3434\n",
      "33156,3.3431\n",
      "33157,3.3704\n",
      "33158,3.3577\n",
      "33159,3.3773\n",
      "33160,3.3412\n",
      "33161,3.3969\n",
      "33162,3.3333\n",
      "33163,3.3630\n",
      "33164,3.3246\n",
      "33165,3.3627\n",
      "33166,3.3665\n",
      "33167,3.3542\n",
      "33168,3.3595\n",
      "33169,3.3598\n",
      "33170,3.3549\n",
      "33171,3.3481\n",
      "33172,3.3644\n",
      "33173,3.3685\n",
      "33174,3.3450\n",
      "33175,3.3683\n",
      "33176,3.3559\n",
      "33177,3.3738\n",
      "33178,3.3614\n",
      "33179,3.3618\n",
      "33180,3.3663\n",
      "33181,3.3414\n",
      "33182,3.3747\n",
      "33183,3.3500\n",
      "33184,3.3792\n",
      "33185,3.3549\n",
      "33186,3.3532\n",
      "33187,3.3629\n",
      "33188,3.3602\n",
      "33189,3.3728\n",
      "33190,3.3575\n",
      "33191,3.3416\n",
      "33192,3.3857\n",
      "33193,3.3568\n",
      "33194,3.3957\n",
      "33195,3.3316\n",
      "33196,3.3506\n",
      "33197,3.3482\n",
      "33198,3.3785\n",
      "33199,3.3627\n",
      "33200,3.3841\n",
      "(step: 33200, epoch: 0, block: 33996800), Train Loss: 3.2813, Val Loss: 3.2905\n",
      "Learning rate: 4.94984025e-05\n",
      "Scheduler step: 33200\n",
      "Checkpoint (step: 33200, epoch: 0, block: 33996800) Saved\n",
      "Step Documented\n",
      "12:28:25\n",
      "33201,3.3683\n",
      "33202,3.3452\n",
      "33203,3.3860\n",
      "33204,3.3347\n",
      "33205,3.3994\n",
      "33206,3.3442\n",
      "33207,3.3823\n",
      "33208,3.3688\n",
      "33209,3.3601\n",
      "33210,3.3554\n",
      "33211,3.3557\n",
      "33212,3.3620\n",
      "33213,3.3491\n",
      "33214,3.3804\n",
      "33215,3.3350\n",
      "33216,3.3388\n",
      "33217,3.3647\n",
      "33218,3.3409\n",
      "33219,3.3458\n",
      "33220,3.3648\n",
      "33221,3.3303\n",
      "33222,3.3467\n",
      "33223,3.3550\n",
      "33224,3.3544\n",
      "33225,3.3851\n",
      "33226,3.3545\n",
      "33227,3.3740\n",
      "33228,3.3715\n",
      "33229,3.3673\n",
      "33230,3.3917\n",
      "33231,3.3634\n",
      "33232,3.3816\n",
      "33233,3.3651\n",
      "33234,3.3530\n",
      "33235,3.3597\n",
      "33236,3.3492\n",
      "33237,3.3488\n",
      "33238,3.3831\n",
      "33239,3.3487\n",
      "33240,3.3932\n",
      "33241,3.3682\n",
      "33242,3.3204\n",
      "33243,3.3862\n",
      "33244,3.3317\n",
      "33245,3.3508\n",
      "33246,3.3554\n",
      "33247,3.3402\n",
      "33248,3.3645\n",
      "33249,3.3480\n",
      "33250,3.3555\n",
      "33251,3.3638\n",
      "33252,3.3267\n",
      "33253,3.3640\n",
      "33254,3.3536\n",
      "33255,3.3904\n",
      "33256,3.3804\n",
      "33257,3.3607\n",
      "33258,3.3449\n",
      "33259,3.3570\n",
      "33260,3.3729\n",
      "33261,3.3642\n",
      "33262,3.3567\n",
      "33263,3.3785\n",
      "33264,3.3591\n",
      "33265,3.3322\n",
      "33266,3.3612\n",
      "33267,3.3465\n",
      "33268,3.3480\n",
      "33269,3.3657\n",
      "33270,3.3633\n",
      "33271,3.3662\n",
      "33272,3.3557\n",
      "33273,3.3449\n",
      "33274,3.3594\n",
      "33275,3.3654\n",
      "33276,3.3791\n",
      "33277,3.3465\n",
      "33278,3.3672\n",
      "33279,3.3484\n",
      "33280,3.3553\n",
      "33281,3.3571\n",
      "33282,3.3758\n",
      "33283,3.3576\n",
      "33284,3.3394\n",
      "33285,3.3515\n",
      "33286,3.3540\n",
      "33287,3.3824\n",
      "33288,3.3562\n",
      "33289,3.3537\n",
      "33290,3.3694\n",
      "33291,3.3688\n",
      "33292,3.3736\n",
      "33293,3.3470\n",
      "33294,3.3603\n",
      "33295,3.3347\n",
      "33296,3.3372\n",
      "33297,3.3665\n",
      "33298,3.4092\n",
      "33299,3.3854\n",
      "33300,3.3869\n",
      "(step: 33300, epoch: 0, block: 34099200), Train Loss: 3.2676, Val Loss: 3.3228\n",
      "Learning rate: 4.92683489e-05\n",
      "Scheduler step: 33300\n",
      "Checkpoint (step: 33300, epoch: 0, block: 34099200) Saved\n",
      "Step Documented\n",
      "12:54:46\n",
      "33301,3.3508\n",
      "33302,3.3661\n",
      "33303,3.3774\n",
      "33304,3.3587\n",
      "33305,3.3888\n",
      "33306,3.3844\n",
      "33307,3.3563\n",
      "33308,3.3570\n",
      "33309,3.3572\n",
      "33310,3.3396\n",
      "33311,3.3861\n",
      "33312,3.3676\n",
      "33313,3.3575\n",
      "33314,3.3486\n",
      "33315,3.3774\n",
      "33316,3.3455\n",
      "33317,3.3641\n",
      "33318,3.3408\n",
      "33319,3.3411\n",
      "33320,3.3609\n",
      "33321,3.3916\n",
      "33322,3.3483\n",
      "33323,3.3740\n",
      "33324,3.3627\n",
      "33325,3.3539\n",
      "33326,3.3530\n",
      "33327,3.3817\n",
      "33328,3.3754\n",
      "33329,3.3821\n",
      "33330,3.3703\n",
      "33331,3.3741\n",
      "33332,3.3518\n",
      "33333,3.3441\n",
      "33334,3.3725\n",
      "33335,3.3312\n",
      "33336,3.4117\n",
      "33337,3.3925\n",
      "33338,3.3803\n",
      "33339,3.3443\n",
      "33340,3.3405\n",
      "33341,3.3562\n",
      "33342,3.3397\n",
      "33343,3.3654\n",
      "33344,3.3430\n",
      "33345,3.3625\n",
      "33346,3.3458\n",
      "33347,3.3731\n",
      "33348,3.3340\n",
      "33349,3.3465\n",
      "33350,3.3858\n",
      "33351,3.3596\n",
      "33352,3.3496\n",
      "33353,3.3439\n",
      "33354,3.3764\n",
      "33355,3.3758\n",
      "33356,3.3528\n",
      "33357,3.3739\n",
      "33358,3.3681\n",
      "33359,3.3905\n",
      "33360,3.3546\n",
      "33361,3.3966\n",
      "33362,3.3661\n",
      "33363,3.3762\n",
      "33364,3.3539\n",
      "33365,3.3500\n",
      "33366,3.3698\n",
      "33367,3.3695\n",
      "33368,3.3874\n",
      "33369,3.3430\n",
      "33370,3.3496\n",
      "33371,3.3578\n",
      "33372,3.3552\n",
      "33373,3.3481\n",
      "33374,3.3477\n",
      "33375,3.3384\n",
      "33376,3.3578\n",
      "33377,3.3760\n",
      "33378,3.3655\n",
      "33379,3.3860\n",
      "33380,3.3625\n",
      "33381,3.3471\n",
      "33382,3.3605\n",
      "33383,3.3599\n",
      "33384,3.3317\n",
      "33385,3.3510\n",
      "33386,3.3472\n",
      "33387,3.3631\n",
      "33388,3.3762\n",
      "33389,3.3846\n",
      "33390,3.3472\n",
      "33391,3.3604\n",
      "33392,3.3601\n",
      "33393,3.3625\n",
      "33394,3.3653\n",
      "33395,3.3403\n",
      "33396,3.3558\n",
      "33397,3.3803\n",
      "33398,3.3726\n",
      "33399,3.3612\n",
      "33400,3.3842\n",
      "(step: 33400, epoch: 0, block: 34201600), Train Loss: 3.2968, Val Loss: 3.2793\n",
      "Learning rate: 4.90384149e-05\n",
      "Scheduler step: 33400\n",
      "Checkpoint (step: 33400, epoch: 0, block: 34201600) Saved\n",
      "Step Documented\n",
      "13:21:09\n",
      "33401,3.3623\n",
      "33402,3.3769\n",
      "33403,3.3401\n",
      "33404,3.3552\n",
      "33405,3.3732\n",
      "33406,3.3678\n",
      "33407,3.3355\n",
      "33408,3.3592\n",
      "33409,3.3520\n",
      "33410,3.3582\n",
      "33411,3.3548\n",
      "33412,3.3646\n",
      "33413,3.3853\n",
      "33414,3.3394\n",
      "33415,3.3784\n",
      "33416,3.3440\n",
      "33417,3.3568\n",
      "33418,3.3859\n",
      "33419,3.3549\n",
      "33420,3.3502\n",
      "33421,3.3632\n",
      "33422,3.3422\n",
      "33423,3.3451\n",
      "33424,3.3538\n",
      "33425,3.3418\n",
      "33426,3.3545\n",
      "33427,3.3786\n",
      "33428,3.3717\n",
      "33429,3.3466\n",
      "33430,3.4058\n",
      "33431,3.3629\n",
      "33432,3.3738\n",
      "33433,3.3527\n",
      "33434,3.3599\n",
      "33435,3.3773\n",
      "33436,3.3615\n",
      "33437,3.3935\n",
      "33438,3.3545\n",
      "33439,3.3589\n",
      "33440,3.3722\n",
      "33441,3.3588\n",
      "33442,3.3774\n",
      "33443,3.3708\n",
      "33444,3.3428\n",
      "33445,3.3573\n",
      "33446,3.3579\n",
      "33447,3.3453\n",
      "33448,3.3286\n",
      "33449,3.3617\n",
      "33450,3.3981\n",
      "33451,3.3693\n",
      "33452,3.3182\n",
      "33453,3.3812\n",
      "33454,3.3246\n",
      "33455,3.3515\n",
      "33456,3.3480\n",
      "33457,3.3534\n",
      "33458,3.3385\n",
      "33459,3.3502\n",
      "33460,3.3286\n",
      "33461,3.3590\n",
      "33462,3.3703\n",
      "33463,3.3664\n",
      "33464,3.3584\n",
      "33465,3.3452\n",
      "33466,3.3446\n",
      "33467,3.3716\n",
      "33468,3.3383\n",
      "33469,3.3480\n",
      "33470,3.3471\n",
      "33471,3.3733\n",
      "33472,3.3482\n",
      "33473,3.3470\n",
      "33474,3.3620\n",
      "33475,3.3641\n",
      "33476,3.3393\n",
      "33477,3.3830\n",
      "33478,3.3674\n",
      "33479,3.3418\n",
      "33480,3.3531\n",
      "33481,3.3611\n",
      "33482,3.3468\n",
      "33483,3.3412\n",
      "33484,3.3562\n",
      "33485,3.3437\n",
      "33486,3.3389\n",
      "33487,3.3459\n",
      "33488,3.3631\n",
      "33489,3.3558\n",
      "33490,3.3456\n",
      "33491,3.3465\n",
      "33492,3.3665\n",
      "33493,3.3429\n",
      "33494,3.3406\n",
      "33495,3.4037\n",
      "33496,3.3531\n",
      "33497,3.3434\n",
      "33498,3.3636\n",
      "33499,3.3433\n",
      "33500,3.3688\n",
      "(step: 33500, epoch: 0, block: 34304000), Train Loss: 3.2482, Val Loss: 3.2948\n",
      "Learning rate: 4.88086063e-05\n",
      "Scheduler step: 33500\n",
      "Checkpoint (step: 33500, epoch: 0, block: 34304000) Saved\n",
      "Step Documented\n",
      "13:47:30\n",
      "33501,3.3642\n",
      "33502,3.3493\n",
      "33503,3.3623\n",
      "33504,3.3621\n",
      "33505,3.3657\n",
      "33506,3.3487\n",
      "33507,3.3443\n",
      "33508,3.3268\n",
      "33509,3.3810\n",
      "33510,3.3841\n",
      "33511,3.3785\n",
      "33512,3.3477\n",
      "33513,3.3665\n",
      "33514,3.3204\n",
      "33515,3.3596\n",
      "33516,3.3555\n",
      "33517,3.3719\n",
      "33518,3.3505\n",
      "33519,3.3670\n",
      "33520,3.3310\n",
      "33521,3.3458\n",
      "33522,3.3536\n",
      "33523,3.3730\n",
      "33524,3.3693\n",
      "33525,3.3498\n",
      "33526,3.3389\n",
      "33527,3.3717\n",
      "33528,3.3386\n",
      "33529,3.3560\n",
      "33530,3.3651\n",
      "33531,3.3796\n",
      "33532,3.3521\n",
      "33533,3.3701\n",
      "33534,3.3952\n",
      "33535,3.3455\n",
      "33536,3.3797\n",
      "33537,3.3619\n",
      "33538,3.3646\n",
      "33539,3.3364\n",
      "33540,3.3455\n",
      "33541,3.3494\n",
      "33542,3.3778\n",
      "33543,3.3743\n",
      "33544,3.3708\n",
      "33545,3.3739\n",
      "33546,3.3668\n",
      "33547,3.3935\n",
      "33548,3.3797\n",
      "33549,3.3389\n",
      "33550,3.4017\n",
      "33551,3.3707\n",
      "33552,3.3405\n",
      "33553,3.3824\n",
      "33554,3.3705\n",
      "33555,3.3624\n",
      "33556,3.3333\n",
      "33557,3.3794\n",
      "33558,3.3490\n",
      "33559,3.3785\n",
      "33560,3.3432\n",
      "33561,3.3830\n",
      "33562,3.3720\n",
      "33563,3.3489\n",
      "33564,3.3619\n",
      "33565,3.3598\n",
      "33566,3.3597\n",
      "33567,3.3565\n",
      "33568,3.3633\n",
      "33569,3.3806\n",
      "33570,3.3692\n",
      "33571,3.3325\n",
      "33572,3.3588\n",
      "33573,3.3593\n",
      "33574,3.3727\n",
      "33575,3.3483\n",
      "33576,3.3803\n",
      "33577,3.3525\n",
      "33578,3.3579\n",
      "33579,3.3428\n",
      "33580,3.3483\n",
      "33581,3.3942\n",
      "33582,3.3565\n",
      "33583,3.3549\n",
      "33584,3.3594\n",
      "33585,3.3297\n",
      "33586,3.3543\n",
      "33587,3.3450\n",
      "33588,3.3117\n",
      "33589,3.3730\n",
      "33590,3.3648\n",
      "33591,3.3310\n",
      "33592,3.3371\n",
      "33593,3.3497\n",
      "33594,3.3668\n",
      "33595,3.3443\n",
      "33596,3.3530\n",
      "33597,3.3713\n",
      "33598,3.3370\n",
      "33599,3.3712\n",
      "33600,3.3348\n",
      "(step: 33600, epoch: 0, block: 34406400), Train Loss: 3.2782, Val Loss: 3.3102\n",
      "Learning rate: 4.85789288e-05\n",
      "Scheduler step: 33600\n",
      "Checkpoint (step: 33600, epoch: 0, block: 34406400) Saved\n",
      "Step Documented\n",
      "14:13:51\n",
      "33601,3.3272\n",
      "33602,3.3459\n",
      "33603,3.3690\n",
      "33604,3.3372\n",
      "33605,3.3357\n",
      "33606,3.3488\n",
      "33607,3.3460\n",
      "33608,3.3549\n",
      "33609,3.3522\n",
      "33610,3.3471\n",
      "33611,3.3634\n",
      "33612,3.3694\n",
      "33613,3.3381\n",
      "33614,3.3739\n",
      "33615,3.3387\n",
      "33616,3.3423\n",
      "33617,3.3551\n",
      "33618,3.3528\n",
      "33619,3.3564\n",
      "33620,3.3670\n",
      "33621,3.3573\n",
      "33622,3.3780\n",
      "33623,3.3782\n",
      "33624,3.3571\n",
      "33625,3.3451\n",
      "33626,3.3422\n",
      "33627,3.3330\n",
      "33628,3.3382\n",
      "33629,3.3570\n",
      "33630,3.3673\n",
      "33631,3.3646\n",
      "33632,3.3413\n",
      "33633,3.3658\n",
      "33634,3.3776\n",
      "33635,3.3538\n",
      "33636,3.3615\n",
      "33637,3.3600\n",
      "33638,3.3627\n",
      "33639,3.3493\n",
      "33640,3.3441\n",
      "33641,3.3803\n",
      "33642,3.3745\n",
      "33643,3.3325\n",
      "33644,3.3674\n",
      "33645,3.3593\n",
      "33646,3.3601\n",
      "33647,3.3603\n",
      "33648,3.3583\n",
      "33649,3.3520\n",
      "33650,3.3284\n",
      "33651,3.3666\n",
      "33652,3.3531\n",
      "33653,3.3601\n",
      "33654,3.3517\n",
      "33655,3.3422\n",
      "33656,3.3663\n",
      "33657,3.3784\n",
      "33658,3.3365\n",
      "33659,3.3575\n",
      "33660,3.3606\n",
      "33661,3.3706\n",
      "33662,3.3600\n",
      "33663,3.3791\n",
      "33664,3.3440\n",
      "33665,3.3264\n",
      "33666,3.3448\n",
      "33667,3.3468\n",
      "33668,3.3525\n",
      "33669,3.3837\n",
      "33670,3.3794\n",
      "33671,3.3352\n",
      "33672,3.3570\n",
      "33673,3.3683\n",
      "33674,3.3810\n",
      "33675,3.3659\n",
      "33676,3.3369\n",
      "33677,3.3597\n",
      "33678,3.3713\n",
      "33679,3.3325\n",
      "33680,3.3532\n",
      "33681,3.3589\n",
      "33682,3.3683\n",
      "33683,3.3705\n",
      "33684,3.3823\n",
      "33685,3.3496\n",
      "33686,3.3599\n",
      "33687,3.3499\n",
      "33688,3.3435\n",
      "33689,3.3686\n",
      "33690,3.3750\n",
      "33691,3.3506\n",
      "33692,3.3463\n",
      "33693,3.3260\n",
      "33694,3.3466\n",
      "33695,3.3490\n",
      "33696,3.3534\n",
      "33697,3.3898\n",
      "33698,3.3654\n",
      "33699,3.3452\n",
      "33700,3.3516\n",
      "(step: 33700, epoch: 0, block: 34508800), Train Loss: 3.2869, Val Loss: 3.3078\n",
      "Learning rate: 4.83493883e-05\n",
      "Scheduler step: 33700\n",
      "Checkpoint (step: 33700, epoch: 0, block: 34508800) Saved\n",
      "Step Documented\n",
      "14:40:12\n",
      "33701,3.3586\n",
      "33702,3.3612\n",
      "33703,3.3481\n",
      "33704,3.3620\n",
      "33705,3.3719\n",
      "33706,3.3515\n",
      "33707,3.3659\n",
      "33708,3.3515\n",
      "33709,3.3394\n",
      "33710,3.3535\n",
      "33711,3.3418\n",
      "33712,3.3672\n",
      "33713,3.3533\n",
      "33714,3.3508\n",
      "33715,3.3646\n",
      "33716,3.3688\n",
      "33717,3.3725\n",
      "33718,3.3836\n",
      "33719,3.3768\n",
      "33720,3.3429\n",
      "33721,3.3507\n",
      "33722,3.3521\n",
      "33723,3.3507\n",
      "33724,3.3659\n",
      "33725,3.3781\n",
      "33726,3.3867\n",
      "33727,3.3564\n",
      "33728,3.3326\n",
      "33729,3.3509\n",
      "33730,3.3653\n",
      "33731,3.3579\n",
      "33732,3.3446\n",
      "33733,3.3723\n",
      "33734,3.3700\n",
      "33735,3.3693\n",
      "33736,3.3736\n",
      "33737,3.3418\n",
      "33738,3.3719\n",
      "33739,3.3479\n",
      "33740,3.3224\n",
      "33741,3.3891\n",
      "33742,3.3642\n",
      "33743,3.3466\n",
      "33744,3.3523\n",
      "33745,3.3818\n",
      "33746,3.3103\n",
      "33747,3.3364\n",
      "33748,3.3429\n",
      "33749,3.3604\n",
      "33750,3.3848\n",
      "33751,3.3601\n",
      "33752,3.3348\n",
      "33753,3.3398\n",
      "33754,3.3441\n",
      "33755,3.3526\n",
      "33756,3.3580\n",
      "33757,3.3507\n",
      "33758,3.3682\n",
      "33759,3.3808\n",
      "33760,3.3605\n",
      "33761,3.3646\n",
      "33762,3.3494\n",
      "33763,3.3392\n",
      "33764,3.3465\n",
      "33765,3.3375\n",
      "33766,3.3835\n",
      "33767,3.3501\n",
      "33768,3.3563\n",
      "33769,3.3578\n",
      "33770,3.3479\n",
      "33771,3.3808\n",
      "33772,3.3487\n",
      "33773,3.3461\n",
      "33774,3.3456\n",
      "33775,3.3674\n",
      "33776,3.3844\n",
      "33777,3.3490\n",
      "33778,3.3589\n",
      "33779,3.3459\n",
      "33780,3.3677\n",
      "33781,3.3571\n",
      "33782,3.3624\n",
      "33783,3.3560\n",
      "33784,3.3660\n",
      "33785,3.3511\n",
      "33786,3.3608\n",
      "33787,3.3843\n",
      "33788,3.3544\n",
      "33789,3.3580\n",
      "33790,3.3347\n",
      "33791,3.3556\n",
      "33792,3.3782\n",
      "33793,3.3629\n",
      "33794,3.3789\n",
      "33795,3.3355\n",
      "33796,3.3968\n",
      "33797,3.3641\n",
      "33798,3.3700\n",
      "33799,3.3630\n",
      "33800,3.3513\n",
      "(step: 33800, epoch: 0, block: 34611200), Train Loss: 3.2542, Val Loss: 3.2837\n",
      "Learning rate: 4.81199906e-05\n",
      "Scheduler step: 33800\n",
      "Checkpoint (step: 33800, epoch: 0, block: 34611200) Saved\n",
      "Step Documented\n",
      "15:06:33\n",
      "33801,3.3689\n",
      "33802,3.3646\n",
      "33803,3.3439\n",
      "33804,3.3446\n",
      "33805,3.3239\n",
      "33806,3.3369\n",
      "33807,3.3510\n",
      "33808,3.3565\n",
      "33809,3.3869\n",
      "33810,3.3723\n",
      "33811,3.3698\n",
      "33812,3.3560\n",
      "33813,3.3652\n",
      "33814,3.3818\n",
      "33815,3.3143\n",
      "33816,3.3535\n",
      "33817,3.3577\n",
      "33818,3.3688\n",
      "33819,3.3800\n",
      "33820,3.3756\n",
      "33821,3.3665\n",
      "33822,3.3675\n",
      "33823,3.3353\n",
      "33824,3.3307\n",
      "33825,3.3495\n",
      "33826,3.3704\n",
      "33827,3.3601\n",
      "33828,3.3463\n",
      "33829,3.3711\n",
      "33830,3.3693\n",
      "33831,3.3514\n",
      "33832,3.3410\n",
      "33833,3.3700\n",
      "33834,3.3296\n",
      "33835,3.3376\n",
      "33836,3.3570\n",
      "33837,3.3771\n",
      "33838,3.3895\n",
      "33839,3.3595\n",
      "33840,3.3560\n",
      "33841,3.3510\n",
      "33842,3.3711\n",
      "33843,3.3728\n",
      "33844,3.3778\n",
      "33845,3.3677\n",
      "33846,3.3410\n",
      "33847,3.3765\n",
      "33848,3.3117\n",
      "33849,3.3297\n",
      "33850,3.3834\n",
      "33851,3.3413\n",
      "33852,3.3526\n",
      "33853,3.3654\n",
      "33854,3.3672\n",
      "33855,3.3622\n",
      "33856,3.3462\n",
      "33857,3.3424\n",
      "33858,3.3392\n",
      "33859,3.3512\n",
      "33860,3.3624\n",
      "33861,3.3539\n",
      "33862,3.3809\n",
      "33863,3.3793\n",
      "33864,3.3343\n",
      "33865,3.3158\n",
      "33866,3.3766\n",
      "33867,3.3663\n",
      "33868,3.3596\n",
      "33869,3.3391\n",
      "33870,3.3879\n",
      "33871,3.3568\n",
      "33872,3.3859\n",
      "33873,3.3558\n",
      "33874,3.3856\n",
      "33875,3.3393\n",
      "33876,3.3363\n",
      "33877,3.3747\n",
      "33878,3.3267\n",
      "33879,3.3538\n",
      "33880,3.3407\n",
      "33881,3.3726\n",
      "33882,3.3597\n",
      "33883,3.3616\n",
      "33884,3.3568\n",
      "33885,3.3710\n",
      "33886,3.3465\n",
      "33887,3.3457\n",
      "33888,3.3581\n",
      "33889,3.3540\n",
      "33890,3.3503\n",
      "33891,3.3405\n",
      "33892,3.3486\n",
      "33893,3.3456\n",
      "33894,3.3448\n",
      "33895,3.3774\n",
      "33896,3.3540\n",
      "33897,3.3821\n",
      "33898,3.3543\n",
      "33899,3.3623\n",
      "33900,3.3491\n",
      "(step: 33900, epoch: 0, block: 34713600), Train Loss: 3.2754, Val Loss: 3.3106\n",
      "Learning rate: 4.78907414e-05\n",
      "Scheduler step: 33900\n",
      "Checkpoint (step: 33900, epoch: 0, block: 34713600) Saved\n",
      "Step Documented\n",
      "15:32:52\n",
      "33901,3.3654\n",
      "33902,3.3497\n",
      "33903,3.3378\n",
      "33904,3.3505\n",
      "33905,3.3376\n",
      "33906,3.3926\n",
      "33907,3.3758\n",
      "33908,3.3501\n",
      "33909,3.3954\n",
      "33910,3.3970\n",
      "33911,3.3582\n",
      "33912,3.3209\n",
      "33913,3.3599\n",
      "33914,3.3784\n",
      "33915,3.3637\n",
      "33916,3.3224\n",
      "33917,3.3648\n",
      "33918,3.3494\n",
      "33919,3.3514\n",
      "33920,3.3486\n",
      "33921,3.3681\n",
      "33922,3.3743\n",
      "33923,3.3591\n",
      "33924,3.3809\n",
      "33925,3.3342\n",
      "33926,3.3567\n",
      "33927,3.3300\n",
      "33928,3.3694\n",
      "33929,3.3472\n",
      "33930,3.3725\n",
      "33931,3.3775\n",
      "33932,3.3485\n",
      "33933,3.3487\n",
      "33934,3.3343\n",
      "33935,3.3378\n",
      "33936,3.3359\n",
      "33937,3.3534\n",
      "33938,3.3358\n",
      "33939,3.3444\n",
      "33940,3.3561\n",
      "33941,3.3690\n",
      "33942,3.3586\n",
      "33943,3.3393\n",
      "33944,3.3623\n",
      "33945,3.3753\n",
      "33946,3.3543\n",
      "33947,3.3577\n",
      "33948,3.3534\n",
      "33949,3.3637\n",
      "33950,3.3757\n",
      "33951,3.3392\n",
      "33952,3.3475\n",
      "33953,3.3472\n",
      "33954,3.3574\n",
      "33955,3.3655\n",
      "33956,3.3879\n",
      "33957,3.3938\n",
      "33958,3.3700\n",
      "33959,3.3696\n",
      "33960,3.3667\n",
      "33961,3.3595\n",
      "33962,3.3325\n",
      "33963,3.3197\n",
      "33964,3.3916\n",
      "33965,3.3672\n",
      "33966,3.3576\n",
      "33967,3.3786\n",
      "33968,3.3766\n",
      "33969,3.3527\n",
      "33970,3.3588\n",
      "33971,3.3352\n",
      "33972,3.3415\n",
      "33973,3.3628\n",
      "33974,3.3858\n",
      "33975,3.3605\n",
      "33976,3.3528\n",
      "33977,3.3494\n",
      "33978,3.3727\n",
      "33979,3.3590\n",
      "33980,3.3312\n",
      "33981,3.3395\n",
      "33982,3.3449\n",
      "33983,3.3493\n",
      "33984,3.3539\n",
      "33985,3.3449\n",
      "33986,3.3593\n",
      "33987,3.3556\n",
      "33988,3.3302\n",
      "33989,3.3766\n",
      "33990,3.3747\n",
      "33991,3.3539\n",
      "33992,3.3738\n",
      "33993,3.3815\n",
      "33994,3.3982\n",
      "33995,3.3441\n",
      "33996,3.3680\n",
      "33997,3.3917\n",
      "33998,3.3411\n",
      "33999,3.3628\n",
      "34000,3.3728\n",
      "(step: 34000, epoch: 0, block: 34816000), Train Loss: 3.2891, Val Loss: 3.3128\n",
      "Learning rate: 4.76616466e-05\n",
      "Scheduler step: 34000\n",
      "Checkpoint (step: 34000, epoch: 0, block: 34816000) Saved\n",
      "Step Documented\n",
      "15:59:14\n",
      "34001,3.3581\n",
      "34002,3.3624\n",
      "34003,3.3919\n",
      "34004,3.3422\n",
      "34005,3.3609\n",
      "34006,3.3597\n",
      "34007,3.3933\n",
      "34008,3.3489\n",
      "34009,3.3341\n",
      "34010,3.3370\n",
      "34011,3.3823\n",
      "34012,3.3697\n",
      "34013,3.3647\n",
      "34014,3.3852\n",
      "34015,3.3442\n",
      "34016,3.3679\n",
      "34017,3.3557\n",
      "34018,3.3560\n",
      "34019,3.3773\n",
      "34020,3.3647\n",
      "34021,3.3506\n",
      "34022,3.3559\n",
      "34023,3.3758\n",
      "34024,3.3553\n",
      "34025,3.3782\n",
      "34026,3.3717\n",
      "34027,3.3515\n",
      "34028,3.3408\n",
      "34029,3.3950\n",
      "34030,3.3396\n",
      "34031,3.3654\n",
      "34032,3.3423\n",
      "34033,3.3363\n",
      "34034,3.3690\n",
      "34035,3.3674\n",
      "34036,3.3305\n",
      "34037,3.3589\n",
      "34038,3.3437\n",
      "34039,3.3733\n",
      "34040,3.3825\n",
      "34041,3.3624\n",
      "34042,3.3547\n",
      "34043,3.3573\n",
      "34044,3.3615\n",
      "34045,3.3556\n",
      "34046,3.3507\n",
      "34047,3.3552\n",
      "34048,3.3546\n",
      "34049,3.3444\n",
      "34050,3.3726\n",
      "34051,3.3505\n",
      "34052,3.3658\n",
      "34053,3.3574\n",
      "34054,3.3707\n",
      "34055,3.3346\n",
      "34056,3.3876\n",
      "34057,3.3679\n",
      "34058,3.3571\n",
      "34059,3.3391\n",
      "34060,3.3631\n",
      "34061,3.3841\n",
      "34062,3.3432\n",
      "34063,3.3618\n",
      "34064,3.3481\n",
      "34065,3.3478\n",
      "34066,3.3434\n",
      "34067,3.3356\n",
      "34068,3.3513\n",
      "34069,3.3776\n",
      "34070,3.3691\n",
      "34071,3.3743\n",
      "34072,3.3674\n",
      "34073,3.3518\n",
      "34074,3.3623\n",
      "34075,3.3626\n",
      "34076,3.3616\n",
      "34077,3.3504\n",
      "34078,3.3713\n",
      "34079,3.3341\n",
      "34080,3.3667\n",
      "34081,3.3613\n",
      "34082,3.3405\n",
      "34083,3.3473\n",
      "34084,3.3493\n",
      "34085,3.3709\n",
      "34086,3.3481\n",
      "34087,3.3387\n",
      "34088,3.3389\n",
      "34089,3.3256\n",
      "34090,3.3343\n",
      "34091,3.3487\n",
      "34092,3.3265\n",
      "34093,3.3756\n",
      "34094,3.3674\n",
      "34095,3.3506\n",
      "34096,3.3524\n",
      "34097,3.3536\n",
      "34098,3.3276\n",
      "34099,3.3469\n",
      "34100,3.3195\n",
      "(step: 34100, epoch: 0, block: 34918400), Train Loss: 3.2907, Val Loss: 3.2797\n",
      "Learning rate: 4.74327120e-05\n",
      "Scheduler step: 34100\n",
      "Checkpoint (step: 34100, epoch: 0, block: 34918400) Saved\n",
      "Step Documented\n",
      "16:25:34\n",
      "34101,3.3815\n",
      "34102,3.3603\n",
      "34103,3.3418\n",
      "34104,3.3796\n",
      "34105,3.3499\n",
      "34106,3.3382\n",
      "34107,3.3538\n",
      "34108,3.3695\n",
      "34109,3.3685\n",
      "34110,3.3826\n",
      "34111,3.3713\n",
      "34112,3.3577\n",
      "34113,3.3649\n",
      "34114,3.3633\n",
      "34115,3.3453\n",
      "34116,3.3509\n",
      "34117,3.3697\n",
      "34118,3.3760\n",
      "34119,3.3647\n",
      "34120,3.3501\n",
      "34121,3.3503\n",
      "34122,3.3364\n",
      "34123,3.3535\n",
      "34124,3.3471\n",
      "34125,3.3660\n",
      "34126,3.3735\n",
      "34127,3.3601\n",
      "34128,3.3634\n",
      "34129,3.3595\n",
      "34130,3.3358\n",
      "34131,3.3673\n",
      "34132,3.3519\n",
      "34133,3.3818\n",
      "34134,3.3510\n",
      "34135,3.3688\n",
      "34136,3.3643\n",
      "34137,3.3512\n",
      "34138,3.3679\n",
      "34139,3.3557\n",
      "34140,3.3413\n",
      "34141,3.3775\n",
      "34142,3.3426\n",
      "34143,3.3435\n",
      "34144,3.3587\n",
      "34145,3.3729\n",
      "34146,3.3622\n",
      "34147,3.3806\n",
      "34148,3.3256\n",
      "34149,3.3664\n",
      "34150,3.3631\n",
      "34151,3.3368\n",
      "34152,3.3614\n",
      "34153,3.3679\n",
      "34154,3.3602\n",
      "34155,3.3325\n",
      "34156,3.3330\n",
      "34157,3.3745\n",
      "34158,3.3491\n",
      "34159,3.3403\n",
      "34160,3.3582\n",
      "34161,3.3424\n",
      "34162,3.3605\n",
      "34163,3.3586\n",
      "34164,3.3728\n",
      "34165,3.3740\n",
      "34166,3.3652\n",
      "34167,3.3547\n",
      "34168,3.3574\n",
      "34169,3.3321\n",
      "34170,3.3766\n",
      "34171,3.3936\n",
      "34172,3.3657\n",
      "34173,3.3380\n",
      "34174,3.3561\n",
      "34175,3.3580\n",
      "34176,3.3557\n",
      "34177,3.3755\n",
      "34178,3.3466\n",
      "34179,3.3365\n",
      "34180,3.3579\n",
      "34181,3.3440\n",
      "34182,3.3690\n",
      "34183,3.3539\n",
      "34184,3.3578\n",
      "34185,3.3354\n",
      "34186,3.3648\n",
      "34187,3.3597\n",
      "34188,3.3360\n",
      "34189,3.3783\n",
      "34190,3.3317\n",
      "34191,3.3649\n",
      "34192,3.3504\n",
      "34193,3.3531\n",
      "34194,3.3886\n",
      "34195,3.3544\n",
      "34196,3.3403\n",
      "34197,3.3547\n",
      "34198,3.3417\n",
      "34199,3.3454\n",
      "34200,3.3737\n",
      "(step: 34200, epoch: 0, block: 35020800), Train Loss: 3.2830, Val Loss: 3.3265\n",
      "Learning rate: 4.72039432e-05\n",
      "Scheduler step: 34200\n",
      "Checkpoint (step: 34200, epoch: 0, block: 35020800) Saved\n",
      "Step Documented\n",
      "16:51:56\n",
      "34201,3.3628\n",
      "34202,3.3576\n",
      "34203,3.3505\n",
      "34204,3.3637\n",
      "34205,3.3651\n",
      "34206,3.3355\n",
      "34207,3.3853\n",
      "34208,3.3214\n",
      "34209,3.3308\n",
      "34210,3.3429\n",
      "34211,3.3495\n",
      "34212,3.3555\n",
      "34213,3.3412\n",
      "34214,3.3611\n",
      "34215,3.3882\n",
      "34216,3.3485\n",
      "34217,3.3594\n",
      "34218,3.3627\n",
      "34219,3.3613\n",
      "34220,3.3658\n",
      "34221,3.3709\n",
      "34222,3.3362\n",
      "34223,3.3309\n",
      "34224,3.3552\n",
      "34225,3.3785\n",
      "34226,3.3401\n",
      "34227,3.3368\n",
      "34228,3.3713\n",
      "34229,3.3817\n",
      "34230,3.3387\n",
      "34231,3.3888\n",
      "34232,3.3244\n",
      "34233,3.3625\n",
      "34234,3.3376\n",
      "34235,3.3625\n",
      "34236,3.3574\n",
      "34237,3.3589\n",
      "34238,3.3556\n",
      "34239,3.3712\n",
      "34240,3.3495\n",
      "34241,3.3839\n",
      "34242,3.3730\n",
      "34243,3.3477\n",
      "34244,3.3565\n",
      "34245,3.3462\n",
      "34246,3.3359\n",
      "34247,3.3395\n",
      "34248,3.3546\n",
      "34249,3.3555\n",
      "34250,3.3716\n",
      "34251,3.3353\n",
      "34252,3.3370\n",
      "34253,3.3548\n",
      "34254,3.3496\n",
      "34255,3.3692\n",
      "34256,3.3193\n",
      "34257,3.3636\n",
      "34258,3.3398\n",
      "34259,3.3600\n",
      "34260,3.3685\n",
      "34261,3.3614\n",
      "34262,3.3527\n",
      "34263,3.3716\n",
      "34264,3.3670\n",
      "34265,3.3602\n",
      "34266,3.3627\n",
      "34267,3.3673\n",
      "34268,3.3466\n",
      "34269,3.3699\n",
      "34270,3.3656\n",
      "34271,3.3387\n",
      "34272,3.3870\n",
      "34273,3.3761\n",
      "34274,3.3514\n",
      "34275,3.3712\n",
      "34276,3.3568\n",
      "34277,3.3575\n",
      "34278,3.3675\n",
      "34279,3.3330\n",
      "34280,3.3415\n",
      "34281,3.3507\n",
      "34282,3.3893\n",
      "34283,3.3292\n",
      "34284,3.3770\n",
      "34285,3.3494\n",
      "34286,3.3663\n",
      "34287,3.3574\n",
      "34288,3.3526\n",
      "34289,3.3623\n",
      "34290,3.3360\n",
      "34291,3.3682\n",
      "34292,3.3830\n",
      "34293,3.3313\n",
      "34294,3.3691\n",
      "34295,3.3351\n",
      "34296,3.3415\n",
      "34297,3.3533\n",
      "34298,3.3771\n",
      "34299,3.3526\n",
      "34300,3.3145\n",
      "(step: 34300, epoch: 0, block: 35123200), Train Loss: 3.2596, Val Loss: 3.2809\n",
      "Learning rate: 4.69753462e-05\n",
      "Scheduler step: 34300\n",
      "Checkpoint (step: 34300, epoch: 0, block: 35123200) Saved\n",
      "Step Documented\n",
      "17:18:17\n",
      "34301,3.3404\n",
      "34302,3.3181\n",
      "34303,3.3709\n",
      "34304,3.3896\n",
      "34305,3.3725\n",
      "34306,3.3622\n",
      "34307,3.3424\n",
      "34308,3.3611\n",
      "34309,3.3226\n",
      "34310,3.3525\n",
      "34311,3.3762\n",
      "34312,3.3336\n",
      "34313,3.3409\n",
      "34314,3.4086\n",
      "34315,3.3471\n",
      "34316,3.3508\n",
      "34317,3.3162\n",
      "34318,3.3476\n",
      "34319,3.3482\n",
      "34320,3.3811\n",
      "34321,3.3516\n",
      "34322,3.3566\n",
      "34323,3.3478\n",
      "34324,3.3373\n",
      "34325,3.3474\n",
      "34326,3.3495\n",
      "34327,3.3532\n",
      "34328,3.3857\n",
      "34329,3.3441\n",
      "34330,3.3425\n",
      "34331,3.3625\n",
      "34332,3.3602\n",
      "34333,3.3419\n",
      "34334,3.3440\n",
      "34335,3.3487\n",
      "34336,3.3617\n",
      "34337,3.3283\n",
      "34338,3.3786\n",
      "34339,3.3626\n",
      "34340,3.3552\n",
      "34341,3.3828\n",
      "34342,3.3450\n",
      "34343,3.3679\n",
      "34344,3.3945\n",
      "34345,3.3630\n",
      "34346,3.3350\n",
      "34347,3.3449\n",
      "34348,3.3473\n",
      "34349,3.3476\n",
      "34350,3.3864\n",
      "34351,3.3582\n",
      "34352,3.3593\n",
      "34353,3.3601\n",
      "34354,3.3414\n",
      "34355,3.3902\n",
      "34356,3.3498\n",
      "34357,3.3584\n",
      "34358,3.3779\n",
      "34359,3.3633\n",
      "34360,3.3690\n",
      "34361,3.3584\n",
      "34362,3.3621\n",
      "34363,3.3612\n",
      "34364,3.3675\n",
      "34365,3.3369\n",
      "34366,3.3837\n",
      "34367,3.3737\n",
      "34368,3.3204\n",
      "34369,3.3534\n",
      "34370,3.3663\n",
      "34371,3.3946\n",
      "34372,3.3608\n",
      "34373,3.3783\n",
      "34374,3.3486\n",
      "34375,3.3498\n",
      "34376,3.3661\n",
      "34377,3.3610\n",
      "34378,3.3410\n",
      "34379,3.3855\n",
      "34380,3.3552\n",
      "34381,3.3362\n",
      "34382,3.3225\n",
      "34383,3.3513\n",
      "34384,3.3566\n",
      "34385,3.3540\n",
      "34386,3.3648\n",
      "34387,3.3540\n",
      "34388,3.3127\n",
      "34389,3.3341\n",
      "34390,3.3570\n",
      "34391,3.3807\n",
      "34392,3.3568\n",
      "34393,3.3594\n",
      "34394,3.3602\n",
      "34395,3.3848\n",
      "34396,3.3835\n",
      "34397,3.3434\n",
      "34398,3.3860\n",
      "34399,3.3466\n",
      "34400,3.3234\n",
      "(step: 34400, epoch: 0, block: 35225600), Train Loss: 3.2705, Val Loss: 3.3019\n",
      "Learning rate: 4.67469267e-05\n",
      "Scheduler step: 34400\n",
      "Checkpoint (step: 34400, epoch: 0, block: 35225600) Saved\n",
      "Step Documented\n",
      "17:44:36\n",
      "34401,3.3658\n",
      "34402,3.3633\n",
      "34403,3.3773\n",
      "34404,3.3462\n",
      "34405,3.3578\n",
      "34406,3.3495\n",
      "34407,3.3531\n",
      "34408,3.3798\n",
      "34409,3.3389\n",
      "34410,3.3587\n",
      "34411,3.3690\n",
      "34412,3.3581\n",
      "34413,3.3568\n",
      "34414,3.3468\n",
      "34415,3.3367\n",
      "34416,3.3247\n",
      "34417,3.3392\n",
      "34418,3.3428\n",
      "34419,3.3735\n",
      "34420,3.3505\n",
      "34421,3.3575\n",
      "34422,3.3659\n",
      "34423,3.3646\n",
      "34424,3.3504\n",
      "34425,3.3709\n",
      "34426,3.3109\n",
      "34427,3.3590\n",
      "34428,3.3717\n",
      "34429,3.3185\n",
      "34430,3.3875\n",
      "34431,3.3674\n",
      "34432,3.3566\n",
      "34433,3.3486\n",
      "34434,3.3463\n",
      "34435,3.3774\n",
      "34436,3.3580\n",
      "34437,3.3693\n",
      "34438,3.3440\n",
      "34439,3.3457\n",
      "34440,3.3725\n",
      "34441,3.3392\n",
      "34442,3.3635\n",
      "34443,3.3806\n",
      "34444,3.3696\n",
      "34445,3.3547\n",
      "34446,3.3958\n",
      "34447,3.3629\n",
      "34448,3.3619\n",
      "34449,3.3445\n",
      "34450,3.3523\n",
      "34451,3.3314\n",
      "34452,3.3640\n",
      "34453,3.3367\n",
      "34454,3.3344\n",
      "34455,3.3505\n",
      "34456,3.3479\n",
      "34457,3.3489\n",
      "34458,3.3337\n",
      "34459,3.3617\n",
      "34460,3.3438\n",
      "34461,3.3958\n",
      "34462,3.3223\n",
      "34463,3.3508\n",
      "34464,3.3482\n",
      "34465,3.3431\n",
      "34466,3.3404\n",
      "34467,3.3597\n",
      "34468,3.3596\n",
      "34469,3.3473\n",
      "34470,3.3613\n",
      "34471,3.3549\n",
      "34472,3.3751\n",
      "34473,3.3663\n",
      "34474,3.3584\n",
      "34475,3.3569\n",
      "34476,3.3459\n",
      "34477,3.3687\n",
      "34478,3.3461\n",
      "34479,3.3133\n",
      "34480,3.3451\n",
      "34481,3.3790\n",
      "34482,3.3376\n",
      "34483,3.3562\n",
      "34484,3.3650\n",
      "34485,3.3599\n",
      "34486,3.3688\n",
      "34487,3.3489\n",
      "34488,3.3466\n",
      "34489,3.3975\n",
      "34490,3.3443\n",
      "34491,3.3360\n",
      "34492,3.3589\n",
      "34493,3.3690\n",
      "34494,3.3615\n",
      "34495,3.3484\n",
      "34496,3.3455\n",
      "34497,3.3446\n",
      "34498,3.3507\n",
      "34499,3.3692\n",
      "34500,3.3739\n",
      "(step: 34500, epoch: 0, block: 35328000), Train Loss: 3.2811, Val Loss: 3.3051\n",
      "Learning rate: 4.65186904e-05\n",
      "Scheduler step: 34500\n",
      "Checkpoint (step: 34500, epoch: 0, block: 35328000) Saved\n",
      "Step Documented\n",
      "18:10:57\n",
      "34501,3.3478\n",
      "34502,3.3562\n",
      "34503,3.3913\n",
      "34504,3.3744\n",
      "34505,3.3569\n",
      "34506,3.3653\n",
      "34507,3.3239\n",
      "34508,3.3605\n",
      "34509,3.3384\n",
      "34510,3.3770\n",
      "34511,3.3493\n",
      "34512,3.3499\n",
      "34513,3.3594\n",
      "34514,3.3821\n",
      "34515,3.3671\n",
      "34516,3.3466\n",
      "34517,3.3624\n",
      "34518,3.3256\n",
      "34519,3.3751\n",
      "34520,3.3484\n",
      "34521,3.3658\n",
      "34522,3.3383\n",
      "34523,3.3484\n",
      "34524,3.3664\n",
      "34525,3.3402\n",
      "34526,3.3646\n",
      "34527,3.3478\n",
      "34528,3.3660\n",
      "34529,3.3871\n",
      "34530,3.3463\n",
      "34531,3.3444\n",
      "34532,3.3512\n",
      "34533,3.3555\n",
      "34534,3.3453\n",
      "34535,3.3322\n",
      "34536,3.3595\n",
      "34537,3.3418\n",
      "34538,3.3777\n",
      "34539,3.3666\n",
      "34540,3.3610\n",
      "34541,3.3349\n",
      "34542,3.3733\n",
      "34543,3.3420\n",
      "34544,3.3345\n",
      "34545,3.3691\n",
      "34546,3.3482\n",
      "34547,3.3640\n",
      "34548,3.3750\n",
      "34549,3.3425\n",
      "34550,3.3728\n",
      "34551,3.3690\n",
      "34552,3.3251\n",
      "34553,3.3491\n",
      "34554,3.3364\n",
      "34555,3.3509\n",
      "34556,3.3500\n",
      "34557,3.3599\n",
      "34558,3.3465\n",
      "34559,3.3527\n",
      "34560,3.3408\n",
      "34561,3.3701\n",
      "34562,3.3771\n",
      "34563,3.3604\n",
      "34564,3.3679\n",
      "34565,3.3245\n",
      "34566,3.3676\n",
      "34567,3.3786\n",
      "34568,3.3579\n",
      "34569,3.3506\n",
      "34570,3.3738\n",
      "34571,3.3709\n",
      "34572,3.3694\n",
      "34573,3.3613\n",
      "34574,3.4103\n",
      "34575,3.3705\n",
      "34576,3.3474\n",
      "34577,3.3534\n",
      "34578,3.3692\n",
      "34579,3.3196\n",
      "34580,3.3434\n",
      "34581,3.3636\n",
      "34582,3.3678\n",
      "34583,3.3209\n",
      "34584,3.3453\n",
      "34585,3.3627\n",
      "34586,3.3305\n",
      "34587,3.3658\n",
      "34588,3.3564\n",
      "34589,3.3229\n",
      "34590,3.3542\n",
      "34591,3.3623\n",
      "34592,3.3514\n",
      "34593,3.3466\n",
      "34594,3.3636\n",
      "34595,3.3490\n",
      "34596,3.3603\n",
      "34597,3.3682\n",
      "34598,3.3455\n",
      "34599,3.3540\n",
      "34600,3.3865\n",
      "(step: 34600, epoch: 0, block: 35430400), Train Loss: 3.2769, Val Loss: 3.3315\n",
      "Learning rate: 4.62906431e-05\n",
      "Scheduler step: 34600\n",
      "Checkpoint (step: 34600, epoch: 0, block: 35430400) Saved\n",
      "Step Documented\n",
      "18:37:17\n",
      "34601,3.3524\n",
      "34602,3.3373\n",
      "34603,3.3690\n",
      "34604,3.3637\n",
      "34605,3.3636\n",
      "34606,3.3343\n",
      "34607,3.3640\n",
      "34608,3.3854\n",
      "34609,3.3756\n",
      "34610,3.3838\n",
      "34611,3.3624\n",
      "34612,3.3259\n",
      "34613,3.3644\n",
      "34614,3.3715\n",
      "34615,3.3558\n",
      "34616,3.3687\n",
      "34617,3.3436\n",
      "34618,3.3515\n",
      "34619,3.3661\n",
      "34620,3.3543\n",
      "34621,3.3536\n",
      "34622,3.3214\n",
      "34623,3.3339\n",
      "34624,3.3567\n",
      "34625,3.3462\n",
      "34626,3.3734\n",
      "34627,3.3806\n",
      "34628,3.3897\n",
      "34629,3.3459\n",
      "34630,3.3563\n",
      "34631,3.3624\n",
      "34632,3.3362\n",
      "34633,3.3268\n",
      "34634,3.3388\n",
      "34635,3.3375\n",
      "34636,3.3407\n",
      "34637,3.3589\n",
      "34638,3.3707\n",
      "34639,3.3477\n",
      "34640,3.3661\n",
      "34641,3.3497\n",
      "34642,3.3520\n",
      "34643,3.3597\n",
      "34644,3.3646\n",
      "34645,3.3523\n",
      "34646,3.3636\n",
      "34647,3.3709\n",
      "34648,3.3424\n",
      "34649,3.3770\n",
      "34650,3.3440\n",
      "34651,3.3703\n",
      "34652,3.4009\n",
      "34653,3.3860\n",
      "34654,3.3318\n",
      "34655,3.3392\n",
      "34656,3.3710\n",
      "34657,3.3526\n",
      "34658,3.3755\n",
      "34659,3.3759\n",
      "34660,3.3638\n",
      "34661,3.3750\n",
      "34662,3.3381\n",
      "34663,3.3853\n",
      "34664,3.3301\n",
      "34665,3.3781\n",
      "34666,3.3777\n",
      "34667,3.3722\n",
      "34668,3.3600\n",
      "34669,3.3596\n",
      "34670,3.3327\n",
      "34671,3.3797\n",
      "34672,3.3493\n",
      "34673,3.3551\n",
      "34674,3.3644\n",
      "34675,3.3453\n",
      "34676,3.3709\n",
      "34677,3.3502\n",
      "34678,3.3685\n",
      "34679,3.3186\n",
      "34680,3.3733\n",
      "34681,3.3554\n",
      "34682,3.3313\n",
      "34683,3.3283\n",
      "34684,3.3624\n",
      "34685,3.3469\n",
      "34686,3.3522\n",
      "34687,3.3619\n",
      "34688,3.3344\n",
      "34689,3.3979\n",
      "34690,3.3539\n",
      "34691,3.3508\n",
      "34692,3.3835\n",
      "34693,3.3707\n",
      "34694,3.3837\n",
      "34695,3.3294\n",
      "34696,3.3731\n",
      "34697,3.3465\n",
      "34698,3.3326\n",
      "34699,3.3393\n",
      "34700,3.3673\n",
      "(step: 34700, epoch: 0, block: 35532800), Train Loss: 3.2656, Val Loss: 3.2810\n",
      "Learning rate: 4.60627907e-05\n",
      "Scheduler step: 34700\n",
      "Checkpoint (step: 34700, epoch: 0, block: 35532800) Saved\n",
      "Step Documented\n",
      "19:03:36\n",
      "34701,3.3303\n",
      "34702,3.3562\n",
      "34703,3.3652\n",
      "34704,3.3603\n",
      "34705,3.3677\n",
      "34706,3.3777\n",
      "34707,3.3549\n",
      "34708,3.3554\n",
      "34709,3.3628\n",
      "34710,3.3594\n",
      "34711,3.3569\n",
      "34712,3.3529\n",
      "34713,3.3659\n",
      "34714,3.3423\n",
      "34715,3.3778\n",
      "34716,3.3624\n",
      "34717,3.3701\n",
      "34718,3.3493\n",
      "34719,3.3443\n",
      "34720,3.3611\n",
      "34721,3.3542\n",
      "34722,3.3457\n",
      "34723,3.3188\n",
      "34724,3.3588\n",
      "34725,3.3357\n",
      "34726,3.3235\n",
      "34727,3.3540\n",
      "34728,3.3712\n",
      "34729,3.3495\n",
      "34730,3.3479\n",
      "34731,3.3517\n",
      "34732,3.3495\n",
      "34733,3.3646\n",
      "34734,3.3550\n",
      "34735,3.3400\n",
      "34736,3.3516\n",
      "34737,3.3716\n",
      "34738,3.3637\n",
      "34739,3.3552\n",
      "34740,3.3324\n",
      "34741,3.3821\n",
      "34742,3.3713\n",
      "34743,3.3441\n",
      "34744,3.3616\n",
      "34745,3.3527\n",
      "34746,3.3403\n",
      "34747,3.3632\n",
      "34748,3.3361\n",
      "34749,3.3466\n",
      "34750,3.3275\n",
      "34751,3.3361\n",
      "34752,3.3320\n",
      "34753,3.3523\n",
      "34754,3.3619\n",
      "34755,3.3416\n",
      "34756,3.3528\n",
      "34757,3.3595\n",
      "34758,3.3578\n",
      "34759,3.3546\n",
      "34760,3.3683\n",
      "34761,3.3470\n",
      "34762,3.3490\n",
      "34763,3.3583\n",
      "34764,3.3644\n",
      "34765,3.3491\n",
      "34766,3.3689\n",
      "34767,3.3567\n",
      "34768,3.3482\n",
      "34769,3.3392\n",
      "34770,3.3708\n",
      "34771,3.3267\n",
      "34772,3.3560\n",
      "34773,3.3372\n",
      "34774,3.3659\n",
      "34775,3.3727\n",
      "34776,3.3680\n",
      "34777,3.3990\n",
      "34778,3.3539\n",
      "34779,3.3557\n",
      "34780,3.3363\n",
      "34781,3.3422\n",
      "34782,3.3228\n",
      "34783,3.3772\n",
      "34784,3.3480\n",
      "34785,3.3419\n",
      "34786,3.3692\n",
      "34787,3.3498\n",
      "34788,3.3369\n",
      "34789,3.3570\n",
      "34790,3.3467\n",
      "34791,3.3306\n",
      "34792,3.3635\n",
      "34793,3.3559\n",
      "34794,3.3366\n",
      "34795,3.3498\n",
      "34796,3.3383\n",
      "34797,3.3502\n",
      "34798,3.3591\n",
      "34799,3.3573\n",
      "34800,3.3476\n",
      "(step: 34800, epoch: 0, block: 35635200), Train Loss: 3.2809, Val Loss: 3.2815\n",
      "Learning rate: 4.58351388e-05\n",
      "Scheduler step: 34800\n",
      "Checkpoint (step: 34800, epoch: 0, block: 35635200) Saved\n",
      "Step Documented\n",
      "19:29:58\n",
      "34801,3.3649\n",
      "34802,3.3333\n",
      "34803,3.3936\n",
      "34804,3.3366\n",
      "34805,3.3668\n",
      "34806,3.3833\n",
      "34807,3.3296\n",
      "34808,3.3912\n",
      "34809,3.3637\n",
      "34810,3.3645\n",
      "34811,3.3710\n",
      "34812,3.3850\n",
      "34813,3.3540\n",
      "34814,3.3545\n",
      "34815,3.3671\n",
      "34816,3.3497\n",
      "34817,3.3336\n",
      "34818,3.3460\n",
      "34819,3.3587\n",
      "34820,3.3496\n",
      "34821,3.3423\n",
      "34822,3.3615\n",
      "34823,3.3590\n",
      "34824,3.3709\n",
      "34825,3.3434\n",
      "34826,3.3455\n",
      "34827,3.3645\n",
      "34828,3.3599\n",
      "34829,3.3603\n",
      "34830,3.3372\n",
      "34831,3.3462\n",
      "34832,3.3297\n",
      "34833,3.3646\n",
      "34834,3.3723\n",
      "34835,3.3447\n",
      "34836,3.3380\n",
      "34837,3.3549\n",
      "34838,3.3440\n",
      "34839,3.3392\n",
      "34840,3.3217\n",
      "34841,3.3513\n",
      "34842,3.3361\n",
      "34843,3.3739\n",
      "34844,3.3639\n",
      "34845,3.3867\n",
      "34846,3.3923\n",
      "34847,3.3662\n",
      "34848,3.3168\n",
      "34849,3.3183\n",
      "34850,3.3454\n",
      "34851,3.3567\n",
      "34852,3.3578\n",
      "34853,3.3296\n",
      "34854,3.3526\n",
      "34855,3.3473\n",
      "34856,3.3548\n",
      "34857,3.3238\n",
      "34858,3.3765\n",
      "34859,3.3437\n",
      "34860,3.3285\n",
      "34861,3.3681\n",
      "34862,3.3517\n",
      "34863,3.3451\n",
      "34864,3.3661\n",
      "34865,3.3592\n",
      "34866,3.3412\n",
      "34867,3.3727\n",
      "34868,3.3754\n",
      "34869,3.3717\n",
      "34870,3.3608\n",
      "34871,3.3477\n",
      "34872,3.3587\n",
      "34873,3.3390\n",
      "34874,3.3652\n",
      "34875,3.3521\n",
      "34876,3.3607\n",
      "34877,3.3578\n",
      "34878,3.3533\n",
      "34879,3.3450\n",
      "34880,3.3524\n",
      "34881,3.3763\n",
      "34882,3.3566\n",
      "34883,3.3670\n",
      "34884,3.3875\n",
      "34885,3.3633\n",
      "34886,3.3492\n",
      "34887,3.3361\n",
      "34888,3.3610\n",
      "34889,3.3569\n",
      "34890,3.3496\n",
      "34891,3.3725\n",
      "34892,3.3607\n",
      "34893,3.3469\n",
      "34894,3.3628\n",
      "34895,3.3562\n",
      "34896,3.3566\n",
      "34897,3.3713\n",
      "34898,3.3440\n",
      "34899,3.3443\n",
      "34900,3.3251\n",
      "(step: 34900, epoch: 0, block: 35737600), Train Loss: 3.2716, Val Loss: 3.2883\n",
      "Learning rate: 4.56076931e-05\n",
      "Scheduler step: 34900\n",
      "Checkpoint (step: 34900, epoch: 0, block: 35737600) Saved\n",
      "Step Documented\n",
      "19:56:18\n",
      "34901,3.3431\n",
      "34902,3.3566\n",
      "34903,3.3280\n",
      "34904,3.3598\n",
      "34905,3.3491\n",
      "34906,3.3233\n",
      "34907,3.3498\n",
      "34908,3.3881\n",
      "34909,3.3425\n",
      "34910,3.3865\n",
      "34911,3.3453\n",
      "34912,3.3671\n",
      "34913,3.3401\n",
      "34914,3.3708\n",
      "34915,3.3334\n",
      "34916,3.3459\n",
      "34917,3.3642\n",
      "34918,3.3761\n",
      "34919,3.3368\n",
      "34920,3.3556\n",
      "34921,3.3460\n",
      "34922,3.3174\n",
      "34923,3.3510\n",
      "34924,3.3708\n",
      "34925,3.3347\n",
      "34926,3.3510\n",
      "34927,3.3755\n",
      "34928,3.3758\n",
      "34929,3.3652\n",
      "34930,3.3406\n",
      "34931,3.3557\n",
      "34932,3.3302\n",
      "34933,3.3509\n",
      "34934,3.3286\n",
      "34935,3.3663\n",
      "34936,3.3510\n",
      "34937,3.3786\n",
      "34938,3.3737\n",
      "34939,3.3346\n",
      "34940,3.3501\n",
      "34941,3.3276\n",
      "34942,3.3669\n",
      "34943,3.3617\n",
      "34944,3.3333\n",
      "34945,3.3881\n",
      "34946,3.3592\n",
      "34947,3.3685\n",
      "34948,3.3343\n",
      "34949,3.3627\n",
      "34950,3.3930\n",
      "34951,3.3486\n",
      "34952,3.3161\n",
      "34953,3.3333\n",
      "34954,3.3605\n",
      "34955,3.3396\n",
      "34956,3.3516\n",
      "34957,3.3591\n",
      "34958,3.3557\n",
      "34959,3.3534\n",
      "34960,3.3399\n",
      "34961,3.3609\n",
      "34962,3.3633\n",
      "34963,3.3767\n",
      "34964,3.3743\n",
      "34965,3.3749\n",
      "34966,3.3629\n",
      "34967,3.3564\n",
      "34968,3.3655\n",
      "34969,3.3493\n",
      "34970,3.3406\n",
      "34971,3.3655\n",
      "34972,3.3608\n",
      "34973,3.3657\n",
      "34974,3.3770\n",
      "34975,3.3371\n",
      "34976,3.3694\n",
      "34977,3.3367\n",
      "34978,3.3636\n",
      "34979,3.3700\n",
      "34980,3.3417\n",
      "34981,3.3495\n",
      "34982,3.3507\n",
      "34983,3.3386\n",
      "34984,3.3585\n",
      "34985,3.3620\n",
      "34986,3.3552\n",
      "34987,3.3438\n",
      "34988,3.3765\n",
      "34989,3.3607\n",
      "34990,3.3473\n",
      "34991,3.3455\n",
      "34992,3.3801\n",
      "34993,3.3533\n",
      "34994,3.3546\n",
      "34995,3.3362\n",
      "34996,3.3396\n",
      "34997,3.3561\n",
      "34998,3.3724\n",
      "34999,3.3580\n",
      "35000,3.3647\n",
      "(step: 35000, epoch: 0, block: 35840000), Train Loss: 3.2714, Val Loss: 3.3079\n",
      "Learning rate: 4.53804595e-05\n",
      "Scheduler step: 35000\n",
      "Checkpoint (step: 35000, epoch: 0, block: 35840000) Saved\n",
      "Step Documented\n",
      "20:22:40\n",
      "35001,3.3516\n",
      "35002,3.3397\n",
      "35003,3.3492\n",
      "35004,3.3525\n",
      "35005,3.3709\n",
      "35006,3.3542\n",
      "35007,3.3326\n",
      "35008,3.3509\n",
      "35009,3.3313\n",
      "35010,3.3560\n",
      "35011,3.3669\n",
      "35012,3.3569\n",
      "35013,3.3139\n",
      "35014,3.4027\n",
      "35015,3.3578\n",
      "35016,3.3459\n",
      "35017,3.3294\n",
      "35018,3.3612\n",
      "35019,3.3574\n",
      "35020,3.3597\n",
      "35021,3.3768\n",
      "35022,3.3774\n",
      "35023,3.3482\n",
      "35024,3.3694\n",
      "35025,3.3295\n",
      "35026,3.3401\n",
      "35027,3.3297\n",
      "35028,3.3824\n",
      "35029,3.3651\n",
      "35030,3.3651\n",
      "35031,3.3546\n",
      "35032,3.3414\n",
      "35033,3.3550\n",
      "35034,3.3640\n",
      "35035,3.3736\n",
      "35036,3.3524\n",
      "35037,3.3903\n",
      "35038,3.3450\n",
      "35039,3.3530\n",
      "35040,3.3578\n",
      "35041,3.3428\n",
      "35042,3.3418\n",
      "35043,3.3568\n",
      "35044,3.3800\n",
      "35045,3.3214\n",
      "35046,3.3709\n",
      "35047,3.3710\n",
      "35048,3.3724\n",
      "35049,3.3594\n",
      "35050,3.3449\n",
      "35051,3.3323\n",
      "35052,3.3658\n",
      "35053,3.3469\n",
      "35054,3.3712\n",
      "35055,3.3725\n",
      "35056,3.3691\n",
      "35057,3.3497\n",
      "35058,3.3566\n",
      "35059,3.3367\n",
      "35060,3.3462\n",
      "35061,3.3639\n",
      "35062,3.3346\n",
      "35063,3.3774\n",
      "35064,3.3503\n",
      "35065,3.3694\n",
      "35066,3.3466\n",
      "35067,3.3448\n",
      "35068,3.3508\n",
      "35069,3.3203\n",
      "35070,3.3822\n",
      "35071,3.3430\n",
      "35072,3.3580\n",
      "35073,3.3641\n",
      "35074,3.3454\n",
      "35075,3.3594\n",
      "35076,3.3580\n",
      "35077,3.3563\n",
      "35078,3.3719\n",
      "35079,3.3435\n",
      "35080,3.3428\n",
      "35081,3.3773\n",
      "35082,3.3496\n",
      "35083,3.3543\n",
      "35084,3.3398\n",
      "35085,3.3612\n",
      "35086,3.3603\n",
      "35087,3.3453\n",
      "35088,3.3427\n",
      "35089,3.3708\n",
      "35090,3.3554\n",
      "35091,3.3277\n",
      "35092,3.3677\n",
      "35093,3.3295\n",
      "35094,3.3401\n",
      "35095,3.3542\n",
      "35096,3.3563\n",
      "35097,3.3834\n",
      "35098,3.3647\n",
      "35099,3.3712\n",
      "35100,3.3271\n",
      "(step: 35100, epoch: 0, block: 35942400), Train Loss: 3.2476, Val Loss: 3.3290\n",
      "Learning rate: 4.51534437e-05\n",
      "Scheduler step: 35100\n",
      "Checkpoint (step: 35100, epoch: 0, block: 35942400) Saved\n",
      "Step Documented\n",
      "20:49:00\n",
      "35101,3.3424\n",
      "35102,3.3609\n",
      "35103,3.3690\n",
      "35104,3.3823\n",
      "35105,3.3419\n",
      "35106,3.3370\n",
      "35107,3.3790\n",
      "35108,3.3820\n",
      "35109,3.3419\n",
      "35110,3.3375\n",
      "35111,3.3569\n",
      "35112,3.3707\n",
      "35113,3.3539\n",
      "35114,3.3639\n",
      "35115,3.3661\n",
      "35116,3.3226\n",
      "35117,3.3785\n",
      "35118,3.3405\n",
      "35119,3.3505\n",
      "35120,3.3482\n",
      "35121,3.3729\n",
      "35122,3.3544\n",
      "35123,3.3546\n",
      "35124,3.3103\n",
      "35125,3.3323\n",
      "35126,3.3265\n",
      "35127,3.3642\n",
      "35128,3.3526\n",
      "35129,3.3288\n",
      "35130,3.3693\n",
      "35131,3.3733\n",
      "35132,3.3853\n",
      "35133,3.3594\n",
      "35134,3.3358\n",
      "35135,3.3231\n",
      "35136,3.3402\n",
      "35137,3.3571\n",
      "35138,3.3547\n",
      "35139,3.3247\n",
      "35140,3.3463\n",
      "35141,3.3387\n",
      "35142,3.3652\n",
      "35143,3.3154\n",
      "35144,3.3628\n",
      "35145,3.3487\n",
      "35146,3.3344\n",
      "35147,3.3328\n",
      "35148,3.3718\n",
      "35149,3.3498\n",
      "35150,3.3308\n",
      "35151,3.3386\n",
      "35152,3.3310\n",
      "35153,3.3568\n",
      "35154,3.3696\n",
      "35155,3.3227\n",
      "35156,3.3771\n",
      "35157,3.3382\n",
      "35158,3.3523\n",
      "35159,3.3707\n",
      "35160,3.3470\n",
      "35161,3.3632\n",
      "35162,3.3455\n",
      "35163,3.3683\n",
      "35164,3.3624\n",
      "35165,3.3664\n",
      "35166,3.3421\n",
      "35167,3.3234\n",
      "35168,3.3512\n",
      "35169,3.3488\n",
      "35170,3.3686\n",
      "35171,3.3817\n",
      "35172,3.3819\n",
      "35173,3.3450\n",
      "35174,3.3824\n",
      "35175,3.3195\n",
      "35176,3.3557\n",
      "35177,3.3661\n",
      "35178,3.3384\n",
      "35179,3.3268\n",
      "35180,3.3575\n",
      "35181,3.3555\n",
      "35182,3.3487\n",
      "35183,3.3684\n",
      "35184,3.3766\n",
      "35185,3.3423\n",
      "35186,3.3219\n",
      "35187,3.3385\n",
      "35188,3.3664\n",
      "35189,3.3280\n",
      "35190,3.3713\n",
      "35191,3.3613\n",
      "35192,3.3506\n",
      "35193,3.3729\n",
      "35194,3.3408\n",
      "35195,3.3196\n",
      "35196,3.3546\n",
      "35197,3.3930\n",
      "35198,3.3476\n",
      "35199,3.3560\n",
      "35200,3.3558\n",
      "(step: 35200, epoch: 0, block: 36044800), Train Loss: 3.2588, Val Loss: 3.3236\n",
      "Learning rate: 4.49266514e-05\n",
      "Scheduler step: 35200\n",
      "Checkpoint (step: 35200, epoch: 0, block: 36044800) Saved\n",
      "Step Documented\n",
      "21:15:23\n",
      "35201,3.3549\n",
      "35202,3.3256\n",
      "35203,3.3117\n",
      "35204,3.3474\n",
      "35205,3.3461\n",
      "35206,3.3354\n",
      "35207,3.3465\n",
      "35208,3.3792\n",
      "35209,3.3553\n",
      "35210,3.3483\n",
      "35211,3.3585\n",
      "35212,3.3466\n",
      "35213,3.3432\n",
      "35214,3.3562\n",
      "35215,3.3543\n",
      "35216,3.3394\n",
      "35217,3.3720\n",
      "35218,3.3385\n",
      "35219,3.3685\n",
      "35220,3.3585\n",
      "35221,3.3484\n",
      "35222,3.3738\n",
      "35223,3.3447\n",
      "35224,3.3277\n",
      "35225,3.3507\n",
      "35226,3.3290\n",
      "35227,3.3497\n",
      "35228,3.3645\n",
      "35229,3.3679\n",
      "35230,3.3478\n",
      "35231,3.3641\n",
      "35232,3.3449\n",
      "35233,3.3684\n",
      "35234,3.3733\n",
      "35235,3.3544\n",
      "35236,3.3498\n",
      "35237,3.3575\n",
      "35238,3.3474\n",
      "35239,3.3592\n",
      "35240,3.3609\n",
      "35241,3.3593\n",
      "35242,3.3651\n",
      "35243,3.3563\n",
      "35244,3.3531\n",
      "35245,3.3551\n",
      "35246,3.3376\n",
      "35247,3.3682\n",
      "35248,3.3415\n",
      "35249,3.3499\n",
      "35250,3.3290\n",
      "35251,3.3560\n",
      "35252,3.3525\n",
      "35253,3.3492\n",
      "35254,3.3351\n",
      "35255,3.3487\n",
      "35256,3.3521\n",
      "35257,3.3503\n",
      "35258,3.3541\n",
      "35259,3.3596\n",
      "35260,3.3604\n",
      "35261,3.3588\n",
      "35262,3.3059\n",
      "35263,3.3596\n",
      "35264,3.3325\n",
      "35265,3.3447\n",
      "35266,3.3649\n",
      "35267,3.3447\n",
      "35268,3.3567\n",
      "35269,3.3512\n",
      "35270,3.3674\n",
      "35271,3.3511\n",
      "35272,3.3603\n",
      "35273,3.3511\n",
      "35274,3.3433\n",
      "35275,3.3444\n",
      "35276,3.3315\n",
      "35277,3.3516\n",
      "35278,3.3149\n",
      "35279,3.3327\n",
      "35280,3.3747\n",
      "35281,3.3577\n",
      "35282,3.3430\n",
      "35283,3.3500\n",
      "35284,3.3471\n",
      "35285,3.3475\n",
      "35286,3.3673\n",
      "35287,3.3600\n",
      "35288,3.3501\n",
      "35289,3.3351\n",
      "35290,3.3472\n",
      "35291,3.3611\n",
      "35292,3.3445\n",
      "35293,3.3521\n",
      "35294,3.3591\n",
      "35295,3.3504\n",
      "35296,3.3355\n",
      "35297,3.3321\n",
      "35298,3.3578\n",
      "35299,3.3574\n",
      "35300,3.3483\n",
      "(step: 35300, epoch: 0, block: 36147200), Train Loss: 3.2680, Val Loss: 3.2812\n",
      "Learning rate: 4.47000884e-05\n",
      "Scheduler step: 35300\n",
      "Checkpoint (step: 35300, epoch: 0, block: 36147200) Saved\n",
      "Step Documented\n",
      "21:41:45\n",
      "35301,3.3326\n",
      "35302,3.3401\n",
      "35303,3.3624\n",
      "35304,3.3737\n",
      "35305,3.3304\n",
      "35306,3.3532\n",
      "35307,3.3658\n",
      "35308,3.3287\n",
      "35309,3.3505\n",
      "35310,3.3410\n",
      "35311,3.3574\n",
      "35312,3.3518\n",
      "35313,3.3274\n",
      "35314,3.3693\n",
      "35315,3.3369\n",
      "35316,3.3589\n",
      "35317,3.3704\n",
      "35318,3.3233\n",
      "35319,3.3234\n",
      "35320,3.3357\n",
      "35321,3.3508\n",
      "35322,3.3657\n",
      "35323,3.3542\n",
      "35324,3.3396\n",
      "35325,3.3462\n",
      "35326,3.3667\n",
      "35327,3.3279\n",
      "35328,3.3301\n",
      "35329,3.3392\n",
      "35330,3.3521\n",
      "35331,3.3510\n",
      "35332,3.3586\n",
      "35333,3.3607\n",
      "35334,3.3246\n",
      "35335,3.3616\n",
      "35336,3.3591\n",
      "35337,3.3474\n",
      "35338,3.3309\n",
      "35339,3.3608\n",
      "35340,3.3578\n",
      "35341,3.3628\n",
      "35342,3.3421\n",
      "35343,3.3561\n",
      "35344,3.3456\n",
      "35345,3.3669\n",
      "35346,3.3355\n",
      "35347,3.3513\n",
      "35348,3.3311\n",
      "35349,3.3175\n",
      "35350,3.3393\n",
      "35351,3.3464\n",
      "35352,3.3556\n",
      "35353,3.3584\n",
      "35354,3.3450\n",
      "35355,3.3605\n",
      "35356,3.3645\n",
      "35357,3.3250\n",
      "35358,3.3546\n",
      "35359,3.3224\n",
      "35360,3.3446\n",
      "35361,3.3507\n",
      "35362,3.3442\n",
      "35363,3.3174\n",
      "35364,3.3451\n",
      "35365,3.3626\n",
      "35366,3.3584\n",
      "35367,3.3603\n",
      "35368,3.3557\n",
      "35369,3.3679\n",
      "35370,3.3654\n",
      "35371,3.3472\n",
      "35372,3.3414\n",
      "35373,3.3654\n",
      "35374,3.3729\n",
      "35375,3.3735\n",
      "35376,3.3483\n",
      "35377,3.3446\n",
      "35378,3.3759\n",
      "35379,3.3613\n",
      "35380,3.3435\n",
      "35381,3.3346\n",
      "35382,3.3390\n",
      "35383,3.3809\n",
      "35384,3.3360\n",
      "35385,3.3489\n",
      "35386,3.3436\n",
      "35387,3.3285\n",
      "35388,3.3768\n",
      "35389,3.3500\n",
      "35390,3.3439\n",
      "35391,3.3508\n",
      "35392,3.3629\n",
      "35393,3.3632\n",
      "35394,3.3363\n",
      "35395,3.3281\n",
      "35396,3.3369\n",
      "35397,3.3295\n",
      "35398,3.3334\n",
      "35399,3.3354\n",
      "35400,3.3240\n",
      "(step: 35400, epoch: 0, block: 36249600), Train Loss: 3.2916, Val Loss: 3.3157\n",
      "Learning rate: 4.44737603e-05\n",
      "Scheduler step: 35400\n",
      "Checkpoint (step: 35400, epoch: 0, block: 36249600) Saved\n",
      "Step Documented\n",
      "22:08:11\n",
      "35401,3.3621\n",
      "35402,3.3557\n",
      "35403,3.3770\n",
      "35404,3.3712\n",
      "35405,3.3794\n",
      "35406,3.3879\n",
      "35407,3.3622\n",
      "35408,3.3552\n",
      "35409,3.3623\n",
      "35410,3.3551\n",
      "35411,3.3488\n",
      "35412,3.3627\n",
      "35413,3.3338\n",
      "35414,3.3447\n",
      "35415,3.3425\n",
      "35416,3.3452\n",
      "35417,3.3523\n",
      "35418,3.3247\n",
      "35419,3.3512\n",
      "35420,3.3397\n",
      "35421,3.3721\n",
      "35422,3.3646\n",
      "35423,3.3434\n",
      "35424,3.3607\n",
      "35425,3.3803\n",
      "35426,3.3097\n",
      "35427,3.3687\n",
      "35428,3.3447\n",
      "35429,3.3319\n",
      "35430,3.3656\n",
      "35431,3.3051\n",
      "35432,3.3507\n",
      "35433,3.3243\n",
      "35434,3.3590\n",
      "35435,3.3515\n",
      "35436,3.3690\n",
      "35437,3.3559\n",
      "35438,3.3704\n",
      "35439,3.3812\n",
      "35440,3.3778\n",
      "35441,3.3561\n",
      "35442,3.3340\n",
      "35443,3.3748\n",
      "35444,3.3646\n",
      "35445,3.3999\n",
      "35446,3.3445\n",
      "35447,3.3525\n",
      "35448,3.3192\n",
      "35449,3.3596\n",
      "35450,3.3371\n",
      "35451,3.3423\n",
      "35452,3.3705\n",
      "35453,3.3657\n",
      "35454,3.3288\n",
      "35455,3.3475\n",
      "35456,3.3637\n",
      "35457,3.3423\n",
      "35458,3.3374\n",
      "35459,3.3483\n",
      "35460,3.3574\n",
      "35461,3.3625\n",
      "35462,3.3464\n",
      "35463,3.3474\n",
      "35464,3.3859\n",
      "35465,3.3341\n",
      "35466,3.3458\n",
      "35467,3.3548\n",
      "35468,3.3655\n",
      "35469,3.3414\n",
      "35470,3.3587\n",
      "35471,3.3397\n",
      "35472,3.3577\n",
      "35473,3.3580\n",
      "35474,3.3626\n",
      "35475,3.3396\n",
      "35476,3.3601\n",
      "35477,3.3667\n",
      "35478,3.3327\n",
      "35479,3.3570\n",
      "35480,3.3738\n",
      "35481,3.3280\n",
      "35482,3.3414\n",
      "35483,3.3854\n",
      "35484,3.3457\n",
      "35485,3.3630\n",
      "35486,3.3191\n",
      "35487,3.3451\n",
      "35488,3.3452\n",
      "35489,3.3573\n",
      "35490,3.3690\n",
      "35491,3.3190\n",
      "35492,3.3691\n",
      "35493,3.3504\n",
      "35494,3.3799\n",
      "35495,3.3543\n",
      "35496,3.3586\n",
      "35497,3.3298\n",
      "35498,3.3884\n",
      "35499,3.3313\n",
      "35500,3.3277\n",
      "(step: 35500, epoch: 0, block: 36352000), Train Loss: 3.2666, Val Loss: 3.2735\n",
      "Learning rate: 4.42476729e-05\n",
      "Scheduler step: 35500\n",
      "Checkpoint (step: 35500, epoch: 0, block: 36352000) Saved\n",
      "Step Documented\n",
      "22:34:33\n",
      "35501,3.3620\n",
      "35502,3.3841\n",
      "35503,3.3506\n",
      "35504,3.3546\n",
      "35505,3.3547\n",
      "35506,3.3702\n",
      "35507,3.3449\n",
      "35508,3.3380\n",
      "35509,3.3370\n",
      "35510,3.3659\n",
      "35511,3.3787\n",
      "35512,3.3451\n",
      "35513,3.3566\n",
      "35514,3.3588\n",
      "35515,3.3310\n",
      "35516,3.3423\n",
      "35517,3.3339\n",
      "35518,3.3616\n",
      "35519,3.3617\n",
      "35520,3.3453\n",
      "35521,3.4054\n",
      "35522,3.3340\n",
      "35523,3.3541\n",
      "35524,3.3182\n",
      "35525,3.3645\n",
      "35526,3.3461\n",
      "35527,3.3401\n",
      "35528,3.3462\n",
      "35529,3.3558\n",
      "35530,3.3517\n",
      "35531,3.3544\n",
      "35532,3.3450\n",
      "35533,3.3676\n",
      "35534,3.3614\n",
      "35535,3.3552\n",
      "35536,3.3358\n",
      "35537,3.3515\n",
      "35538,3.3741\n",
      "35539,3.3536\n",
      "35540,3.3390\n",
      "35541,3.3489\n",
      "35542,3.3397\n",
      "35543,3.3611\n",
      "35544,3.3569\n",
      "35545,3.3555\n",
      "35546,3.3625\n",
      "35547,3.3527\n",
      "35548,3.3476\n",
      "35549,3.3608\n",
      "35550,3.3363\n",
      "35551,3.3488\n",
      "35552,3.3653\n",
      "35553,3.3603\n",
      "35554,3.3236\n",
      "35555,3.3168\n",
      "35556,3.3675\n",
      "35557,3.3477\n",
      "35558,3.3480\n",
      "35559,3.3519\n",
      "35560,3.3388\n",
      "35561,3.3296\n",
      "35562,3.3671\n",
      "35563,3.3362\n",
      "35564,3.3507\n",
      "35565,3.3552\n",
      "35566,3.3629\n",
      "35567,3.3388\n",
      "35568,3.3324\n",
      "35569,3.3591\n",
      "35570,3.3392\n",
      "35571,3.3618\n",
      "35572,3.3445\n",
      "35573,3.3281\n",
      "35574,3.3377\n",
      "35575,3.3298\n",
      "35576,3.3536\n",
      "35577,3.3600\n",
      "35578,3.3388\n",
      "35579,3.3696\n",
      "35580,3.3493\n",
      "35581,3.3484\n",
      "35582,3.3748\n",
      "35583,3.3359\n",
      "35584,3.3376\n",
      "35585,3.3625\n",
      "35586,3.3529\n",
      "35587,3.3573\n",
      "35588,3.3227\n",
      "35589,3.3483\n",
      "35590,3.3550\n",
      "35591,3.3382\n",
      "35592,3.3600\n",
      "35593,3.3536\n",
      "35594,3.3496\n",
      "35595,3.3183\n",
      "35596,3.3325\n",
      "35597,3.3394\n",
      "35598,3.3482\n",
      "35599,3.3358\n",
      "35600,3.3593\n",
      "(step: 35600, epoch: 0, block: 36454400), Train Loss: 3.2724, Val Loss: 3.2513\n",
      "Learning rate: 4.40218320e-05\n",
      "Scheduler step: 35600\n",
      "Checkpoint (step: 35600, epoch: 0, block: 36454400) Saved\n",
      "Step Documented\n",
      "23:00:54\n",
      "35601,3.3412\n",
      "35602,3.3434\n",
      "35603,3.3406\n",
      "35604,3.3385\n",
      "35605,3.3492\n",
      "35606,3.3547\n",
      "35607,3.3288\n",
      "35608,3.3278\n",
      "35609,3.3638\n",
      "35610,3.3226\n",
      "35611,3.3803\n",
      "35612,3.3388\n",
      "35613,3.3548\n",
      "35614,3.3628\n",
      "35615,3.3355\n",
      "35616,3.3259\n",
      "35617,3.3564\n",
      "35618,3.3497\n",
      "35619,3.3339\n",
      "35620,3.3454\n",
      "35621,3.3665\n",
      "35622,3.3519\n",
      "35623,3.3279\n",
      "35624,3.3577\n",
      "35625,3.3623\n",
      "35626,3.3732\n",
      "35627,3.3625\n",
      "35628,3.3288\n",
      "35629,3.3459\n",
      "35630,3.3349\n",
      "35631,3.3780\n",
      "35632,3.3656\n",
      "35633,3.3585\n",
      "35634,3.3843\n",
      "35635,3.3406\n",
      "35636,3.3754\n",
      "35637,3.3268\n",
      "35638,3.3426\n",
      "35639,3.3606\n",
      "35640,3.3299\n",
      "35641,3.3764\n",
      "35642,3.3496\n",
      "35643,3.3496\n",
      "35644,3.3774\n",
      "35645,3.3388\n",
      "35646,3.3462\n",
      "35647,3.3638\n",
      "35648,3.3556\n",
      "35649,3.3315\n",
      "35650,3.3371\n",
      "35651,3.3260\n",
      "35652,3.3513\n",
      "35653,3.3872\n",
      "35654,3.3676\n",
      "35655,3.3613\n",
      "35656,3.3745\n",
      "35657,3.3262\n",
      "35658,3.3615\n",
      "35659,3.3765\n",
      "35660,3.3286\n",
      "35661,3.3653\n",
      "35662,3.3437\n",
      "35663,3.3220\n",
      "35664,3.3485\n",
      "35665,3.3499\n",
      "35666,3.3352\n",
      "35667,3.3596\n",
      "35668,3.3469\n",
      "35669,3.3242\n",
      "35670,3.3425\n",
      "35671,3.3407\n",
      "35672,3.3417\n",
      "35673,3.3511\n",
      "35674,3.3366\n",
      "35675,3.3690\n",
      "35676,3.3549\n",
      "35677,3.3705\n",
      "35678,3.3512\n",
      "35679,3.3147\n",
      "35680,3.3642\n",
      "35681,3.3290\n",
      "35682,3.3477\n",
      "35683,3.3615\n",
      "35684,3.3628\n",
      "35685,3.3462\n",
      "35686,3.3587\n",
      "35687,3.3669\n",
      "35688,3.3590\n",
      "35689,3.3660\n",
      "35690,3.3483\n",
      "35691,3.3357\n",
      "35692,3.3366\n",
      "35693,3.3637\n",
      "35694,3.3339\n",
      "35695,3.3458\n",
      "35696,3.3075\n",
      "35697,3.3559\n",
      "35698,3.3619\n",
      "35699,3.3369\n",
      "35700,3.3505\n",
      "(step: 35700, epoch: 0, block: 36556800), Train Loss: 3.2827, Val Loss: 3.2747\n",
      "Learning rate: 4.37962431e-05\n",
      "Scheduler step: 35700\n",
      "Checkpoint (step: 35700, epoch: 0, block: 36556800) Saved\n",
      "Step Documented\n",
      "23:27:15\n",
      "35701,3.3754\n",
      "35702,3.3557\n",
      "35703,3.3553\n",
      "35704,3.3197\n",
      "35705,3.3377\n",
      "35706,3.3454\n",
      "35707,3.3575\n",
      "35708,3.3637\n",
      "35709,3.3618\n",
      "35710,3.3631\n",
      "35711,3.3345\n",
      "35712,3.3307\n",
      "35713,3.3361\n",
      "35714,3.3275\n",
      "35715,3.3652\n",
      "35716,3.3788\n",
      "35717,3.3559\n",
      "35718,3.3759\n",
      "35719,3.3549\n",
      "35720,3.3420\n",
      "35721,3.3475\n",
      "35722,3.3442\n",
      "35723,3.3626\n",
      "35724,3.3475\n",
      "35725,3.3564\n",
      "35726,3.3429\n",
      "35727,3.3561\n",
      "35728,3.3586\n",
      "35729,3.3549\n",
      "35730,3.3385\n",
      "35731,3.3442\n",
      "35732,3.3600\n",
      "35733,3.3367\n",
      "35734,3.3499\n",
      "35735,3.3425\n",
      "35736,3.3338\n",
      "35737,3.3436\n",
      "35738,3.3488\n",
      "35739,3.3338\n",
      "35740,3.3415\n",
      "35741,3.3633\n",
      "35742,3.3169\n",
      "35743,3.3761\n",
      "35744,3.3621\n",
      "35745,3.3396\n",
      "35746,3.3467\n",
      "35747,3.3751\n",
      "35748,3.3577\n",
      "35749,3.3485\n",
      "35750,3.3543\n",
      "35751,3.3409\n",
      "35752,3.3035\n",
      "35753,3.3508\n",
      "35754,3.3775\n",
      "35755,3.3472\n",
      "35756,3.3628\n",
      "35757,3.3579\n",
      "35758,3.3279\n",
      "35759,3.3456\n",
      "35760,3.3640\n",
      "35761,3.3217\n",
      "35762,3.3585\n",
      "35763,3.3444\n",
      "35764,3.3625\n",
      "35765,3.3406\n",
      "35766,3.3365\n",
      "35767,3.3444\n",
      "35768,3.3694\n",
      "35769,3.3296\n",
      "35770,3.3535\n",
      "35771,3.3732\n",
      "35772,3.3223\n",
      "35773,3.3645\n",
      "35774,3.3493\n",
      "35775,3.3511\n",
      "35776,3.3621\n",
      "35777,3.3613\n",
      "35778,3.3621\n",
      "35779,3.3719\n",
      "35780,3.3184\n",
      "35781,3.3486\n",
      "35782,3.3722\n",
      "35783,3.3263\n",
      "35784,3.3558\n",
      "35785,3.3691\n",
      "35786,3.3644\n",
      "35787,3.3721\n",
      "35788,3.3340\n",
      "35789,3.3495\n",
      "35790,3.3581\n",
      "35791,3.3435\n",
      "35792,3.3664\n",
      "35793,3.3616\n",
      "35794,3.3432\n",
      "35795,3.3608\n",
      "35796,3.3658\n",
      "35797,3.3109\n",
      "35798,3.3873\n",
      "35799,3.3307\n",
      "35800,3.3521\n",
      "(step: 35800, epoch: 0, block: 36659200), Train Loss: 3.2695, Val Loss: 3.2788\n",
      "Learning rate: 4.35709121e-05\n",
      "Scheduler step: 35800\n",
      "Checkpoint (step: 35800, epoch: 0, block: 36659200) Saved\n",
      "Step Documented\n",
      "23:53:36\n",
      "35801,3.3349\n",
      "35802,3.3293\n",
      "35803,3.3323\n",
      "35804,3.3741\n",
      "35805,3.3563\n",
      "35806,3.3648\n",
      "35807,3.3462\n",
      "35808,3.3589\n",
      "35809,3.3547\n",
      "35810,3.3665\n",
      "35811,3.3318\n",
      "35812,3.3661\n",
      "35813,3.3361\n",
      "35814,3.3273\n",
      "35815,3.3604\n",
      "35816,3.3562\n",
      "35817,3.3435\n",
      "35818,3.3472\n",
      "35819,3.3620\n",
      "35820,3.3208\n",
      "35821,3.3596\n",
      "35822,3.3493\n",
      "35823,3.3372\n",
      "35824,3.3083\n",
      "35825,3.3432\n",
      "35826,3.3445\n",
      "35827,3.3520\n",
      "35828,3.3570\n",
      "35829,3.3214\n",
      "35830,3.3428\n",
      "35831,3.3540\n",
      "35832,3.3471\n",
      "35833,3.3652\n",
      "35834,3.3271\n",
      "35835,3.3186\n",
      "35836,3.3650\n",
      "35837,3.3359\n",
      "35838,3.3539\n",
      "35839,3.3567\n",
      "35840,3.3428\n",
      "35841,3.3509\n",
      "35842,3.3538\n",
      "35843,3.3613\n",
      "35844,3.3631\n",
      "35845,3.3678\n",
      "35846,3.3173\n",
      "35847,3.3431\n",
      "35848,3.3513\n",
      "35849,3.3553\n",
      "35850,3.3255\n",
      "35851,3.3829\n",
      "35852,3.3296\n",
      "35853,3.3580\n",
      "35854,3.3610\n",
      "35855,3.3487\n",
      "35856,3.3406\n",
      "35857,3.3293\n",
      "35858,3.3681\n",
      "35859,3.3450\n",
      "35860,3.3444\n",
      "35861,3.3500\n",
      "35862,3.3540\n",
      "35863,3.3762\n",
      "35864,3.3814\n",
      "35865,3.3581\n",
      "35866,3.3631\n",
      "35867,3.3641\n",
      "35868,3.3586\n",
      "35869,3.3587\n",
      "35870,3.3378\n",
      "35871,3.3663\n",
      "35872,3.3247\n",
      "35873,3.3643\n",
      "35874,3.3680\n",
      "35875,3.3425\n",
      "35876,3.3684\n",
      "35877,3.3550\n",
      "35878,3.3484\n",
      "35879,3.3869\n",
      "35880,3.3300\n",
      "35881,3.3373\n",
      "35882,3.3593\n",
      "35883,3.3488\n",
      "35884,3.3621\n",
      "35885,3.3473\n",
      "35886,3.3412\n",
      "35887,3.3256\n",
      "35888,3.3577\n",
      "35889,3.3452\n",
      "35890,3.3753\n",
      "35891,3.3380\n",
      "35892,3.3458\n",
      "35893,3.3160\n",
      "35894,3.3618\n",
      "35895,3.3624\n",
      "35896,3.3679\n",
      "35897,3.3424\n",
      "35898,3.3635\n",
      "35899,3.3642\n",
      "35900,3.3315\n",
      "(step: 35900, epoch: 0, block: 36761600), Train Loss: 3.2942, Val Loss: 3.2956\n",
      "Learning rate: 4.33458445e-05\n",
      "Scheduler step: 35900\n",
      "Checkpoint (step: 35900, epoch: 0, block: 36761600) Saved\n",
      "Step Documented\n",
      "00:19:55\n",
      "35901,3.3751\n",
      "35902,3.3453\n",
      "35903,3.3269\n",
      "35904,3.3442\n",
      "35905,3.3557\n",
      "35906,3.3744\n",
      "35907,3.3738\n",
      "35908,3.3530\n",
      "35909,3.3565\n",
      "35910,3.3602\n",
      "35911,3.3455\n",
      "35912,3.3490\n",
      "35913,3.3614\n",
      "35914,3.3353\n",
      "35915,3.3345\n",
      "35916,3.3527\n",
      "35917,3.3334\n",
      "35918,3.3449\n",
      "35919,3.3618\n",
      "35920,3.3529\n",
      "35921,3.3388\n",
      "35922,3.3833\n",
      "35923,3.3739\n",
      "35924,3.3303\n",
      "35925,3.3438\n",
      "35926,3.3718\n",
      "35927,3.3721\n",
      "35928,3.3635\n",
      "35929,3.3681\n",
      "35930,3.3206\n",
      "35931,3.3396\n",
      "35932,3.3378\n",
      "35933,3.3482\n",
      "35934,3.3570\n",
      "35935,3.3257\n",
      "35936,3.3908\n",
      "35937,3.3586\n",
      "35938,3.3468\n",
      "35939,3.3486\n",
      "35940,3.3465\n",
      "35941,3.3932\n",
      "35942,3.3492\n",
      "35943,3.3199\n",
      "35944,3.3317\n",
      "35945,3.3715\n",
      "35946,3.3691\n",
      "35947,3.3546\n",
      "35948,3.3352\n",
      "35949,3.3547\n",
      "35950,3.3426\n",
      "35951,3.3668\n",
      "35952,3.3748\n",
      "35953,3.3638\n",
      "35954,3.3355\n",
      "35955,3.3590\n",
      "35956,3.3830\n",
      "35957,3.3387\n",
      "35958,3.3725\n",
      "35959,3.3383\n",
      "35960,3.3500\n",
      "35961,3.3695\n",
      "35962,3.3267\n",
      "35963,3.3194\n",
      "35964,3.3728\n",
      "35965,3.3613\n",
      "35966,3.3553\n",
      "35967,3.3637\n",
      "35968,3.3488\n",
      "35969,3.3654\n",
      "35970,3.3503\n",
      "35971,3.3606\n",
      "35972,3.3507\n",
      "35973,3.3345\n",
      "35974,3.3467\n",
      "35975,3.3651\n",
      "35976,3.3373\n",
      "35977,3.3592\n",
      "35978,3.3570\n",
      "35979,3.3750\n",
      "35980,3.3600\n",
      "35981,3.3400\n",
      "35982,3.3456\n",
      "35983,3.3569\n",
      "35984,3.3794\n",
      "35985,3.3539\n",
      "35986,3.3373\n",
      "35987,3.3325\n",
      "35988,3.3600\n",
      "35989,3.3775\n",
      "35990,3.3304\n",
      "35991,3.3640\n",
      "35992,3.3490\n",
      "35993,3.3435\n",
      "35994,3.3570\n",
      "35995,3.3475\n",
      "35996,3.3328\n",
      "35997,3.3372\n",
      "35998,3.3459\n",
      "35999,3.3565\n",
      "36000,3.3846\n",
      "(step: 36000, epoch: 0, block: 36864000), Train Loss: 3.3127, Val Loss: 3.3002\n",
      "Learning rate: 4.31210461e-05\n",
      "Scheduler step: 36000\n",
      "Checkpoint (step: 36000, epoch: 0, block: 36864000) Saved\n",
      "Step Documented\n",
      "00:46:14\n",
      "36001,3.3416\n",
      "36002,3.3519\n",
      "36003,3.3792\n",
      "36004,3.3684\n",
      "36005,3.3452\n",
      "36006,3.3530\n",
      "36007,3.3538\n",
      "36008,3.3567\n",
      "36009,3.3826\n",
      "36010,3.3017\n",
      "36011,3.3292\n",
      "36012,3.3432\n",
      "36013,3.3549\n",
      "36014,3.3508\n",
      "36015,3.3363\n",
      "36016,3.3422\n",
      "36017,3.3403\n",
      "36018,3.3424\n",
      "36019,3.3710\n",
      "36020,3.3257\n",
      "36021,3.3593\n",
      "36022,3.3543\n",
      "36023,3.3949\n",
      "36024,3.3550\n",
      "36025,3.3692\n",
      "36026,3.3583\n",
      "36027,3.3565\n",
      "36028,3.3553\n",
      "36029,3.3199\n",
      "36030,3.3588\n",
      "36031,3.3739\n",
      "36032,3.3516\n",
      "36033,3.3452\n",
      "36034,3.3422\n",
      "36035,3.3430\n",
      "36036,3.3472\n",
      "36037,3.3506\n",
      "36038,3.3601\n",
      "36039,3.3517\n",
      "36040,3.3357\n",
      "36041,3.3547\n",
      "36042,3.3173\n",
      "36043,3.3505\n",
      "36044,3.3557\n",
      "36045,3.3680\n",
      "36046,3.3313\n",
      "36047,3.3694\n",
      "36048,3.3569\n",
      "36049,3.3487\n",
      "36050,3.3301\n",
      "36051,3.3316\n",
      "36052,3.3314\n",
      "36053,3.3663\n",
      "36054,3.3151\n",
      "36055,3.3683\n",
      "36056,3.3691\n",
      "36057,3.3444\n",
      "36058,3.3786\n",
      "36059,3.3650\n",
      "36060,3.3570\n",
      "36061,3.3402\n",
      "36062,3.3731\n",
      "36063,3.3405\n",
      "36064,3.3535\n",
      "36065,3.3480\n",
      "36066,3.3886\n",
      "36067,3.3582\n",
      "36068,3.3646\n",
      "36069,3.3306\n",
      "36070,3.3554\n",
      "36071,3.3206\n",
      "36072,3.3706\n",
      "36073,3.3554\n",
      "36074,3.3240\n",
      "36075,3.3595\n",
      "36076,3.3577\n",
      "36077,3.3345\n",
      "36078,3.3586\n",
      "36079,3.3504\n",
      "36080,3.3506\n",
      "36081,3.3854\n",
      "36082,3.3306\n",
      "36083,3.3634\n",
      "36084,3.3712\n",
      "36085,3.3940\n",
      "36086,3.3595\n",
      "36087,3.3494\n",
      "36088,3.3192\n",
      "36089,3.3423\n",
      "36090,3.3372\n",
      "36091,3.3534\n",
      "36092,3.3388\n",
      "36093,3.3383\n",
      "36094,3.3703\n",
      "36095,3.3435\n",
      "36096,3.3431\n",
      "36097,3.3420\n",
      "36098,3.3513\n",
      "36099,3.3570\n",
      "36100,3.3492\n",
      "(step: 36100, epoch: 0, block: 36966400), Train Loss: 3.2679, Val Loss: 3.2760\n",
      "Learning rate: 4.28965226e-05\n",
      "Scheduler step: 36100\n",
      "Checkpoint (step: 36100, epoch: 0, block: 36966400) Saved\n",
      "Step Documented\n",
      "01:12:35\n",
      "36101,3.3511\n",
      "36102,3.3521\n",
      "36103,3.3475\n",
      "36104,3.3341\n",
      "36105,3.3587\n",
      "36106,3.3611\n",
      "36107,3.3456\n",
      "36108,3.3565\n",
      "36109,3.3221\n",
      "36110,3.3531\n",
      "36111,3.3592\n",
      "36112,3.3440\n",
      "36113,3.3566\n",
      "36114,3.3399\n",
      "36115,3.3599\n",
      "36116,3.3291\n",
      "36117,3.3172\n",
      "36118,3.3088\n",
      "36119,3.3529\n",
      "36120,3.3618\n",
      "36121,3.3452\n",
      "36122,3.3525\n",
      "36123,3.3460\n",
      "36124,3.3486\n",
      "36125,3.3461\n",
      "36126,3.3807\n",
      "36127,3.3465\n",
      "36128,3.3786\n",
      "36129,3.3456\n",
      "36130,3.3548\n",
      "36131,3.3636\n",
      "36132,3.3649\n",
      "36133,3.3373\n",
      "36134,3.3673\n",
      "36135,3.3591\n",
      "36136,3.3474\n",
      "36137,3.3385\n",
      "36138,3.3045\n",
      "36139,3.3291\n",
      "36140,3.3467\n",
      "36141,3.3437\n",
      "36142,3.3514\n",
      "36143,3.3535\n",
      "36144,3.3469\n",
      "36145,3.3582\n",
      "36146,3.3844\n",
      "36147,3.3512\n",
      "36148,3.3521\n",
      "36149,3.3562\n",
      "36150,3.3474\n",
      "36151,3.3557\n",
      "36152,3.3525\n",
      "36153,3.3238\n",
      "36154,3.3589\n",
      "36155,3.3542\n",
      "36156,3.3511\n",
      "36157,3.3336\n",
      "36158,3.3453\n",
      "36159,3.3502\n",
      "36160,3.3334\n",
      "36161,3.3350\n",
      "36162,3.3864\n",
      "36163,3.3371\n",
      "36164,3.3300\n",
      "36165,3.3404\n",
      "36166,3.3533\n",
      "36167,3.3420\n",
      "36168,3.3265\n",
      "36169,3.3391\n",
      "36170,3.3647\n",
      "36171,3.3694\n",
      "36172,3.3583\n",
      "36173,3.3415\n",
      "36174,3.3201\n",
      "36175,3.3398\n",
      "36176,3.3663\n",
      "36177,3.3619\n",
      "36178,3.3380\n",
      "36179,3.3547\n",
      "36180,3.3355\n",
      "36181,3.3443\n",
      "36182,3.3393\n",
      "36183,3.3196\n",
      "36184,3.3505\n",
      "36185,3.3605\n",
      "36186,3.3577\n",
      "36187,3.3652\n",
      "36188,3.3484\n",
      "36189,3.3510\n",
      "36190,3.3507\n",
      "36191,3.3266\n",
      "36192,3.3634\n",
      "36193,3.3460\n",
      "36194,3.3564\n",
      "36195,3.3551\n",
      "36196,3.3853\n",
      "36197,3.3527\n",
      "36198,3.3861\n",
      "36199,3.3652\n",
      "36200,3.3546\n",
      "(step: 36200, epoch: 0, block: 37068800), Train Loss: 3.2837, Val Loss: 3.3058\n",
      "Learning rate: 4.26722797e-05\n",
      "Scheduler step: 36200\n",
      "Checkpoint (step: 36200, epoch: 0, block: 37068800) Saved\n",
      "Step Documented\n",
      "01:38:55\n",
      "36201,3.3288\n",
      "36202,3.3559\n",
      "36203,3.3064\n",
      "36204,3.3593\n",
      "36205,3.3517\n",
      "36206,3.3459\n",
      "36207,3.3308\n",
      "36208,3.3663\n",
      "36209,3.3493\n",
      "36210,3.3502\n",
      "36211,3.3352\n",
      "36212,3.3405\n",
      "36213,3.3464\n",
      "36214,3.3500\n",
      "36215,3.3823\n",
      "36216,3.3241\n",
      "36217,3.3728\n",
      "36218,3.3543\n",
      "36219,3.3511\n",
      "36220,3.3532\n",
      "36221,3.3484\n",
      "36222,3.3565\n",
      "36223,3.2968\n",
      "36224,3.3444\n",
      "36225,3.3398\n",
      "36226,3.3619\n",
      "36227,3.3686\n",
      "36228,3.3575\n",
      "36229,3.3457\n",
      "36230,3.3257\n",
      "36231,3.3478\n",
      "36232,3.3626\n",
      "36233,3.3554\n",
      "36234,3.3339\n",
      "36235,3.3541\n",
      "36236,3.3514\n",
      "36237,3.3383\n",
      "36238,3.3445\n",
      "36239,3.3752\n",
      "36240,3.3241\n",
      "36241,3.3808\n",
      "36242,3.3674\n",
      "36243,3.3429\n",
      "36244,3.3527\n",
      "36245,3.3339\n",
      "36246,3.3473\n",
      "36247,3.3686\n",
      "36248,3.3380\n",
      "36249,3.3442\n",
      "36250,3.3534\n",
      "36251,3.3458\n",
      "36252,3.3790\n",
      "36253,3.3293\n",
      "36254,3.3719\n",
      "36255,3.3473\n",
      "36256,3.3513\n",
      "36257,3.3603\n",
      "36258,3.3208\n",
      "36259,3.3480\n",
      "36260,3.3625\n",
      "36261,3.3723\n",
      "36262,3.3329\n",
      "36263,3.3659\n",
      "36264,3.3333\n",
      "36265,3.3491\n",
      "36266,3.3069\n",
      "36267,3.3883\n",
      "36268,3.3320\n",
      "36269,3.3525\n",
      "36270,3.3591\n",
      "36271,3.3450\n",
      "36272,3.3160\n",
      "36273,3.3590\n",
      "36274,3.3628\n",
      "36275,3.3832\n",
      "36276,3.3572\n",
      "36277,3.3352\n",
      "36278,3.3385\n",
      "36279,3.3414\n",
      "36280,3.3583\n",
      "36281,3.3715\n",
      "36282,3.3603\n",
      "36283,3.3589\n",
      "36284,3.3811\n",
      "36285,3.3323\n",
      "36286,3.3267\n",
      "36287,3.3636\n",
      "36288,3.3410\n",
      "36289,3.3484\n",
      "36290,3.3580\n",
      "36291,3.3156\n",
      "36292,3.3592\n",
      "36293,3.3361\n",
      "36294,3.3765\n",
      "36295,3.3226\n",
      "36296,3.3584\n",
      "36297,3.3711\n",
      "36298,3.3552\n",
      "36299,3.3559\n",
      "36300,3.3546\n",
      "(step: 36300, epoch: 0, block: 37171200), Train Loss: 3.2711, Val Loss: 3.2786\n",
      "Learning rate: 4.24483229e-05\n",
      "Scheduler step: 36300\n",
      "Checkpoint (step: 36300, epoch: 0, block: 37171200) Saved\n",
      "Step Documented\n",
      "02:05:15\n",
      "36301,3.3608\n",
      "36302,3.3419\n",
      "36303,3.3443\n",
      "36304,3.3541\n",
      "36305,3.3524\n",
      "36306,3.3471\n",
      "36307,3.3676\n",
      "36308,3.3291\n",
      "36309,3.3460\n",
      "36310,3.3216\n",
      "36311,3.3469\n",
      "36312,3.3460\n",
      "36313,3.3381\n",
      "36314,3.3493\n",
      "36315,3.3420\n",
      "36316,3.3574\n",
      "36317,3.3568\n",
      "36318,3.3589\n",
      "36319,3.3211\n",
      "36320,3.3665\n",
      "36321,3.3631\n",
      "36322,3.3539\n",
      "36323,3.3355\n",
      "36324,3.3439\n",
      "36325,3.3427\n",
      "36326,3.3521\n",
      "36327,3.3465\n",
      "36328,3.3512\n",
      "36329,3.3621\n",
      "36330,3.3455\n",
      "36331,3.3700\n",
      "36332,3.3402\n",
      "36333,3.3544\n",
      "36334,3.3531\n",
      "36335,3.3705\n",
      "36336,3.3589\n",
      "36337,3.3611\n",
      "36338,3.3379\n",
      "36339,3.3711\n",
      "36340,3.3530\n",
      "36341,3.3399\n",
      "36342,3.3666\n",
      "36343,3.3636\n",
      "36344,3.3393\n",
      "36345,3.3352\n",
      "36346,3.3473\n",
      "36347,3.3591\n",
      "36348,3.3431\n",
      "36349,3.3374\n",
      "36350,3.3613\n",
      "36351,3.3158\n",
      "36352,3.3239\n",
      "36353,3.3466\n",
      "36354,3.3881\n",
      "36355,3.3297\n",
      "36356,3.3260\n",
      "36357,3.3240\n",
      "36358,3.3615\n",
      "36359,3.3454\n",
      "36360,3.3553\n",
      "36361,3.3311\n",
      "36362,3.3669\n",
      "36363,3.3205\n",
      "36364,3.3573\n",
      "36365,3.3854\n",
      "36366,3.3435\n",
      "36367,3.3507\n",
      "36368,3.3439\n",
      "36369,3.3566\n",
      "36370,3.3480\n",
      "36371,3.3403\n",
      "36372,3.3475\n",
      "36373,3.3513\n",
      "36374,3.3540\n",
      "36375,3.3545\n",
      "36376,3.3505\n",
      "36377,3.3697\n",
      "36378,3.3125\n",
      "36379,3.3706\n",
      "36380,3.3734\n",
      "36381,3.3686\n",
      "36382,3.3556\n",
      "36383,3.3505\n",
      "36384,3.3342\n",
      "36385,3.3677\n",
      "36386,3.3239\n",
      "36387,3.3443\n",
      "36388,3.3225\n",
      "36389,3.3358\n",
      "36390,3.3372\n",
      "36391,3.3434\n",
      "36392,3.3372\n",
      "36393,3.3442\n",
      "36394,3.3340\n",
      "36395,3.3565\n",
      "36396,3.3583\n",
      "36397,3.3527\n",
      "36398,3.3870\n",
      "36399,3.3611\n",
      "36400,3.3515\n",
      "(step: 36400, epoch: 0, block: 37273600), Train Loss: 3.2727, Val Loss: 3.2809\n",
      "Learning rate: 4.22246581e-05\n",
      "Scheduler step: 36400\n",
      "Checkpoint (step: 36400, epoch: 0, block: 37273600) Saved\n",
      "Step Documented\n",
      "02:31:35\n",
      "36401,3.3461\n",
      "36402,3.3412\n",
      "36403,3.3417\n",
      "36404,3.3367\n",
      "36405,3.3211\n",
      "36406,3.3498\n",
      "36407,3.3538\n",
      "36408,3.3474\n",
      "36409,3.3640\n",
      "36410,3.3632\n",
      "36411,3.3589\n",
      "36412,3.3357\n",
      "36413,3.3585\n",
      "36414,3.3506\n",
      "36415,3.3410\n",
      "36416,3.3683\n",
      "36417,3.3596\n",
      "36418,3.3688\n",
      "36419,3.3796\n",
      "36420,3.3577\n",
      "36421,3.3326\n",
      "36422,3.3571\n",
      "36423,3.3552\n",
      "36424,3.3436\n",
      "36425,3.3410\n",
      "36426,3.3424\n",
      "36427,3.3449\n",
      "36428,3.3447\n",
      "36429,3.3565\n",
      "36430,3.3742\n",
      "36431,3.3450\n",
      "36432,3.3414\n",
      "36433,3.3431\n",
      "36434,3.3688\n",
      "36435,3.3494\n",
      "36436,3.3627\n",
      "36437,3.3429\n",
      "36438,3.3470\n",
      "36439,3.3705\n",
      "36440,3.3380\n",
      "36441,3.3446\n",
      "36442,3.3558\n",
      "36443,3.3704\n",
      "36444,3.3440\n",
      "36445,3.3308\n",
      "36446,3.3455\n",
      "36447,3.3305\n",
      "36448,3.3537\n",
      "36449,3.3665\n",
      "36450,3.3560\n",
      "36451,3.3409\n",
      "36452,3.3622\n",
      "36453,3.3418\n",
      "36454,3.3669\n",
      "36455,3.3319\n",
      "36456,3.3486\n",
      "36457,3.3801\n",
      "36458,3.3306\n",
      "36459,3.3400\n",
      "36460,3.3410\n",
      "36461,3.3384\n",
      "36462,3.3673\n",
      "36463,3.3655\n",
      "36464,3.3506\n",
      "36465,3.3727\n",
      "36466,3.3503\n",
      "36467,3.3437\n",
      "36468,3.3470\n",
      "36469,3.3293\n",
      "36470,3.3657\n",
      "36471,3.3240\n",
      "36472,3.3651\n",
      "36473,3.3709\n",
      "36474,3.3464\n",
      "36475,3.3107\n",
      "36476,3.3462\n",
      "36477,3.3393\n",
      "36478,3.3608\n",
      "36479,3.3158\n",
      "36480,3.3526\n",
      "36481,3.3518\n",
      "36482,3.3620\n",
      "36483,3.3633\n",
      "36484,3.3768\n",
      "36485,3.3416\n",
      "36486,3.3469\n",
      "36487,3.3530\n",
      "36488,3.3643\n",
      "36489,3.3255\n",
      "36490,3.3622\n",
      "36491,3.3214\n",
      "36492,3.3583\n",
      "36493,3.3487\n",
      "36494,3.3429\n",
      "36495,3.3455\n",
      "36496,3.3314\n",
      "36497,3.3681\n",
      "36498,3.3446\n",
      "36499,3.3518\n",
      "36500,3.3305\n",
      "(step: 36500, epoch: 0, block: 37376000), Train Loss: 3.2552, Val Loss: 3.3200\n",
      "Learning rate: 4.20012907e-05\n",
      "Scheduler step: 36500\n",
      "Checkpoint (step: 36500, epoch: 0, block: 37376000) Saved\n",
      "Step Documented\n",
      "02:57:56\n",
      "36501,3.3617\n",
      "36502,3.3645\n",
      "36503,3.3411\n",
      "36504,3.3582\n",
      "36505,3.3454\n",
      "36506,3.3668\n",
      "36507,3.3500\n",
      "36508,3.3590\n",
      "36509,3.3318\n",
      "36510,3.3254\n",
      "36511,3.3475\n",
      "36512,3.3712\n",
      "36513,3.3518\n",
      "36514,3.3596\n",
      "36515,3.3436\n",
      "36516,3.3377\n",
      "36517,3.3599\n",
      "36518,3.3439\n",
      "36519,3.3578\n",
      "36520,3.3502\n",
      "36521,3.3708\n",
      "36522,3.3645\n",
      "36523,3.3488\n",
      "36524,3.3644\n",
      "36525,3.3146\n",
      "36526,3.3608\n",
      "36527,3.3442\n",
      "36528,3.3653\n",
      "36529,3.3763\n",
      "36530,3.3484\n",
      "36531,3.3271\n",
      "36532,3.3457\n",
      "36533,3.3415\n",
      "36534,3.3260\n",
      "36535,3.3611\n",
      "36536,3.3101\n",
      "36537,3.3268\n",
      "36538,3.3523\n",
      "36539,3.3908\n",
      "36540,3.3439\n",
      "36541,3.3486\n",
      "36542,3.3515\n",
      "36543,3.3804\n",
      "36544,3.3569\n",
      "36545,3.3534\n",
      "36546,3.3437\n",
      "36547,3.3393\n",
      "36548,3.3432\n",
      "36549,3.3382\n",
      "36550,3.3705\n",
      "36551,3.3305\n",
      "36552,3.3085\n",
      "36553,3.3533\n",
      "36554,3.3236\n",
      "36555,3.3410\n",
      "36556,3.3380\n",
      "36557,3.3211\n",
      "36558,3.3398\n",
      "36559,3.4017\n",
      "36560,3.3249\n",
      "36561,3.3495\n",
      "36562,3.3802\n",
      "36563,3.3475\n",
      "36564,3.3681\n",
      "36565,3.3423\n",
      "36566,3.3526\n",
      "36567,3.3650\n",
      "36568,3.3393\n",
      "36569,3.3434\n",
      "36570,3.3483\n",
      "36571,3.3672\n",
      "36572,3.3326\n",
      "36573,3.3566\n",
      "36574,3.3676\n",
      "36575,3.3425\n",
      "36576,3.3210\n",
      "36577,3.3457\n",
      "36578,3.3303\n",
      "36579,3.3565\n",
      "36580,3.3411\n",
      "36581,3.3337\n",
      "36582,3.3347\n",
      "36583,3.3277\n",
      "36584,3.3389\n",
      "36585,3.3566\n",
      "36586,3.3550\n",
      "36587,3.3267\n",
      "36588,3.3324\n",
      "36589,3.3364\n",
      "36590,3.3213\n",
      "36591,3.3561\n",
      "36592,3.3228\n",
      "36593,3.3606\n",
      "36594,3.3668\n",
      "36595,3.3439\n",
      "36596,3.3564\n",
      "36597,3.3431\n",
      "36598,3.3631\n",
      "36599,3.3661\n",
      "36600,3.3491\n",
      "(step: 36600, epoch: 0, block: 37478400), Train Loss: 3.2468, Val Loss: 3.2842\n",
      "Learning rate: 4.17782265e-05\n",
      "Scheduler step: 36600\n",
      "Checkpoint (step: 36600, epoch: 0, block: 37478400) Saved\n",
      "Step Documented\n",
      "03:24:16\n",
      "36601,3.3475\n",
      "36602,3.3246\n",
      "36603,3.3327\n",
      "36604,3.3440\n",
      "36605,3.3497\n",
      "36606,3.3311\n",
      "36607,3.3551\n",
      "36608,3.3318\n",
      "36609,3.3355\n",
      "36610,3.3376\n",
      "36611,3.3525\n",
      "36612,3.3328\n",
      "36613,3.3228\n",
      "36614,3.3787\n",
      "36615,3.3464\n",
      "36616,3.3356\n",
      "36617,3.3254\n",
      "36618,3.3337\n",
      "36619,3.3565\n",
      "36620,3.3427\n",
      "36621,3.3556\n",
      "36622,3.3526\n",
      "36623,3.3614\n",
      "36624,3.3365\n",
      "36625,3.3703\n",
      "36626,3.3420\n",
      "36627,3.3722\n",
      "36628,3.3628\n",
      "36629,3.3533\n",
      "36630,3.3506\n",
      "36631,3.3529\n",
      "36632,3.3665\n",
      "36633,3.3444\n",
      "36634,3.3360\n",
      "36635,3.3227\n",
      "36636,3.3379\n",
      "36637,3.3858\n",
      "36638,3.3647\n",
      "36639,3.3385\n",
      "36640,3.3171\n",
      "36641,3.3379\n",
      "36642,3.3466\n",
      "36643,3.3178\n",
      "36644,3.3438\n",
      "36645,3.3328\n",
      "36646,3.3375\n",
      "36647,3.3531\n",
      "36648,3.3545\n",
      "36649,3.3681\n",
      "36650,3.3472\n",
      "36651,3.3633\n",
      "36652,3.3401\n",
      "36653,3.3627\n",
      "36654,3.3359\n",
      "36655,3.3675\n",
      "36656,3.3162\n",
      "36657,3.3604\n",
      "36658,3.3624\n",
      "36659,3.3553\n",
      "36660,3.3509\n",
      "36661,3.3441\n",
      "36662,3.3203\n",
      "36663,3.3051\n",
      "36664,3.3490\n",
      "36665,3.3604\n",
      "36666,3.3531\n",
      "36667,3.3719\n",
      "36668,3.3450\n",
      "36669,3.3477\n",
      "36670,3.3529\n",
      "36671,3.3443\n",
      "36672,3.3670\n",
      "36673,3.3525\n",
      "36674,3.3550\n",
      "36675,3.3267\n",
      "36676,3.3440\n",
      "36677,3.3615\n",
      "36678,3.3444\n",
      "36679,3.3365\n",
      "36680,3.3033\n",
      "36681,3.3305\n",
      "36682,3.3491\n",
      "36683,3.3497\n",
      "36684,3.3563\n",
      "36685,3.3264\n",
      "36686,3.3449\n",
      "36687,3.3370\n",
      "36688,3.3548\n",
      "36689,3.3254\n",
      "36690,3.3553\n",
      "36691,3.3597\n",
      "36692,3.3810\n",
      "36693,3.3599\n",
      "36694,3.3332\n",
      "36695,3.3390\n",
      "36696,3.3182\n",
      "36697,3.3798\n",
      "36698,3.3410\n",
      "36699,3.3333\n",
      "36700,3.3366\n",
      "(step: 36700, epoch: 0, block: 37580800), Train Loss: 3.2754, Val Loss: 3.3007\n",
      "Learning rate: 4.15554711e-05\n",
      "Scheduler step: 36700\n",
      "Checkpoint (step: 36700, epoch: 0, block: 37580800) Saved\n",
      "Step Documented\n",
      "03:50:36\n",
      "36701,3.3410\n",
      "36702,3.3258\n",
      "36703,3.3493\n",
      "36704,3.3416\n",
      "36705,3.3438\n",
      "36706,3.3453\n",
      "36707,3.3451\n",
      "36708,3.3679\n",
      "36709,3.3662\n",
      "36710,3.3491\n",
      "36711,3.3252\n",
      "36712,3.3584\n",
      "36713,3.3606\n",
      "36714,3.3475\n",
      "36715,3.3720\n",
      "36716,3.3450\n",
      "36717,3.3585\n",
      "36718,3.3583\n",
      "36719,3.3469\n",
      "36720,3.3401\n",
      "36721,3.3711\n",
      "36722,3.3369\n",
      "36723,3.3542\n",
      "36724,3.3245\n",
      "36725,3.3374\n",
      "36726,3.3683\n",
      "36727,3.3307\n",
      "36728,3.3422\n",
      "36729,3.3521\n",
      "36730,3.3324\n",
      "36731,3.3478\n",
      "36732,3.3483\n",
      "36733,3.3657\n",
      "36734,3.3737\n",
      "36735,3.3703\n",
      "36736,3.3583\n",
      "36737,3.3759\n",
      "36738,3.3508\n",
      "36739,3.3661\n",
      "36740,3.3471\n",
      "36741,3.3431\n",
      "36742,3.3230\n",
      "36743,3.3606\n",
      "36744,3.3603\n",
      "36745,3.3315\n",
      "36746,3.3336\n",
      "36747,3.3594\n",
      "36748,3.3553\n",
      "36749,3.3464\n",
      "36750,3.3447\n",
      "36751,3.3253\n",
      "36752,3.3311\n",
      "36753,3.3471\n",
      "36754,3.3657\n",
      "36755,3.3298\n",
      "36756,3.3567\n",
      "36757,3.3466\n",
      "36758,3.3390\n",
      "36759,3.3381\n",
      "36760,3.3566\n",
      "36761,3.3510\n",
      "36762,3.3543\n",
      "36763,3.3780\n",
      "36764,3.3377\n",
      "36765,3.3518\n",
      "36766,3.3531\n",
      "36767,3.3598\n",
      "36768,3.3342\n",
      "36769,3.3317\n",
      "36770,3.3573\n",
      "36771,3.3158\n",
      "36772,3.3517\n",
      "36773,3.3374\n",
      "36774,3.3602\n",
      "36775,3.3449\n",
      "36776,3.3892\n",
      "36777,3.3588\n",
      "36778,3.3278\n",
      "36779,3.3585\n",
      "36780,3.3793\n",
      "36781,3.3616\n",
      "36782,3.3405\n",
      "36783,3.3623\n",
      "36784,3.3551\n",
      "36785,3.3467\n",
      "36786,3.3509\n",
      "36787,3.3381\n",
      "36788,3.3417\n",
      "36789,3.3465\n",
      "36790,3.3442\n",
      "36791,3.3218\n",
      "36792,3.3660\n",
      "36793,3.3457\n",
      "36794,3.3669\n",
      "36795,3.3498\n",
      "36796,3.3525\n",
      "36797,3.3371\n",
      "36798,3.3719\n",
      "36799,3.3504\n",
      "36800,3.3481\n",
      "(step: 36800, epoch: 0, block: 37683200), Train Loss: 3.2902, Val Loss: 3.2902\n",
      "Learning rate: 4.13330301e-05\n",
      "Scheduler step: 36800\n",
      "Checkpoint (step: 36800, epoch: 0, block: 37683200) Saved\n",
      "Step Documented\n",
      "04:16:56\n",
      "36801,3.3386\n",
      "36802,3.3448\n",
      "36803,3.3404\n",
      "36804,3.3481\n",
      "36805,3.3416\n",
      "36806,3.3401\n",
      "36807,3.3276\n",
      "36808,3.3476\n",
      "36809,3.3416\n",
      "36810,3.3278\n",
      "36811,3.3173\n",
      "36812,3.3422\n",
      "36813,3.3236\n",
      "36814,3.3604\n",
      "36815,3.3258\n",
      "36816,3.3358\n",
      "36817,3.3568\n",
      "36818,3.3514\n",
      "36819,3.3497\n",
      "36820,3.3255\n",
      "36821,3.3401\n",
      "36822,3.3331\n",
      "36823,3.3540\n",
      "36824,3.3269\n",
      "36825,3.3290\n",
      "36826,3.3416\n",
      "36827,3.3767\n",
      "36828,3.3496\n",
      "36829,3.3777\n",
      "36830,3.3430\n",
      "36831,3.3281\n",
      "36832,3.3248\n",
      "36833,3.3767\n",
      "36834,3.3470\n",
      "36835,3.3531\n",
      "36836,3.3316\n",
      "36837,3.3326\n",
      "36838,3.3614\n",
      "36839,3.3554\n",
      "36840,3.3140\n",
      "36841,3.3727\n",
      "36842,3.3379\n",
      "36843,3.3414\n",
      "36844,3.3354\n",
      "36845,3.3420\n",
      "36846,3.3585\n",
      "36847,3.3369\n",
      "36848,3.3572\n",
      "36849,3.3546\n",
      "36850,3.3623\n",
      "36851,3.3446\n",
      "36852,3.3518\n",
      "36853,3.3509\n",
      "36854,3.3568\n",
      "36855,3.3631\n",
      "36856,3.3502\n",
      "36857,3.3482\n",
      "36858,3.3541\n",
      "36859,3.3586\n",
      "36860,3.3468\n",
      "36861,3.3391\n",
      "36862,3.3758\n",
      "36863,3.3606\n",
      "36864,3.3743\n",
      "36865,3.3581\n",
      "36866,3.3250\n",
      "36867,3.3322\n",
      "36868,3.3294\n",
      "36869,3.3243\n",
      "36870,3.3685\n",
      "36871,3.3415\n",
      "36872,3.3482\n",
      "36873,3.3586\n",
      "36874,3.3460\n",
      "36875,3.3631\n",
      "36876,3.3773\n",
      "36877,3.3213\n",
      "36878,3.3465\n",
      "36879,3.3468\n",
      "36880,3.3518\n",
      "36881,3.3234\n",
      "36882,3.3551\n",
      "36883,3.3464\n",
      "36884,3.3307\n",
      "36885,3.3783\n",
      "36886,3.3628\n",
      "36887,3.3659\n",
      "36888,3.3628\n",
      "36889,3.3637\n",
      "36890,3.3437\n",
      "36891,3.3568\n",
      "36892,3.3528\n",
      "36893,3.3533\n",
      "36894,3.3445\n",
      "36895,3.3758\n",
      "36896,3.3265\n",
      "36897,3.3497\n",
      "36898,3.3451\n",
      "36899,3.3328\n",
      "36900,3.3605\n",
      "(step: 36900, epoch: 0, block: 37785600), Train Loss: 3.2706, Val Loss: 3.2717\n",
      "Learning rate: 4.11109092e-05\n",
      "Scheduler step: 36900\n",
      "Checkpoint (step: 36900, epoch: 0, block: 37785600) Saved\n",
      "Step Documented\n",
      "04:43:15\n",
      "36901,3.3901\n",
      "36902,3.3219\n",
      "36903,3.3269\n",
      "36904,3.3396\n",
      "36905,3.3467\n",
      "36906,3.3249\n",
      "36907,3.3421\n",
      "36908,3.3232\n",
      "36909,3.3379\n",
      "36910,3.3730\n",
      "36911,3.3038\n",
      "36912,3.3576\n",
      "36913,3.3406\n",
      "36914,3.3401\n",
      "36915,3.3301\n",
      "36916,3.3318\n",
      "36917,3.3953\n",
      "36918,3.3493\n",
      "36919,3.3643\n",
      "36920,3.3489\n",
      "36921,3.3471\n",
      "36922,3.3767\n",
      "36923,3.3098\n",
      "36924,3.3333\n",
      "36925,3.3479\n",
      "36926,3.3338\n",
      "36927,3.3499\n",
      "36928,3.3154\n",
      "36929,3.3675\n",
      "36930,3.3469\n",
      "36931,3.3309\n",
      "36932,3.3477\n",
      "36933,3.3379\n",
      "36934,3.3724\n",
      "36935,3.3663\n",
      "36936,3.3483\n",
      "36937,3.3447\n",
      "36938,3.3429\n",
      "36939,3.3342\n",
      "36940,3.3284\n",
      "36941,3.3514\n",
      "36942,3.3336\n",
      "36943,3.3497\n",
      "36944,3.3482\n",
      "36945,3.3579\n",
      "36946,3.3275\n",
      "36947,3.3499\n",
      "36948,3.3518\n",
      "36949,3.3384\n",
      "36950,3.3444\n",
      "36951,3.3489\n",
      "36952,3.3410\n",
      "36953,3.3364\n",
      "36954,3.3466\n",
      "36955,3.3122\n",
      "36956,3.3676\n",
      "36957,3.3834\n",
      "36958,3.3618\n",
      "36959,3.3200\n",
      "36960,3.3527\n",
      "36961,3.3370\n",
      "36962,3.3807\n",
      "36963,3.3303\n",
      "36964,3.3390\n",
      "36965,3.3394\n",
      "36966,3.3500\n",
      "36967,3.3516\n",
      "36968,3.3506\n",
      "36969,3.3488\n",
      "36970,3.3534\n",
      "36971,3.3409\n",
      "36972,3.3215\n",
      "36973,3.3365\n",
      "36974,3.3437\n",
      "36975,3.3355\n",
      "36976,3.3418\n",
      "36977,3.3624\n",
      "36978,3.3345\n",
      "36979,3.3687\n",
      "36980,3.3212\n",
      "36981,3.3675\n",
      "36982,3.3545\n",
      "36983,3.3433\n",
      "36984,3.3577\n",
      "36985,3.3720\n",
      "36986,3.3709\n",
      "36987,3.3637\n",
      "36988,3.3508\n",
      "36989,3.3432\n",
      "36990,3.3284\n",
      "36991,3.3558\n",
      "36992,3.3306\n",
      "36993,3.3304\n",
      "36994,3.3781\n",
      "36995,3.3534\n",
      "36996,3.3738\n",
      "36997,3.3655\n",
      "36998,3.3388\n",
      "36999,3.3772\n",
      "37000,3.3555\n",
      "(step: 37000, epoch: 0, block: 37888000), Train Loss: 3.2404, Val Loss: 3.3135\n",
      "Learning rate: 4.08891139e-05\n",
      "Scheduler step: 37000\n",
      "Checkpoint (step: 37000, epoch: 0, block: 37888000) Saved\n",
      "Step Documented\n",
      "05:09:36\n",
      "37001,3.3338\n",
      "37002,3.3764\n",
      "37003,3.3347\n",
      "37004,3.3385\n",
      "37005,3.3471\n",
      "37006,3.3498\n",
      "37007,3.3729\n",
      "37008,3.3550\n",
      "37009,3.3451\n",
      "37010,3.3387\n",
      "37011,3.3734\n",
      "37012,3.3550\n",
      "37013,3.3381\n",
      "37014,3.3624\n",
      "37015,3.3543\n",
      "37016,3.3500\n",
      "37017,3.3572\n",
      "37018,3.3323\n",
      "37019,3.3762\n",
      "37020,3.3583\n",
      "37021,3.3305\n",
      "37022,3.3306\n",
      "37023,3.3281\n",
      "37024,3.3462\n",
      "37025,3.3373\n",
      "37026,3.3400\n",
      "37027,3.3656\n",
      "37028,3.3724\n",
      "37029,3.3306\n",
      "37030,3.3362\n",
      "37031,3.3233\n",
      "37032,3.3665\n",
      "37033,3.3533\n",
      "37034,3.3258\n",
      "37035,3.3727\n",
      "37036,3.3404\n",
      "37037,3.3431\n",
      "37038,3.3458\n",
      "37039,3.3431\n",
      "37040,3.3453\n",
      "37041,3.3620\n",
      "37042,3.3256\n",
      "37043,3.3640\n",
      "37044,3.3609\n",
      "37045,3.3455\n",
      "37046,3.3484\n",
      "37047,3.3471\n",
      "37048,3.3206\n",
      "37049,3.3320\n",
      "37050,3.3528\n",
      "37051,3.3011\n",
      "37052,3.3494\n",
      "37053,3.3542\n",
      "37054,3.3600\n",
      "37055,3.3456\n",
      "37056,3.3526\n",
      "37057,3.3373\n",
      "37058,3.3625\n",
      "37059,3.3451\n",
      "37060,3.3519\n",
      "37061,3.3280\n",
      "37062,3.3410\n",
      "37063,3.3680\n",
      "37064,3.3302\n",
      "37065,3.3279\n",
      "37066,3.3635\n",
      "37067,3.3516\n",
      "37068,3.3348\n",
      "37069,3.3759\n",
      "37070,3.3434\n",
      "37071,3.3487\n",
      "37072,3.3433\n",
      "37073,3.3351\n",
      "37074,3.3301\n",
      "37075,3.3306\n",
      "37076,3.3349\n",
      "37077,3.3268\n",
      "37078,3.3543\n",
      "37079,3.3616\n",
      "37080,3.3608\n",
      "37081,3.3298\n",
      "37082,3.3473\n",
      "37083,3.3752\n",
      "37084,3.3397\n",
      "37085,3.3447\n",
      "37086,3.3379\n",
      "37087,3.3232\n",
      "37088,3.3573\n",
      "37089,3.3355\n",
      "37090,3.3360\n",
      "37091,3.3464\n",
      "37092,3.3514\n",
      "37093,3.3494\n",
      "37094,3.3509\n",
      "37095,3.3237\n",
      "37096,3.3455\n",
      "37097,3.3561\n",
      "37098,3.3340\n",
      "37099,3.3297\n",
      "37100,3.3566\n",
      "(step: 37100, epoch: 0, block: 37990400), Train Loss: 3.2666, Val Loss: 3.2927\n",
      "Learning rate: 4.06676499e-05\n",
      "Scheduler step: 37100\n",
      "Checkpoint (step: 37100, epoch: 0, block: 37990400) Saved\n",
      "Step Documented\n",
      "05:35:56\n",
      "37101,3.3619\n",
      "37102,3.3354\n",
      "37103,3.3802\n",
      "37104,3.3299\n",
      "37105,3.3443\n",
      "37106,3.3628\n",
      "37107,3.3743\n",
      "37108,3.3241\n",
      "37109,3.3473\n",
      "37110,3.3381\n",
      "37111,3.3687\n",
      "37112,3.3473\n",
      "37113,3.3603\n",
      "37114,3.3708\n",
      "37115,3.3390\n",
      "37116,3.3441\n",
      "37117,3.3763\n",
      "37118,3.3592\n",
      "37119,3.3371\n",
      "37120,3.3479\n",
      "37121,3.3266\n",
      "37122,3.3409\n",
      "37123,3.3827\n",
      "37124,3.3267\n",
      "37125,3.3279\n",
      "37126,3.3351\n",
      "37127,3.3600\n",
      "37128,3.3204\n",
      "37129,3.3411\n",
      "37130,3.3551\n",
      "37131,3.3512\n",
      "37132,3.3230\n",
      "37133,3.3404\n",
      "37134,3.3323\n",
      "37135,3.3633\n",
      "37136,3.3275\n",
      "37137,3.3580\n",
      "37138,3.3163\n",
      "37139,3.3615\n",
      "37140,3.3752\n",
      "37141,3.3247\n",
      "37142,3.3265\n",
      "37143,3.3794\n",
      "37144,3.3598\n",
      "37145,3.3222\n",
      "37146,3.3482\n",
      "37147,3.3522\n",
      "37148,3.3511\n",
      "37149,3.3387\n",
      "37150,3.3362\n",
      "37151,3.3394\n",
      "37152,3.3421\n",
      "37153,3.3610\n",
      "37154,3.3475\n",
      "37155,3.3292\n",
      "37156,3.3486\n",
      "37157,3.3669\n",
      "37158,3.3202\n",
      "37159,3.3737\n",
      "37160,3.3136\n",
      "37161,3.3505\n",
      "37162,3.3640\n",
      "37163,3.3518\n",
      "37164,3.3432\n",
      "37165,3.3540\n",
      "37166,3.3509\n",
      "37167,3.3198\n",
      "37168,3.3636\n",
      "37169,3.3813\n",
      "37170,3.3427\n",
      "37171,3.3231\n",
      "37172,3.3508\n",
      "37173,3.3424\n",
      "37174,3.2932\n",
      "37175,3.3650\n",
      "37176,3.3187\n",
      "37177,3.3372\n",
      "37178,3.3382\n",
      "37179,3.3392\n",
      "37180,3.3372\n",
      "37181,3.3443\n",
      "37182,3.3327\n",
      "37183,3.3420\n",
      "37184,3.3198\n",
      "37185,3.3440\n",
      "37186,3.3367\n",
      "37187,3.3353\n",
      "37188,3.3393\n",
      "37189,3.3290\n",
      "37190,3.3620\n",
      "37191,3.3479\n",
      "37192,3.3543\n",
      "37193,3.3446\n",
      "37194,3.3510\n",
      "37195,3.3572\n",
      "37196,3.3126\n",
      "37197,3.3556\n",
      "37198,3.3346\n",
      "37199,3.3316\n",
      "37200,3.3354\n",
      "(step: 37200, epoch: 0, block: 38092800), Train Loss: 3.2637, Val Loss: 3.2943\n",
      "Learning rate: 4.04465228e-05\n",
      "Scheduler step: 37200\n",
      "Checkpoint (step: 37200, epoch: 0, block: 38092800) Saved\n",
      "Step Documented\n",
      "06:02:16\n",
      "37201,3.3309\n",
      "37202,3.3535\n",
      "37203,3.3528\n",
      "37204,3.3494\n",
      "37205,3.3683\n",
      "37206,3.3473\n",
      "37207,3.3688\n",
      "37208,3.3615\n",
      "37209,3.3521\n",
      "37210,3.3503\n",
      "37211,3.3673\n",
      "37212,3.3422\n",
      "37213,3.3238\n",
      "37214,3.3533\n",
      "37215,3.3736\n",
      "37216,3.3227\n",
      "37217,3.3256\n",
      "37218,3.3566\n",
      "37219,3.3583\n",
      "37220,3.3357\n",
      "37221,3.3290\n",
      "37222,3.3438\n",
      "37223,3.3650\n",
      "37224,3.3126\n",
      "37225,3.3454\n",
      "37226,3.3692\n",
      "37227,3.3779\n",
      "37228,3.3567\n",
      "37229,3.3451\n",
      "37230,3.3631\n",
      "37231,3.3628\n",
      "37232,3.3285\n",
      "37233,3.3558\n",
      "37234,3.3633\n",
      "37235,3.3352\n",
      "37236,3.3845\n",
      "37237,3.3211\n",
      "37238,3.3364\n",
      "37239,3.3740\n",
      "37240,3.3759\n",
      "37241,3.3633\n",
      "37242,3.3135\n",
      "37243,3.3612\n",
      "37244,3.3436\n",
      "37245,3.3227\n",
      "37246,3.3535\n",
      "37247,3.3455\n",
      "37248,3.3027\n",
      "37249,3.3646\n",
      "37250,3.3291\n",
      "37251,3.3506\n",
      "37252,3.3607\n",
      "37253,3.3451\n",
      "37254,3.3695\n",
      "37255,3.3545\n",
      "37256,3.3283\n",
      "37257,3.3210\n",
      "37258,3.3519\n",
      "37259,3.3453\n",
      "37260,3.3465\n",
      "37261,3.3728\n",
      "37262,3.3490\n",
      "37263,3.3325\n",
      "37264,3.3511\n",
      "37265,3.3700\n",
      "37266,3.3589\n",
      "37267,3.3183\n",
      "37268,3.3668\n",
      "37269,3.3535\n",
      "37270,3.3510\n",
      "37271,3.3603\n",
      "37272,3.3766\n",
      "37273,3.3399\n",
      "37274,3.3756\n",
      "37275,3.3302\n",
      "37276,3.3428\n",
      "37277,3.3382\n",
      "37278,3.3771\n",
      "37279,3.3440\n",
      "37280,3.3727\n",
      "37281,3.3410\n",
      "37282,3.3553\n",
      "37283,3.3297\n",
      "37284,3.3633\n",
      "37285,3.3638\n",
      "37286,3.3398\n",
      "37287,3.3706\n",
      "37288,3.3169\n",
      "37289,3.3631\n",
      "37290,3.3503\n",
      "37291,3.3475\n",
      "37292,3.3519\n",
      "37293,3.3408\n",
      "37294,3.3456\n",
      "37295,3.3385\n",
      "37296,3.3470\n",
      "37297,3.3413\n",
      "37298,3.3509\n",
      "37299,3.3268\n",
      "37300,3.3430\n",
      "(step: 37300, epoch: 0, block: 38195200), Train Loss: 3.2864, Val Loss: 3.2712\n",
      "Learning rate: 4.02257380e-05\n",
      "Scheduler step: 37300\n",
      "Checkpoint (step: 37300, epoch: 0, block: 38195200) Saved\n",
      "Step Documented\n",
      "06:28:36\n",
      "37301,3.3334\n",
      "37302,3.3220\n",
      "37303,3.3571\n",
      "37304,3.3775\n",
      "37305,3.3863\n",
      "37306,3.3651\n",
      "37307,3.3261\n",
      "37308,3.3523\n",
      "37309,3.3458\n",
      "37310,3.3546\n",
      "37311,3.3466\n",
      "37312,3.3776\n",
      "37313,3.3172\n",
      "37314,3.3485\n",
      "37315,3.3538\n",
      "37316,3.3395\n",
      "37317,3.3736\n",
      "37318,3.3476\n",
      "37319,3.3226\n",
      "37320,3.3889\n",
      "37321,3.3392\n",
      "37322,3.3574\n",
      "37323,3.3371\n",
      "37324,3.3661\n",
      "37325,3.3415\n",
      "37326,3.3549\n",
      "37327,3.3614\n",
      "37328,3.3467\n",
      "37329,3.3431\n",
      "37330,3.3409\n",
      "37331,3.3251\n",
      "37332,3.3548\n",
      "37333,3.3287\n",
      "37334,3.3739\n",
      "37335,3.3318\n",
      "37336,3.3638\n",
      "37337,3.3155\n",
      "37338,3.3346\n",
      "37339,3.3614\n",
      "37340,3.3782\n",
      "37341,3.3195\n",
      "37342,3.3599\n",
      "37343,3.3202\n",
      "37344,3.3643\n",
      "37345,3.3266\n",
      "37346,3.3579\n",
      "37347,3.3256\n",
      "37348,3.3383\n",
      "37349,3.3454\n",
      "37350,3.3347\n",
      "37351,3.3504\n",
      "37352,3.3557\n",
      "37353,3.3415\n",
      "37354,3.3362\n",
      "37355,3.3537\n",
      "37356,3.3144\n",
      "37357,3.3467\n",
      "37358,3.3504\n",
      "37359,3.3514\n",
      "37360,3.3488\n",
      "37361,3.3352\n",
      "37362,3.3652\n",
      "37363,3.3716\n",
      "37364,3.3349\n",
      "37365,3.3616\n",
      "37366,3.3577\n",
      "37367,3.3304\n",
      "37368,3.3563\n",
      "37369,3.3144\n",
      "37370,3.3337\n",
      "37371,3.3469\n",
      "37372,3.3429\n",
      "37373,3.3460\n",
      "37374,3.3309\n",
      "37375,3.3426\n",
      "37376,3.3381\n",
      "37377,3.3368\n",
      "37378,3.3612\n",
      "37379,3.3577\n",
      "37380,3.3380\n",
      "37381,3.3396\n",
      "37382,3.3798\n",
      "37383,3.3305\n",
      "37384,3.3296\n",
      "37385,3.3344\n",
      "37386,3.3353\n",
      "37387,3.3593\n",
      "37388,3.3637\n",
      "37389,3.3596\n",
      "37390,3.3487\n",
      "37391,3.3710\n",
      "37392,3.3514\n",
      "37393,3.3518\n",
      "37394,3.3325\n",
      "37395,3.3415\n",
      "37396,3.3337\n",
      "37397,3.3569\n",
      "37398,3.3426\n",
      "37399,3.3454\n",
      "37400,3.3365\n",
      "(step: 37400, epoch: 0, block: 38297600), Train Loss: 3.2697, Val Loss: 3.2856\n",
      "Learning rate: 4.00053014e-05\n",
      "Scheduler step: 37400\n",
      "Checkpoint (step: 37400, epoch: 0, block: 38297600) Saved\n",
      "Step Documented\n",
      "06:54:55\n",
      "37401,3.3646\n",
      "37402,3.3613\n",
      "37403,3.3262\n",
      "37404,3.3789\n",
      "37405,3.3256\n",
      "37406,3.3600\n",
      "37407,3.3294\n",
      "37408,3.3162\n",
      "37409,3.3640\n",
      "37410,3.3450\n",
      "37411,3.3521\n",
      "37412,3.3687\n",
      "37413,3.3674\n",
      "37414,3.3522\n",
      "37415,3.3830\n",
      "37416,3.3483\n",
      "37417,3.3431\n",
      "37418,3.3324\n",
      "37419,3.3662\n",
      "37420,3.3523\n",
      "37421,3.3332\n",
      "37422,3.3316\n",
      "37423,3.3845\n",
      "37424,3.3413\n",
      "37425,3.3170\n",
      "37426,3.3442\n",
      "37427,3.3424\n",
      "37428,3.3763\n",
      "37429,3.3435\n",
      "37430,3.3383\n",
      "37431,3.3461\n",
      "37432,3.3288\n",
      "37433,3.3208\n",
      "37434,3.3621\n",
      "37435,3.3215\n",
      "37436,3.3510\n",
      "37437,3.3468\n",
      "37438,3.3375\n",
      "37439,3.3570\n",
      "37440,3.3417\n",
      "37441,3.3523\n",
      "37442,3.3486\n",
      "37443,3.3745\n",
      "37444,3.3617\n",
      "37445,3.3607\n",
      "37446,3.3358\n",
      "37447,3.3765\n",
      "37448,3.3400\n",
      "37449,3.3409\n",
      "37450,3.3541\n",
      "37451,3.3607\n",
      "37452,3.3400\n",
      "37453,3.3508\n",
      "37454,3.3508\n",
      "37455,3.3616\n",
      "37456,3.3743\n",
      "37457,3.3389\n",
      "37458,3.3559\n",
      "37459,3.3387\n",
      "37460,3.3235\n",
      "37461,3.3359\n",
      "37462,3.3531\n",
      "37463,3.3623\n",
      "37464,3.3280\n",
      "37465,3.3732\n",
      "37466,3.3492\n",
      "37467,3.3337\n",
      "37468,3.3600\n",
      "37469,3.3416\n",
      "37470,3.3739\n",
      "37471,3.3612\n",
      "37472,3.3416\n",
      "37473,3.3283\n",
      "37474,3.3456\n",
      "37475,3.3287\n",
      "37476,3.3595\n",
      "37477,3.3219\n",
      "37478,3.3563\n",
      "37479,3.3398\n",
      "37480,3.3349\n",
      "37481,3.3689\n",
      "37482,3.3704\n",
      "37483,3.3326\n",
      "37484,3.3297\n",
      "37485,3.3304\n",
      "37486,3.3555\n",
      "37487,3.3600\n",
      "37488,3.3331\n",
      "37489,3.3258\n",
      "37490,3.3406\n",
      "37491,3.3612\n",
      "37492,3.3484\n",
      "37493,3.3373\n",
      "37494,3.3369\n",
      "37495,3.3384\n",
      "37496,3.3338\n",
      "37497,3.3528\n",
      "37498,3.3329\n",
      "37499,3.3706\n",
      "37500,3.3322\n",
      "(step: 37500, epoch: 0, block: 38400000), Train Loss: 3.2687, Val Loss: 3.2911\n",
      "Learning rate: 3.97852183e-05\n",
      "Scheduler step: 37500\n",
      "Checkpoint (step: 37500, epoch: 0, block: 38400000) Saved\n",
      "Step Documented\n",
      "07:21:15\n",
      "37501,3.3533\n",
      "37502,3.3362\n",
      "37503,3.3462\n",
      "37504,3.3440\n",
      "37505,3.3527\n",
      "37506,3.3611\n",
      "37507,3.3190\n",
      "37508,3.3446\n",
      "37509,3.3458\n",
      "37510,3.3851\n",
      "37511,3.3582\n",
      "37512,3.3522\n",
      "37513,3.3425\n",
      "37514,3.3083\n",
      "37515,3.3733\n",
      "37516,3.3376\n",
      "37517,3.3260\n",
      "37518,3.3552\n",
      "37519,3.3467\n",
      "37520,3.3537\n",
      "37521,3.3738\n",
      "37522,3.3498\n",
      "37523,3.3077\n",
      "37524,3.3376\n",
      "37525,3.3659\n",
      "37526,3.3406\n",
      "37527,3.3550\n",
      "37528,3.3608\n",
      "37529,3.3227\n",
      "37530,3.3454\n",
      "37531,3.3481\n",
      "37532,3.3664\n",
      "37533,3.3576\n",
      "37534,3.3651\n",
      "37535,3.3299\n",
      "37536,3.3368\n",
      "37537,3.3356\n",
      "37538,3.3683\n",
      "37539,3.3358\n",
      "37540,3.3622\n",
      "37541,3.3428\n",
      "37542,3.3534\n",
      "37543,3.3461\n",
      "37544,3.3441\n",
      "37545,3.3736\n",
      "37546,3.3484\n",
      "37547,3.3529\n",
      "37548,3.3379\n",
      "37549,3.3437\n",
      "37550,3.3335\n",
      "37551,3.3508\n",
      "37552,3.3596\n",
      "37553,3.3582\n",
      "37554,3.3351\n",
      "37555,3.3456\n",
      "37556,3.3111\n",
      "37557,3.3352\n",
      "37558,3.3554\n",
      "37559,3.3593\n",
      "37560,3.3417\n",
      "37561,3.3623\n",
      "37562,3.3417\n",
      "37563,3.3426\n",
      "37564,3.3355\n",
      "37565,3.3242\n",
      "37566,3.3672\n",
      "37567,3.3710\n",
      "37568,3.3358\n",
      "37569,3.3431\n",
      "37570,3.3323\n",
      "37571,3.3465\n",
      "37572,3.3300\n",
      "37573,3.3494\n",
      "37574,3.3223\n",
      "37575,3.3686\n",
      "37576,3.3445\n",
      "37577,3.3594\n",
      "37578,3.3348\n",
      "37579,3.3537\n",
      "37580,3.3469\n",
      "37581,3.3570\n",
      "37582,3.3206\n",
      "37583,3.3948\n",
      "37584,3.3567\n",
      "37585,3.3312\n",
      "37586,3.3391\n",
      "37587,3.3498\n",
      "37588,3.3634\n",
      "37589,3.3562\n",
      "37590,3.3521\n",
      "37591,3.3320\n",
      "37592,3.3177\n",
      "37593,3.3484\n",
      "37594,3.3354\n",
      "37595,3.3415\n",
      "37596,3.3375\n",
      "37597,3.3272\n",
      "37598,3.3175\n",
      "37599,3.3496\n",
      "37600,3.3387\n",
      "(step: 37600, epoch: 0, block: 38502400), Train Loss: 3.2737, Val Loss: 3.2555\n",
      "Learning rate: 3.95654943e-05\n",
      "Scheduler step: 37600\n",
      "Checkpoint (step: 37600, epoch: 0, block: 38502400) Saved\n",
      "Step Documented\n",
      "07:47:35\n",
      "37601,3.3455\n",
      "37602,3.3382\n",
      "37603,3.3781\n",
      "37604,3.3558\n",
      "37605,3.3636\n",
      "37606,3.3529\n",
      "37607,3.3445\n",
      "37608,3.3307\n",
      "37609,3.3380\n",
      "37610,3.3547\n",
      "37611,3.3352\n",
      "37612,3.3663\n",
      "37613,3.3272\n",
      "37614,3.3082\n",
      "37615,3.3641\n",
      "37616,3.3356\n",
      "37617,3.3274\n",
      "37618,3.3701\n",
      "37619,3.3322\n",
      "37620,3.3548\n",
      "37621,3.3522\n",
      "37622,3.3254\n",
      "37623,3.3419\n",
      "37624,3.3514\n",
      "37625,3.3396\n",
      "37626,3.3575\n",
      "37627,3.3289\n",
      "37628,3.3603\n",
      "37629,3.3368\n",
      "37630,3.3427\n",
      "37631,3.3659\n",
      "37632,3.3659\n",
      "37633,3.3610\n",
      "37634,3.3533\n",
      "37635,3.3621\n",
      "37636,3.3489\n",
      "37637,3.3552\n",
      "37638,3.3435\n",
      "37639,3.3340\n",
      "37640,3.3618\n",
      "37641,3.3621\n",
      "37642,3.3402\n",
      "37643,3.3441\n",
      "37644,3.3294\n",
      "37645,3.3445\n",
      "37646,3.3224\n",
      "37647,3.3449\n",
      "37648,3.3357\n",
      "37649,3.3307\n",
      "37650,3.3430\n",
      "37651,3.3269\n",
      "37652,3.3364\n",
      "37653,3.3707\n",
      "37654,3.3604\n",
      "37655,3.3487\n",
      "37656,3.3399\n",
      "37657,3.3705\n",
      "37658,3.3616\n",
      "37659,3.3372\n",
      "37660,3.3699\n",
      "37661,3.3652\n",
      "37662,3.3353\n",
      "37663,3.3482\n",
      "37664,3.3652\n",
      "37665,3.3477\n",
      "37666,3.3480\n",
      "37667,3.3213\n",
      "37668,3.3530\n",
      "37669,3.3578\n",
      "37670,3.3274\n",
      "37671,3.3546\n",
      "37672,3.3650\n",
      "37673,3.3526\n",
      "37674,3.3348\n",
      "37675,3.3355\n",
      "37676,3.3475\n",
      "37677,3.3630\n",
      "37678,3.3531\n",
      "37679,3.3474\n",
      "37680,3.3568\n",
      "37681,3.3564\n",
      "37682,3.3599\n",
      "37683,3.3486\n",
      "37684,3.3928\n",
      "37685,3.3433\n",
      "37686,3.3391\n",
      "37687,3.3221\n",
      "37688,3.3344\n",
      "37689,3.3366\n",
      "37690,3.3295\n",
      "37691,3.3546\n",
      "37692,3.3520\n",
      "37693,3.3799\n",
      "37694,3.3822\n",
      "37695,3.3465\n",
      "37696,3.3179\n",
      "37697,3.3332\n",
      "37698,3.3371\n",
      "37699,3.3294\n",
      "37700,3.3078\n",
      "(step: 37700, epoch: 0, block: 38604800), Train Loss: 3.2731, Val Loss: 3.2848\n",
      "Learning rate: 3.93461351e-05\n",
      "Scheduler step: 37700\n",
      "Checkpoint (step: 37700, epoch: 0, block: 38604800) Saved\n",
      "Step Documented\n",
      "08:13:55\n",
      "37701,3.3804\n",
      "37702,3.3205\n",
      "37703,3.3144\n",
      "37704,3.3646\n",
      "37705,3.3467\n",
      "37706,3.3397\n",
      "37707,3.3573\n",
      "37708,3.3572\n",
      "37709,3.3588\n",
      "37710,3.3544\n",
      "37711,3.3469\n",
      "37712,3.3531\n",
      "37713,3.3280\n",
      "37714,3.3699\n",
      "37715,3.3734\n",
      "37716,3.3574\n",
      "37717,3.3416\n",
      "37718,3.3219\n",
      "37719,3.3480\n",
      "37720,3.3304\n",
      "37721,3.3696\n",
      "37722,3.3587\n",
      "37723,3.3524\n",
      "37724,3.3588\n",
      "37725,3.3323\n",
      "37726,3.3536\n",
      "37727,3.3661\n",
      "37728,3.3384\n",
      "37729,3.3328\n",
      "37730,3.3314\n",
      "37731,3.3257\n",
      "37732,3.3346\n",
      "37733,3.3763\n",
      "37734,3.3558\n",
      "37735,3.3616\n",
      "37736,3.3487\n",
      "37737,3.3075\n",
      "37738,3.3416\n",
      "37739,3.3679\n",
      "37740,3.3402\n",
      "37741,3.3512\n",
      "37742,3.3390\n",
      "37743,3.3511\n",
      "37744,3.3432\n",
      "37745,3.3502\n",
      "37746,3.3318\n",
      "37747,3.3224\n",
      "37748,3.3646\n",
      "37749,3.3213\n",
      "37750,3.3435\n",
      "37751,3.3452\n",
      "37752,3.3322\n",
      "37753,3.3377\n",
      "37754,3.3456\n",
      "37755,3.3613\n",
      "37756,3.3337\n",
      "37757,3.3498\n",
      "37758,3.3447\n",
      "37759,3.3606\n",
      "37760,3.3447\n",
      "37761,3.3678\n",
      "37762,3.3260\n",
      "37763,3.3493\n",
      "37764,3.3639\n",
      "37765,3.3183\n",
      "37766,3.3656\n",
      "37767,3.3681\n",
      "37768,3.3364\n",
      "37769,3.3212\n",
      "37770,3.3492\n",
      "37771,3.3449\n",
      "37772,3.3676\n",
      "37773,3.3445\n",
      "37774,3.3545\n",
      "37775,3.3411\n",
      "37776,3.3396\n",
      "37777,3.3301\n",
      "37778,3.3393\n",
      "37779,3.3317\n",
      "37780,3.3529\n",
      "37781,3.3660\n",
      "37782,3.3350\n",
      "37783,3.3550\n",
      "37784,3.3143\n",
      "37785,3.3510\n",
      "37786,3.3512\n",
      "37787,3.3435\n",
      "37788,3.3603\n",
      "37789,3.3392\n",
      "37790,3.3274\n",
      "37791,3.3605\n",
      "37792,3.3241\n",
      "37793,3.3394\n",
      "37794,3.3455\n",
      "37795,3.3627\n",
      "37796,3.3483\n",
      "37797,3.3312\n",
      "37798,3.3115\n",
      "37799,3.3366\n",
      "37800,3.3625\n",
      "(step: 37800, epoch: 0, block: 38707200), Train Loss: 3.2425, Val Loss: 3.2701\n",
      "Learning rate: 3.91271461e-05\n",
      "Scheduler step: 37800\n",
      "Checkpoint (step: 37800, epoch: 0, block: 38707200) Saved\n",
      "Step Documented\n",
      "08:40:14\n",
      "37801,3.3494\n",
      "37802,3.3554\n",
      "37803,3.3539\n",
      "37804,3.3182\n",
      "37805,3.3517\n",
      "37806,3.3391\n",
      "37807,3.3637\n",
      "37808,3.3525\n",
      "37809,3.3468\n",
      "37810,3.3100\n",
      "37811,3.3211\n",
      "37812,3.3636\n",
      "37813,3.3448\n",
      "37814,3.3474\n",
      "37815,3.3497\n",
      "37816,3.3695\n",
      "37817,3.3254\n",
      "37818,3.3650\n",
      "37819,3.3401\n",
      "37820,3.3475\n",
      "37821,3.3273\n",
      "37822,3.3339\n",
      "37823,3.3513\n",
      "37824,3.3501\n",
      "37825,3.3277\n",
      "37826,3.3415\n",
      "37827,3.3201\n",
      "37828,3.3678\n",
      "37829,3.3283\n",
      "37830,3.3336\n",
      "37831,3.3210\n",
      "37832,3.3295\n",
      "37833,3.3629\n",
      "37834,3.3281\n",
      "37835,3.3360\n",
      "37836,3.3616\n",
      "37837,3.3521\n",
      "37838,3.3469\n",
      "37839,3.3068\n",
      "37840,3.3690\n",
      "37841,3.3493\n",
      "37842,3.3743\n",
      "37843,3.3571\n",
      "37844,3.3425\n",
      "37845,3.3460\n",
      "37846,3.3159\n",
      "37847,3.3471\n",
      "37848,3.3507\n",
      "37849,3.3364\n",
      "37850,3.3363\n",
      "37851,3.3335\n",
      "37852,3.3367\n",
      "37853,3.3260\n",
      "37854,3.3438\n",
      "37855,3.3433\n",
      "37856,3.3612\n",
      "37857,3.3171\n",
      "37858,3.3463\n",
      "37859,3.3399\n",
      "37860,3.3329\n",
      "37861,3.3387\n",
      "37862,3.3503\n",
      "37863,3.3441\n",
      "37864,3.3518\n",
      "37865,3.3456\n",
      "37866,3.3670\n",
      "37867,3.3501\n",
      "37868,3.3874\n",
      "37869,3.3551\n",
      "37870,3.3593\n",
      "37871,3.3497\n",
      "37872,3.3657\n",
      "37873,3.3388\n",
      "37874,3.3575\n",
      "37875,3.3554\n",
      "37876,3.3618\n",
      "37877,3.3740\n",
      "37878,3.3271\n",
      "37879,3.3264\n",
      "37880,3.3222\n",
      "37881,3.3440\n",
      "37882,3.3734\n",
      "37883,3.3373\n",
      "37884,3.3089\n",
      "37885,3.3345\n",
      "37886,3.3723\n",
      "37887,3.3361\n",
      "37888,3.3498\n",
      "37889,3.3508\n",
      "37890,3.3527\n",
      "37891,3.3450\n",
      "37892,3.3507\n",
      "37893,3.3376\n",
      "37894,3.3627\n",
      "37895,3.3609\n",
      "37896,3.3114\n",
      "37897,3.3297\n",
      "37898,3.3296\n",
      "37899,3.3184\n",
      "37900,3.3437\n",
      "(step: 37900, epoch: 0, block: 38809600), Train Loss: 3.2454, Val Loss: 3.2678\n",
      "Learning rate: 3.89085329e-05\n",
      "Scheduler step: 37900\n",
      "Checkpoint (step: 37900, epoch: 0, block: 38809600) Saved\n",
      "Step Documented\n",
      "09:06:34\n",
      "37901,3.3291\n",
      "37902,3.3368\n",
      "37903,3.3537\n",
      "37904,3.3333\n",
      "37905,3.3160\n",
      "37906,3.3599\n",
      "37907,3.3486\n",
      "37908,3.3513\n",
      "37909,3.3573\n",
      "37910,3.3461\n",
      "37911,3.3277\n",
      "37912,3.3482\n",
      "37913,3.3251\n",
      "37914,3.3610\n",
      "37915,3.3061\n",
      "37916,3.3148\n",
      "37917,3.3477\n",
      "37918,3.3356\n",
      "37919,3.3317\n",
      "37920,3.3443\n",
      "37921,3.3511\n",
      "37922,3.3499\n",
      "37923,3.3386\n",
      "37924,3.3269\n",
      "37925,3.3615\n",
      "37926,3.3606\n",
      "37927,3.3482\n",
      "37928,3.3556\n",
      "37929,3.3324\n",
      "37930,3.3767\n",
      "37931,3.3177\n",
      "37932,3.3420\n",
      "37933,3.3187\n",
      "37934,3.3449\n",
      "37935,3.3446\n",
      "37936,3.3544\n",
      "37937,3.3523\n",
      "37938,3.3393\n",
      "37939,3.3337\n",
      "37940,3.3180\n",
      "37941,3.3080\n",
      "37942,3.3438\n",
      "37943,3.3327\n",
      "37944,3.3195\n",
      "37945,3.3267\n",
      "37946,3.3312\n",
      "37947,3.3430\n",
      "37948,3.3218\n",
      "37949,3.3414\n",
      "37950,3.3424\n",
      "37951,3.3444\n",
      "37952,3.3383\n",
      "37953,3.3427\n",
      "37954,3.3250\n",
      "37955,3.3366\n",
      "37956,3.3532\n",
      "37957,3.3785\n",
      "37958,3.3309\n",
      "37959,3.3268\n",
      "37960,3.3311\n",
      "37961,3.3532\n",
      "37962,3.3516\n",
      "37963,3.3769\n",
      "37964,3.3423\n",
      "37965,3.3174\n",
      "37966,3.3990\n",
      "37967,3.3541\n",
      "37968,3.3560\n",
      "37969,3.3409\n",
      "37970,3.3619\n",
      "37971,3.3436\n",
      "37972,3.3228\n",
      "37973,3.3342\n",
      "37974,3.3891\n",
      "37975,3.3386\n",
      "37976,3.3697\n",
      "37977,3.3384\n",
      "37978,3.3710\n",
      "37979,3.3259\n",
      "37980,3.3718\n",
      "37981,3.3424\n",
      "37982,3.3189\n",
      "37983,3.3465\n",
      "37984,3.3420\n",
      "37985,3.3771\n",
      "37986,3.3605\n",
      "37987,3.3537\n",
      "37988,3.3574\n",
      "37989,3.3431\n",
      "37990,3.3690\n",
      "37991,3.3431\n",
      "37992,3.3618\n",
      "37993,3.3282\n",
      "37994,3.3510\n",
      "37995,3.3343\n",
      "37996,3.3531\n",
      "37997,3.3323\n",
      "37998,3.3297\n",
      "37999,3.3237\n",
      "38000,3.3602\n",
      "(step: 38000, epoch: 0, block: 38912000), Train Loss: 3.2344, Val Loss: 3.2731\n",
      "Learning rate: 3.86903010e-05\n",
      "Scheduler step: 38000\n",
      "Checkpoint (step: 38000, epoch: 0, block: 38912000) Saved\n",
      "Step Documented\n",
      "09:32:54\n",
      "38001,3.3278\n",
      "38002,3.3463\n",
      "38003,3.3475\n",
      "38004,3.3044\n",
      "38005,3.3177\n",
      "38006,3.3547\n",
      "38007,3.3505\n",
      "38008,3.3462\n",
      "38009,3.3447\n",
      "38010,3.3308\n",
      "38011,3.3433\n",
      "38012,3.3455\n",
      "38013,3.3337\n",
      "38014,3.3240\n",
      "38015,3.3247\n",
      "38016,3.3588\n",
      "38017,3.3333\n",
      "38018,3.3509\n",
      "38019,3.3233\n",
      "38020,3.3295\n",
      "38021,3.3221\n",
      "38022,3.3276\n",
      "38023,3.3309\n",
      "38024,3.3226\n",
      "38025,3.3059\n",
      "38026,3.3350\n",
      "38027,3.3386\n",
      "38028,3.3340\n",
      "38029,3.3440\n",
      "38030,3.3763\n",
      "38031,3.3052\n",
      "38032,3.3490\n",
      "38033,3.3355\n",
      "38034,3.3175\n",
      "38035,3.3290\n",
      "38036,3.3696\n",
      "38037,3.3507\n",
      "38038,3.3425\n",
      "38039,3.3177\n",
      "38040,3.3827\n",
      "38041,3.3423\n",
      "38042,3.3485\n",
      "38043,3.3529\n",
      "38044,3.3295\n",
      "38045,3.3318\n",
      "38046,3.3420\n",
      "38047,3.3511\n",
      "38048,3.3390\n",
      "38049,3.3540\n",
      "38050,3.3479\n",
      "38051,3.3550\n",
      "38052,3.3757\n",
      "38053,3.3425\n",
      "38054,3.3661\n",
      "38055,3.3331\n",
      "38056,3.3530\n",
      "38057,3.3127\n",
      "38058,3.3361\n",
      "38059,3.3488\n",
      "38060,3.3452\n",
      "38061,3.3682\n",
      "38062,3.3489\n",
      "38063,3.3517\n",
      "38064,3.3136\n",
      "38065,3.3468\n",
      "38066,3.3587\n",
      "38067,3.3228\n",
      "38068,3.3534\n",
      "38069,3.3430\n",
      "38070,3.3340\n",
      "38071,3.3491\n",
      "38072,3.3475\n",
      "38073,3.3046\n",
      "38074,3.3305\n",
      "38075,3.3314\n",
      "38076,3.3399\n",
      "38077,3.3441\n",
      "38078,3.3163\n",
      "38079,3.3453\n",
      "38080,3.3420\n",
      "38081,3.3328\n",
      "38082,3.3474\n",
      "38083,3.3391\n",
      "38084,3.3198\n",
      "38085,3.3334\n",
      "38086,3.3563\n",
      "38087,3.3639\n",
      "38088,3.3479\n",
      "38089,3.3280\n",
      "38090,3.3294\n",
      "38091,3.3332\n",
      "38092,3.3246\n",
      "38093,3.3365\n",
      "38094,3.3349\n",
      "38095,3.3201\n",
      "38096,3.3417\n",
      "38097,3.3375\n",
      "38098,3.3591\n",
      "38099,3.3460\n",
      "38100,3.3633\n",
      "(step: 38100, epoch: 0, block: 39014400), Train Loss: 3.2613, Val Loss: 3.2878\n",
      "Learning rate: 3.84724559e-05\n",
      "Scheduler step: 38100\n",
      "Checkpoint (step: 38100, epoch: 0, block: 39014400) Saved\n",
      "Step Documented\n",
      "09:59:11\n",
      "38101,3.3704\n",
      "38102,3.3215\n",
      "38103,3.3287\n",
      "38104,3.3656\n",
      "38105,3.3603\n",
      "38106,3.3243\n",
      "38107,3.3660\n",
      "38108,3.3606\n",
      "38109,3.3253\n",
      "38110,3.3331\n",
      "38111,3.3230\n",
      "38112,3.3395\n",
      "38113,3.3369\n",
      "38114,3.3495\n",
      "38115,3.3556\n",
      "38116,3.3148\n",
      "38117,3.3297\n",
      "38118,3.3797\n",
      "38119,3.3523\n",
      "38120,3.3286\n",
      "38121,3.3586\n",
      "38122,3.3531\n",
      "38123,3.3414\n",
      "38124,3.3459\n",
      "38125,3.3464\n",
      "38126,3.3545\n",
      "38127,3.3641\n",
      "38128,3.3404\n",
      "38129,3.3534\n",
      "38130,3.3475\n",
      "38131,3.3651\n",
      "38132,3.3434\n",
      "38133,3.3477\n",
      "38134,3.3526\n",
      "38135,3.3498\n",
      "38136,3.3685\n",
      "38137,3.3703\n",
      "38138,3.3636\n",
      "38139,3.3425\n",
      "38140,3.3676\n",
      "38141,3.3555\n",
      "38142,3.3429\n",
      "38143,3.3347\n",
      "38144,3.3346\n",
      "38145,3.3431\n",
      "38146,3.3424\n",
      "38147,3.3700\n",
      "38148,3.3714\n",
      "38149,3.3350\n",
      "38150,3.3431\n",
      "38151,3.3256\n",
      "38152,3.3439\n",
      "38153,3.3459\n",
      "38154,3.3260\n",
      "38155,3.3342\n",
      "38156,3.3644\n",
      "38157,3.3601\n",
      "38158,3.3189\n",
      "38159,3.3224\n",
      "38160,3.3687\n",
      "38161,3.3459\n",
      "38162,3.3429\n",
      "38163,3.3472\n",
      "38164,3.3573\n",
      "38165,3.3435\n",
      "38166,3.3093\n",
      "38167,3.3739\n",
      "38168,3.3403\n",
      "38169,3.3783\n",
      "38170,3.3526\n",
      "38171,3.3137\n",
      "38172,3.3721\n",
      "38173,3.3399\n",
      "38174,3.3013\n",
      "38175,3.3453\n",
      "38176,3.3475\n",
      "38177,3.3561\n",
      "38178,3.3343\n",
      "38179,3.3550\n",
      "38180,3.3420\n",
      "38181,3.3285\n",
      "38182,3.3308\n",
      "38183,3.3242\n",
      "38184,3.3463\n",
      "38185,3.3533\n",
      "38186,3.3236\n",
      "38187,3.3373\n",
      "38188,3.3219\n",
      "38189,3.3287\n",
      "38190,3.3602\n",
      "38191,3.3094\n",
      "38192,3.3420\n",
      "38193,3.3609\n",
      "38194,3.3570\n",
      "38195,3.3366\n",
      "38196,3.3500\n",
      "38197,3.3119\n",
      "38198,3.3405\n",
      "38199,3.3327\n",
      "38200,3.3401\n",
      "(step: 38200, epoch: 0, block: 39116800), Train Loss: 3.2768, Val Loss: 3.2663\n",
      "Learning rate: 3.82550031e-05\n",
      "Scheduler step: 38200\n",
      "Checkpoint (step: 38200, epoch: 0, block: 39116800) Saved\n",
      "Step Documented\n",
      "10:25:30\n",
      "38201,3.3451\n",
      "38202,3.2873\n",
      "38203,3.3473\n",
      "38204,3.3395\n",
      "38205,3.3558\n",
      "38206,3.3368\n",
      "38207,3.3469\n",
      "38208,3.3344\n",
      "38209,3.3394\n",
      "38210,3.3107\n",
      "38211,3.3461\n",
      "38212,3.3488\n",
      "38213,3.3224\n",
      "38214,3.3597\n",
      "38215,3.3436\n",
      "38216,3.3304\n",
      "38217,3.3156\n",
      "38218,3.3664\n",
      "38219,3.3370\n",
      "38220,3.3264\n",
      "38221,3.3288\n",
      "38222,3.3598\n",
      "38223,3.3505\n",
      "38224,3.3735\n",
      "38225,3.3306\n",
      "38226,3.3358\n",
      "38227,3.3669\n",
      "38228,3.3511\n",
      "38229,3.3429\n",
      "38230,3.3608\n",
      "38231,3.3600\n",
      "38232,3.3464\n",
      "38233,3.3562\n",
      "38234,3.3290\n",
      "38235,3.3574\n",
      "38236,3.3232\n",
      "38237,3.3575\n",
      "38238,3.3653\n",
      "38239,3.3254\n",
      "38240,3.3407\n",
      "38241,3.3588\n",
      "38242,3.3088\n",
      "38243,3.3122\n",
      "38244,3.3304\n",
      "38245,3.3594\n",
      "38246,3.3398\n",
      "38247,3.3337\n",
      "38248,3.3332\n",
      "38249,3.3549\n",
      "38250,3.3499\n",
      "38251,3.3416\n",
      "38252,3.3583\n",
      "38253,3.3571\n",
      "38254,3.3430\n",
      "38255,3.3397\n",
      "38256,3.3425\n",
      "38257,3.3436\n",
      "38258,3.3513\n",
      "38259,3.3604\n",
      "38260,3.3200\n",
      "38261,3.3506\n",
      "38262,3.3534\n",
      "38263,3.3436\n",
      "38264,3.3536\n",
      "38265,3.3485\n",
      "38266,3.3191\n",
      "38267,3.3609\n",
      "38268,3.3323\n",
      "38269,3.3442\n",
      "38270,3.3244\n",
      "38271,3.3789\n",
      "38272,3.3504\n",
      "38273,3.3421\n",
      "38274,3.3032\n",
      "38275,3.3514\n",
      "38276,3.3573\n",
      "38277,3.3392\n",
      "38278,3.3658\n",
      "38279,3.3574\n",
      "38280,3.3469\n",
      "38281,3.3470\n",
      "38282,3.3855\n",
      "38283,3.3671\n",
      "38284,3.3555\n",
      "38285,3.3275\n",
      "38286,3.3586\n",
      "38287,3.3541\n",
      "38288,3.3489\n",
      "38289,3.3453\n",
      "38290,3.3564\n",
      "38291,3.3257\n",
      "38292,3.3271\n",
      "38293,3.3356\n",
      "38294,3.3601\n",
      "38295,3.3356\n",
      "38296,3.3368\n",
      "38297,3.3344\n",
      "38298,3.3444\n",
      "38299,3.3527\n",
      "38300,3.3267\n",
      "(step: 38300, epoch: 0, block: 39219200), Train Loss: 3.2637, Val Loss: 3.2915\n",
      "Learning rate: 3.80379481e-05\n",
      "Scheduler step: 38300\n",
      "Checkpoint (step: 38300, epoch: 0, block: 39219200) Saved\n",
      "Step Documented\n",
      "10:51:48\n",
      "38301,3.3527\n",
      "38302,3.3755\n",
      "38303,3.3305\n",
      "38304,3.3640\n",
      "38305,3.3366\n",
      "38306,3.3507\n",
      "38307,3.3299\n",
      "38308,3.3418\n",
      "38309,3.3383\n",
      "38310,3.3301\n",
      "38311,3.3482\n",
      "38312,3.3436\n",
      "38313,3.3581\n",
      "38314,3.3602\n",
      "38315,3.3466\n",
      "38316,3.3235\n",
      "38317,3.3643\n",
      "38318,3.3498\n",
      "38319,3.3731\n",
      "38320,3.3575\n",
      "38321,3.3211\n",
      "38322,3.3354\n",
      "38323,3.3326\n",
      "38324,3.3585\n",
      "38325,3.3380\n",
      "38326,3.3684\n",
      "38327,3.3514\n",
      "38328,3.3447\n",
      "38329,3.3172\n",
      "38330,3.3514\n",
      "38331,3.3400\n",
      "38332,3.3462\n",
      "38333,3.3074\n",
      "38334,3.3325\n",
      "38335,3.3627\n",
      "38336,3.3495\n",
      "38337,3.3464\n",
      "38338,3.3537\n",
      "38339,3.3420\n",
      "38340,3.3266\n",
      "38341,3.3448\n",
      "38342,3.3562\n",
      "38343,3.3623\n",
      "38344,3.3498\n",
      "38345,3.3419\n",
      "38346,3.3465\n",
      "38347,3.3357\n",
      "38348,3.3164\n",
      "38349,3.3462\n",
      "38350,3.3377\n",
      "38351,3.3621\n",
      "38352,3.3465\n",
      "38353,3.3500\n",
      "38354,3.3691\n",
      "38355,3.3561\n",
      "38356,3.3352\n",
      "38357,3.3403\n",
      "38358,3.3533\n",
      "38359,3.3494\n",
      "38360,3.3390\n",
      "38361,3.3458\n",
      "38362,3.3379\n",
      "38363,3.3715\n",
      "38364,3.3575\n",
      "38365,3.3082\n",
      "38366,3.3577\n",
      "38367,3.3672\n",
      "38368,3.3376\n",
      "38369,3.3532\n",
      "38370,3.3416\n",
      "38371,3.3840\n",
      "38372,3.3569\n",
      "38373,3.3274\n",
      "38374,3.3837\n",
      "38375,3.3265\n",
      "38376,3.3623\n",
      "38377,3.3268\n",
      "38378,3.3647\n",
      "38379,3.3528\n",
      "38380,3.3549\n",
      "38381,3.3444\n",
      "38382,3.3396\n",
      "38383,3.3613\n",
      "38384,3.3072\n",
      "38385,3.3366\n",
      "38386,3.3452\n",
      "38387,3.3510\n",
      "38388,3.3620\n",
      "38389,3.3747\n",
      "38390,3.3423\n",
      "38391,3.3283\n",
      "38392,3.3562\n",
      "38393,3.3393\n",
      "38394,3.3313\n",
      "38395,3.3449\n",
      "38396,3.3338\n",
      "38397,3.3418\n",
      "38398,3.3356\n",
      "38399,3.3404\n",
      "38400,3.3382\n",
      "(step: 38400, epoch: 0, block: 39321600), Train Loss: 3.2743, Val Loss: 3.2576\n",
      "Learning rate: 3.78212965e-05\n",
      "Scheduler step: 38400\n",
      "Checkpoint (step: 38400, epoch: 0, block: 39321600) Saved\n",
      "Step Documented\n",
      "11:18:06\n",
      "38401,3.3318\n",
      "38402,3.3462\n",
      "38403,3.3174\n",
      "38404,3.3545\n",
      "38405,3.3462\n",
      "38406,3.3446\n",
      "38407,3.3389\n",
      "38408,3.3350\n",
      "38409,3.3588\n",
      "38410,3.3417\n",
      "38411,3.3391\n",
      "38412,3.3357\n",
      "38413,3.3460\n",
      "38414,3.3389\n",
      "38415,3.3208\n",
      "38416,3.3445\n",
      "38417,3.3459\n",
      "38418,3.3379\n",
      "38419,3.3441\n",
      "38420,3.3296\n",
      "38421,3.3434\n",
      "38422,3.3341\n",
      "38423,3.3552\n",
      "38424,3.3637\n",
      "38425,3.3260\n",
      "38426,3.3415\n",
      "38427,3.3228\n",
      "38428,3.3476\n",
      "38429,3.3212\n",
      "38430,3.3343\n",
      "38431,3.3402\n",
      "38432,3.3465\n",
      "38433,3.3644\n",
      "38434,3.3405\n",
      "38435,3.3468\n",
      "38436,3.3365\n",
      "38437,3.3133\n",
      "38438,3.3217\n",
      "38439,3.3100\n",
      "38440,3.3313\n",
      "38441,3.3244\n",
      "38442,3.3280\n",
      "38443,3.3243\n",
      "38444,3.3514\n",
      "38445,3.3460\n",
      "38446,3.3752\n",
      "38447,3.3498\n",
      "38448,3.3240\n",
      "38449,3.3382\n",
      "38450,3.3547\n",
      "38451,3.3508\n",
      "38452,3.3491\n",
      "38453,3.3625\n",
      "38454,3.3060\n",
      "38455,3.3273\n",
      "38456,3.3391\n",
      "38457,3.3707\n",
      "38458,3.3491\n",
      "38459,3.3393\n",
      "38460,3.3367\n",
      "38461,3.3580\n",
      "38462,3.3724\n",
      "38463,3.3509\n",
      "38464,3.3635\n",
      "38465,3.3548\n",
      "38466,3.3580\n",
      "38467,3.3555\n",
      "38468,3.3521\n",
      "38469,3.3493\n",
      "38470,3.3649\n",
      "38471,3.3446\n",
      "38472,3.3423\n",
      "38473,3.3239\n",
      "38474,3.3324\n",
      "38475,3.3374\n",
      "38476,3.3378\n",
      "38477,3.3065\n",
      "38478,3.3497\n",
      "38479,3.3437\n",
      "38480,3.3387\n",
      "38481,3.3331\n",
      "38482,3.3340\n",
      "38483,3.3606\n",
      "38484,3.3362\n",
      "38485,3.3464\n",
      "38486,3.3134\n",
      "38487,3.3318\n",
      "38488,3.3620\n",
      "38489,3.3406\n",
      "38490,3.3481\n",
      "38491,3.3520\n",
      "38492,3.3321\n",
      "38493,3.3180\n",
      "38494,3.3243\n",
      "38495,3.3639\n",
      "38496,3.3610\n",
      "38497,3.3494\n",
      "38498,3.3605\n",
      "38499,3.3156\n",
      "38500,3.3404\n",
      "(step: 38500, epoch: 0, block: 39424000), Train Loss: 3.2441, Val Loss: 3.2650\n",
      "Learning rate: 3.76050536e-05\n",
      "Scheduler step: 38500\n",
      "Checkpoint (step: 38500, epoch: 0, block: 39424000) Saved\n",
      "Step Documented\n",
      "11:44:25\n",
      "38501,3.3309\n",
      "38502,3.3634\n",
      "38503,3.3507\n",
      "38504,3.3694\n",
      "38505,3.3621\n",
      "38506,3.3576\n",
      "38507,3.3413\n",
      "38508,3.3671\n",
      "38509,3.3347\n",
      "38510,3.3616\n",
      "38511,3.3135\n",
      "38512,3.3230\n",
      "38513,3.3252\n",
      "38514,3.3380\n",
      "38515,3.3601\n",
      "38516,3.3630\n",
      "38517,3.3413\n",
      "38518,3.3486\n",
      "38519,3.3252\n",
      "38520,3.3458\n",
      "38521,3.3287\n",
      "38522,3.3323\n",
      "38523,3.3554\n",
      "38524,3.3315\n",
      "38525,3.3333\n",
      "38526,3.3353\n",
      "38527,3.3562\n",
      "38528,3.3471\n",
      "38529,3.3652\n",
      "38530,3.3472\n",
      "38531,3.3154\n",
      "38532,3.3342\n",
      "38533,3.3487\n",
      "38534,3.3208\n",
      "38535,3.3317\n",
      "38536,3.3535\n",
      "38537,3.3488\n",
      "38538,3.3634\n",
      "38539,3.3306\n",
      "38540,3.3402\n",
      "38541,3.3634\n",
      "38542,3.3566\n",
      "38543,3.3558\n",
      "38544,3.3395\n",
      "38545,3.3354\n",
      "38546,3.3332\n",
      "38547,3.3355\n",
      "38548,3.3611\n",
      "38549,3.3505\n",
      "38550,3.3532\n",
      "38551,3.3316\n",
      "38552,3.3601\n",
      "38553,3.3004\n",
      "38554,3.3555\n",
      "38555,3.3554\n",
      "38556,3.3533\n",
      "38557,3.3724\n",
      "38558,3.3477\n",
      "38559,3.3184\n",
      "38560,3.3402\n",
      "38561,3.3331\n",
      "38562,3.3672\n",
      "38563,3.3531\n",
      "38564,3.3620\n",
      "38565,3.3473\n",
      "38566,3.3449\n",
      "38567,3.3556\n",
      "38568,3.3501\n",
      "38569,3.3449\n",
      "38570,3.3182\n",
      "38571,3.3585\n",
      "38572,3.3558\n",
      "38573,3.3498\n",
      "38574,3.3170\n",
      "38575,3.3437\n",
      "38576,3.3240\n",
      "38577,3.3257\n",
      "38578,3.3542\n",
      "38579,3.3274\n",
      "38580,3.3319\n",
      "38581,3.3210\n",
      "38582,3.3594\n",
      "38583,3.3509\n",
      "38584,3.3444\n",
      "38585,3.3433\n",
      "38586,3.3121\n",
      "38587,3.3497\n",
      "38588,3.3473\n",
      "38589,3.3467\n",
      "38590,3.3030\n",
      "38591,3.3422\n",
      "38592,3.3205\n",
      "38593,3.3546\n",
      "38594,3.3376\n",
      "38595,3.3534\n",
      "38596,3.3317\n",
      "38597,3.3829\n",
      "38598,3.3396\n",
      "38599,3.3648\n",
      "38600,3.3250\n",
      "(step: 38600, epoch: 0, block: 39526400), Train Loss: 3.2677, Val Loss: 3.3104\n",
      "Learning rate: 3.73892249e-05\n",
      "Scheduler step: 38600\n",
      "Checkpoint (step: 38600, epoch: 0, block: 39526400) Saved\n",
      "Step Documented\n",
      "12:10:43\n",
      "38601,3.3646\n",
      "38602,3.3531\n",
      "38603,3.3435\n",
      "38604,3.3462\n",
      "38605,3.3426\n",
      "38606,3.3286\n",
      "38607,3.3466\n",
      "38608,3.3410\n",
      "38609,3.3706\n",
      "38610,3.3499\n",
      "38611,3.3519\n",
      "38612,3.3502\n",
      "38613,3.3215\n",
      "38614,3.3195\n",
      "38615,3.3362\n",
      "38616,3.3274\n",
      "38617,3.3581\n",
      "38618,3.3315\n",
      "38619,3.3268\n",
      "38620,3.3922\n",
      "38621,3.3443\n",
      "38622,3.3298\n",
      "38623,3.3562\n",
      "38624,3.3588\n",
      "38625,3.2939\n",
      "38626,3.3460\n",
      "38627,3.3394\n",
      "38628,3.3383\n",
      "38629,3.3437\n",
      "38630,3.3335\n",
      "38631,3.3448\n",
      "38632,3.3438\n",
      "38633,3.3611\n",
      "38634,3.3255\n",
      "38635,3.3359\n",
      "38636,3.2977\n",
      "38637,3.3660\n",
      "38638,3.3425\n",
      "38639,3.3544\n",
      "38640,3.3571\n",
      "38641,3.3503\n",
      "38642,3.3439\n",
      "38643,3.3481\n",
      "38644,3.3352\n",
      "38645,3.3466\n",
      "38646,3.3312\n",
      "38647,3.3309\n",
      "38648,3.3241\n",
      "38649,3.3085\n",
      "38650,3.3516\n",
      "38651,3.3331\n",
      "38652,3.3275\n",
      "38653,3.3516\n",
      "38654,3.3272\n",
      "38655,3.3656\n",
      "38656,3.3471\n",
      "38657,3.3459\n",
      "38658,3.3400\n",
      "38659,3.3368\n",
      "38660,3.3607\n",
      "38661,3.3578\n",
      "38662,3.3366\n",
      "38663,3.3267\n",
      "38664,3.3415\n",
      "38665,3.3366\n",
      "38666,3.3355\n",
      "38667,3.3349\n",
      "38668,3.3626\n",
      "38669,3.3540\n",
      "38670,3.3517\n",
      "38671,3.3533\n",
      "38672,3.3551\n",
      "38673,3.3529\n",
      "38674,3.3405\n",
      "38675,3.3069\n",
      "38676,3.3400\n",
      "38677,3.3398\n",
      "38678,3.3529\n",
      "38679,3.3693\n",
      "38680,3.3336\n",
      "38681,3.3570\n",
      "38682,3.3504\n",
      "38683,3.3356\n",
      "38684,3.3403\n",
      "38685,3.3412\n",
      "38686,3.3417\n",
      "38687,3.3752\n",
      "38688,3.3417\n",
      "38689,3.3629\n",
      "38690,3.3365\n",
      "38691,3.3281\n",
      "38692,3.3347\n",
      "38693,3.3628\n",
      "38694,3.3672\n",
      "38695,3.3352\n",
      "38696,3.3487\n",
      "38697,3.3094\n",
      "38698,3.3449\n",
      "38699,3.3428\n",
      "38700,3.3364\n",
      "(step: 38700, epoch: 0, block: 39628800), Train Loss: 3.2541, Val Loss: 3.2755\n",
      "Learning rate: 3.71738160e-05\n",
      "Scheduler step: 38700\n",
      "Checkpoint (step: 38700, epoch: 0, block: 39628800) Saved\n",
      "Step Documented\n",
      "12:37:02\n",
      "38701,3.3410\n",
      "38702,3.3327\n",
      "38703,3.3308\n",
      "38704,3.3542\n",
      "38705,3.3422\n",
      "38706,3.3743\n",
      "38707,3.3321\n",
      "38708,3.3755\n",
      "38709,3.3347\n",
      "38710,3.3469\n",
      "38711,3.3423\n",
      "38712,3.3317\n",
      "38713,3.3472\n",
      "38714,3.3360\n",
      "38715,3.3720\n",
      "38716,3.3086\n",
      "38717,3.3489\n",
      "38718,3.3368\n",
      "38719,3.3292\n",
      "38720,3.3411\n",
      "38721,3.3211\n",
      "38722,3.3349\n",
      "38723,3.3307\n",
      "38724,3.3180\n",
      "38725,3.3627\n",
      "38726,3.3646\n",
      "38727,3.3327\n",
      "38728,3.3703\n",
      "38729,3.3564\n",
      "38730,3.3327\n",
      "38731,3.3600\n",
      "38732,3.3352\n",
      "38733,3.3526\n",
      "38734,3.3399\n",
      "38735,3.3483\n",
      "38736,3.3448\n",
      "38737,3.3225\n",
      "38738,3.3390\n",
      "38739,3.3252\n",
      "38740,3.3332\n",
      "38741,3.3239\n",
      "38742,3.3222\n",
      "38743,3.3288\n",
      "38744,3.3210\n",
      "38745,3.3227\n",
      "38746,3.3625\n",
      "38747,3.3624\n",
      "38748,3.3347\n",
      "38749,3.3384\n",
      "38750,3.3421\n",
      "38751,3.3102\n",
      "38752,3.3130\n",
      "38753,3.3455\n",
      "38754,3.3476\n",
      "38755,3.3241\n",
      "38756,3.3682\n",
      "38757,3.3456\n",
      "38758,3.3208\n",
      "38759,3.3190\n",
      "38760,3.3372\n",
      "38761,3.3568\n",
      "38762,3.3396\n",
      "38763,3.3606\n",
      "38764,3.3385\n",
      "38765,3.3254\n",
      "38766,3.3248\n",
      "38767,3.3228\n",
      "38768,3.3582\n",
      "38769,3.3182\n",
      "38770,3.3114\n",
      "38771,3.3389\n",
      "38772,3.3527\n",
      "38773,3.3311\n",
      "38774,3.3341\n",
      "38775,3.3517\n",
      "38776,3.3240\n",
      "38777,3.3520\n",
      "38778,3.3402\n",
      "38779,3.3614\n",
      "38780,3.3230\n",
      "38781,3.3430\n",
      "38782,3.3578\n",
      "38783,3.3386\n",
      "38784,3.3652\n",
      "38785,3.3363\n",
      "38786,3.3348\n",
      "38787,3.3230\n",
      "38788,3.3383\n",
      "38789,3.3462\n",
      "38790,3.3343\n",
      "38791,3.3616\n",
      "38792,3.3392\n",
      "38793,3.3433\n",
      "38794,3.3471\n",
      "38795,3.3720\n",
      "38796,3.3279\n",
      "38797,3.3436\n",
      "38798,3.3557\n",
      "38799,3.3331\n",
      "38800,3.3404\n",
      "(step: 38800, epoch: 0, block: 39731200), Train Loss: 3.2604, Val Loss: 3.2889\n",
      "Learning rate: 3.69588321e-05\n",
      "Scheduler step: 38800\n",
      "Checkpoint (step: 38800, epoch: 0, block: 39731200) Saved\n",
      "Step Documented\n",
      "13:03:22\n",
      "38801,3.3744\n",
      "38802,3.3636\n",
      "38803,3.3413\n",
      "38804,3.3474\n",
      "38805,3.3558\n",
      "38806,3.3823\n",
      "38807,3.3403\n",
      "38808,3.3522\n",
      "38809,3.3608\n",
      "38810,3.3441\n",
      "38811,3.3148\n",
      "38812,3.3452\n",
      "38813,3.3688\n",
      "38814,3.3182\n",
      "38815,3.3560\n",
      "38816,3.3550\n",
      "38817,3.3398\n",
      "38818,3.3263\n",
      "38819,3.3364\n",
      "38820,3.3125\n",
      "38821,3.3252\n",
      "38822,3.3464\n",
      "38823,3.3409\n",
      "38824,3.3240\n",
      "38825,3.3565\n",
      "38826,3.3536\n",
      "38827,3.3414\n",
      "38828,3.3247\n",
      "38829,3.3323\n",
      "38830,3.3486\n",
      "38831,3.3139\n",
      "38832,3.3311\n",
      "38833,3.3483\n",
      "38834,3.3520\n",
      "38835,3.3309\n",
      "38836,3.3327\n",
      "38837,3.3276\n",
      "38838,3.3359\n",
      "38839,3.3360\n",
      "38840,3.3572\n",
      "38841,3.3256\n",
      "38842,3.3272\n",
      "38843,3.3408\n",
      "38844,3.3469\n",
      "38845,3.3263\n",
      "38846,3.3388\n",
      "38847,3.3515\n",
      "38848,3.3459\n",
      "38849,3.3184\n",
      "38850,3.3188\n",
      "38851,3.3513\n",
      "38852,3.3304\n",
      "38853,3.3753\n",
      "38854,3.3325\n",
      "38855,3.3126\n",
      "38856,3.3618\n",
      "38857,3.3471\n",
      "38858,3.3406\n",
      "38859,3.3345\n",
      "38860,3.3334\n",
      "38861,3.3533\n",
      "38862,3.3359\n",
      "38863,3.3173\n",
      "38864,3.3476\n",
      "38865,3.3096\n",
      "38866,3.3484\n",
      "38867,3.3746\n",
      "38868,3.3573\n",
      "38869,3.3301\n",
      "38870,3.3353\n",
      "38871,3.3374\n",
      "38872,3.3460\n",
      "38873,3.3279\n",
      "38874,3.3489\n",
      "38875,3.3142\n",
      "38876,3.3517\n",
      "38877,3.3343\n",
      "38878,3.3230\n",
      "38879,3.3252\n",
      "38880,3.3379\n",
      "38881,3.3469\n",
      "38882,3.3369\n",
      "38883,3.3688\n",
      "38884,3.3636\n",
      "38885,3.3782\n",
      "38886,3.3489\n",
      "38887,3.3595\n",
      "38888,3.3220\n",
      "38889,3.3258\n",
      "38890,3.3603\n",
      "38891,3.3319\n",
      "38892,3.3512\n",
      "38893,3.3629\n",
      "38894,3.3244\n",
      "38895,3.3602\n",
      "38896,3.3525\n",
      "38897,3.3293\n",
      "38898,3.3390\n",
      "38899,3.3287\n",
      "38900,3.3246\n",
      "(step: 38900, epoch: 0, block: 39833600), Train Loss: 3.2534, Val Loss: 3.2988\n",
      "Learning rate: 3.67442789e-05\n",
      "Scheduler step: 38900\n",
      "Checkpoint (step: 38900, epoch: 0, block: 39833600) Saved\n",
      "Step Documented\n",
      "13:29:41\n",
      "38901,3.3165\n",
      "38902,3.3261\n",
      "38903,3.3361\n",
      "38904,3.3293\n",
      "38905,3.3407\n",
      "38906,3.3632\n",
      "38907,3.3609\n",
      "38908,3.3749\n",
      "38909,3.3376\n",
      "38910,3.3460\n",
      "38911,3.3355\n",
      "38912,3.3523\n",
      "38913,3.3523\n",
      "38914,3.3324\n",
      "38915,3.3082\n",
      "38916,3.3261\n",
      "38917,3.3382\n",
      "38918,3.3529\n",
      "38919,3.3355\n",
      "38920,3.3392\n",
      "38921,3.3398\n",
      "38922,3.3093\n",
      "38923,3.3380\n",
      "38924,3.3361\n",
      "38925,3.3518\n",
      "38926,3.3186\n",
      "38927,3.3575\n",
      "38928,3.3586\n",
      "38929,3.3207\n",
      "38930,3.3334\n",
      "38931,3.3443\n",
      "38932,3.3588\n",
      "38933,3.3463\n",
      "38934,3.3319\n",
      "38935,3.3590\n",
      "38936,3.3248\n",
      "38937,3.3484\n",
      "38938,3.3425\n",
      "38939,3.3531\n",
      "38940,3.3650\n",
      "38941,3.3775\n",
      "38942,3.3473\n",
      "38943,3.3829\n",
      "38944,3.3644\n",
      "38945,3.3700\n",
      "38946,3.3264\n",
      "38947,3.3548\n",
      "38948,3.3571\n",
      "38949,3.3718\n",
      "38950,3.3342\n",
      "38951,3.3438\n",
      "38952,3.3510\n",
      "38953,3.3260\n",
      "38954,3.3489\n",
      "38955,3.3252\n",
      "38956,3.3652\n",
      "38957,3.3591\n",
      "38958,3.3216\n",
      "38959,3.3600\n",
      "38960,3.3124\n",
      "38961,3.3427\n",
      "38962,3.3369\n",
      "38963,3.3390\n",
      "38964,3.3447\n",
      "38965,3.3356\n",
      "38966,3.3566\n",
      "38967,3.3639\n",
      "38968,3.3140\n",
      "38969,3.3042\n",
      "38970,3.3514\n",
      "38971,3.3309\n",
      "38972,3.3638\n",
      "38973,3.3542\n",
      "38974,3.3583\n",
      "38975,3.3497\n",
      "38976,3.3290\n",
      "38977,3.3545\n",
      "38978,3.3335\n",
      "38979,3.3510\n",
      "38980,3.3484\n",
      "38981,3.3631\n",
      "38982,3.3222\n",
      "38983,3.3416\n",
      "38984,3.3693\n",
      "38985,3.3395\n",
      "38986,3.3372\n",
      "38987,3.3307\n",
      "38988,3.3377\n",
      "38989,3.3357\n",
      "38990,3.3212\n",
      "38991,3.3338\n",
      "38992,3.3328\n",
      "38993,3.3391\n",
      "38994,3.3374\n",
      "38995,3.3459\n",
      "38996,3.3590\n",
      "38997,3.3369\n",
      "38998,3.3168\n",
      "38999,3.3627\n",
      "39000,3.3701\n",
      "(step: 39000, epoch: 0, block: 39936000), Train Loss: 3.2846, Val Loss: 3.3055\n",
      "Learning rate: 3.65301616e-05\n",
      "Scheduler step: 39000\n",
      "Checkpoint (step: 39000, epoch: 0, block: 39936000) Saved\n",
      "Step Documented\n",
      "13:56:00\n",
      "39001,3.3370\n",
      "39002,3.3264\n",
      "39003,3.3530\n",
      "39004,3.3289\n",
      "39005,3.3141\n",
      "39006,3.3610\n",
      "39007,3.3386\n",
      "39008,3.3370\n",
      "39009,3.3241\n",
      "39010,3.3333\n",
      "39011,3.3269\n",
      "39012,3.3361\n",
      "39013,3.3324\n",
      "39014,3.3326\n",
      "39015,3.3445\n",
      "39016,3.3389\n",
      "39017,3.3397\n",
      "39018,3.3393\n",
      "39019,3.3321\n",
      "39020,3.3349\n",
      "39021,3.3500\n",
      "39022,3.3353\n",
      "39023,3.3267\n",
      "39024,3.3535\n",
      "39025,3.3209\n",
      "39026,3.3186\n",
      "39027,3.3414\n",
      "39028,3.3292\n",
      "39029,3.3313\n",
      "39030,3.3361\n",
      "39031,3.3449\n",
      "39032,3.3252\n",
      "39033,3.3502\n",
      "39034,3.3407\n",
      "39035,3.3442\n",
      "39036,3.3295\n",
      "39037,3.3338\n",
      "39038,3.3020\n",
      "39039,3.3556\n",
      "39040,3.3458\n",
      "39041,3.3221\n",
      "39042,3.3133\n",
      "39043,3.3417\n",
      "39044,3.3243\n",
      "39045,3.3591\n",
      "39046,3.3298\n",
      "39047,3.3433\n",
      "39048,3.3192\n",
      "39049,3.3412\n",
      "39050,3.3544\n",
      "39051,3.3523\n",
      "39052,3.3446\n",
      "39053,3.3325\n",
      "39054,3.3650\n",
      "39055,3.3150\n",
      "39056,3.3287\n",
      "39057,3.3380\n",
      "39058,3.3307\n",
      "39059,3.3592\n",
      "39060,3.3605\n",
      "39061,3.3413\n",
      "39062,3.3332\n",
      "39063,3.3135\n",
      "39064,3.3135\n",
      "39065,3.3460\n",
      "39066,3.3542\n",
      "39067,3.3545\n",
      "39068,3.3306\n",
      "39069,3.3143\n",
      "39070,3.3361\n",
      "39071,3.3497\n",
      "39072,3.3435\n",
      "39073,3.3376\n",
      "39074,3.3348\n",
      "39075,3.3498\n",
      "39076,3.3474\n",
      "39077,3.3253\n",
      "39078,3.3418\n",
      "39079,3.3327\n",
      "39080,3.3296\n",
      "39081,3.3109\n",
      "39082,3.3105\n",
      "39083,3.3288\n",
      "39084,3.3317\n",
      "39085,3.3482\n",
      "39086,3.3599\n",
      "39087,3.3496\n",
      "39088,3.3568\n",
      "39089,3.3779\n",
      "39090,3.3349\n",
      "39091,3.3424\n",
      "39092,3.3536\n",
      "39093,3.3312\n",
      "39094,3.3554\n",
      "39095,3.3293\n",
      "39096,3.3278\n",
      "39097,3.3078\n",
      "39098,3.3370\n",
      "39099,3.3576\n",
      "39100,3.3341\n",
      "(step: 39100, epoch: 0, block: 40038400), Train Loss: 3.2510, Val Loss: 3.2896\n",
      "Learning rate: 3.63164857e-05\n",
      "Scheduler step: 39100\n",
      "Checkpoint (step: 39100, epoch: 0, block: 40038400) Saved\n",
      "Step Documented\n",
      "14:22:18\n",
      "39101,3.3389\n",
      "39102,3.3402\n",
      "39103,3.3611\n",
      "39104,3.3449\n",
      "39105,3.3225\n",
      "39106,3.3200\n",
      "39107,3.3649\n",
      "39108,3.3497\n",
      "39109,3.3579\n",
      "39110,3.3387\n",
      "39111,3.3545\n",
      "39112,3.3491\n",
      "39113,3.3363\n",
      "39114,3.3278\n",
      "39115,3.3355\n",
      "39116,3.3316\n",
      "39117,3.3579\n",
      "39118,3.3375\n",
      "39119,3.3285\n",
      "39120,3.3254\n",
      "39121,3.3617\n",
      "39122,3.3318\n",
      "39123,3.3384\n",
      "39124,3.3437\n",
      "39125,3.3400\n",
      "39126,3.3420\n",
      "39127,3.3586\n",
      "39128,3.3528\n",
      "39129,3.3341\n",
      "39130,3.3420\n",
      "39131,3.3530\n",
      "39132,3.3375\n",
      "39133,3.3400\n",
      "39134,3.3247\n",
      "39135,3.3386\n",
      "39136,3.3284\n",
      "39137,3.3195\n",
      "39138,3.3564\n",
      "39139,3.3493\n",
      "39140,3.3598\n",
      "39141,3.3565\n",
      "39142,3.3383\n",
      "39143,3.3670\n",
      "39144,3.3446\n",
      "39145,3.3222\n",
      "39146,3.3445\n",
      "39147,3.3508\n",
      "39148,3.3298\n",
      "39149,3.3140\n",
      "39150,3.3272\n",
      "39151,3.3278\n",
      "39152,3.3585\n",
      "39153,3.3755\n",
      "39154,3.3521\n",
      "39155,3.3258\n",
      "39156,3.3241\n",
      "39157,3.3329\n",
      "39158,3.3534\n",
      "39159,3.3604\n",
      "39160,3.3365\n",
      "39161,3.3306\n",
      "39162,3.3305\n",
      "39163,3.3271\n",
      "39164,3.3287\n",
      "39165,3.3640\n",
      "39166,3.3343\n",
      "39167,3.3399\n",
      "39168,3.3625\n",
      "39169,3.3513\n",
      "39170,3.3291\n",
      "39171,3.3539\n",
      "39172,3.3328\n",
      "39173,3.3167\n",
      "39174,3.3442\n",
      "39175,3.3336\n",
      "39176,3.3149\n",
      "39177,3.3171\n",
      "39178,3.3355\n",
      "39179,3.3304\n",
      "39180,3.3431\n",
      "39181,3.3352\n",
      "39182,3.3427\n",
      "39183,3.3316\n",
      "39184,3.3433\n",
      "39185,3.3467\n",
      "39186,3.3153\n",
      "39187,3.3362\n",
      "39188,3.3595\n",
      "39189,3.3204\n",
      "39190,3.3426\n",
      "39191,3.3685\n",
      "39192,3.3405\n",
      "39193,3.3029\n",
      "39194,3.3177\n",
      "39195,3.3262\n",
      "39196,3.3246\n",
      "39197,3.3258\n",
      "39198,3.3411\n",
      "39199,3.3264\n",
      "39200,3.3545\n",
      "(step: 39200, epoch: 0, block: 40140800), Train Loss: 3.2530, Val Loss: 3.2747\n",
      "Learning rate: 3.61032566e-05\n",
      "Scheduler step: 39200\n",
      "Checkpoint (step: 39200, epoch: 0, block: 40140800) Saved\n",
      "Step Documented\n",
      "14:48:37\n",
      "39201,3.3720\n",
      "39202,3.3772\n",
      "39203,3.3648\n",
      "39204,3.3655\n",
      "39205,3.3816\n",
      "39206,3.3425\n",
      "39207,3.3539\n",
      "39208,3.3515\n",
      "39209,3.3531\n",
      "39210,3.3395\n",
      "39211,3.3372\n",
      "39212,3.3404\n",
      "39213,3.3265\n",
      "39214,3.3351\n",
      "39215,3.3240\n",
      "39216,3.3349\n",
      "39217,3.3358\n",
      "39218,3.3667\n",
      "39219,3.3325\n",
      "39220,3.3291\n",
      "39221,3.3462\n",
      "39222,3.3381\n",
      "39223,3.3332\n",
      "39224,3.3654\n",
      "39225,3.3436\n",
      "39226,3.3339\n",
      "39227,3.3191\n",
      "39228,3.3769\n",
      "39229,3.3417\n",
      "39230,3.3361\n",
      "39231,3.3464\n",
      "39232,3.3719\n",
      "39233,3.3359\n",
      "39234,3.3360\n",
      "39235,3.3343\n",
      "39236,3.3573\n",
      "39237,3.3606\n",
      "39238,3.3453\n",
      "39239,3.3351\n",
      "39240,3.3040\n",
      "39241,3.3342\n",
      "39242,3.3242\n",
      "39243,3.3204\n",
      "39244,3.3104\n",
      "39245,3.3303\n",
      "39246,3.3365\n",
      "39247,3.3469\n",
      "39248,3.3496\n",
      "39249,3.3255\n",
      "39250,3.3489\n",
      "39251,3.3362\n",
      "39252,3.3256\n",
      "39253,3.3324\n",
      "39254,3.3612\n",
      "39255,3.3223\n",
      "39256,3.3454\n",
      "39257,3.3453\n",
      "39258,3.3367\n",
      "39259,3.3378\n",
      "39260,3.3383\n",
      "39261,3.3385\n",
      "39262,3.3294\n",
      "39263,3.3195\n",
      "39264,3.3504\n",
      "39265,3.3230\n",
      "39266,3.3303\n",
      "39267,3.3416\n",
      "39268,3.3395\n",
      "39269,3.3398\n",
      "39270,3.3609\n",
      "39271,3.3448\n",
      "39272,3.3439\n",
      "39273,3.3569\n",
      "39274,3.3496\n",
      "39275,3.3357\n",
      "39276,3.3406\n",
      "39277,3.3646\n",
      "39278,3.3317\n",
      "39279,3.3460\n",
      "39280,3.3426\n",
      "39281,3.3381\n",
      "39282,3.3490\n",
      "39283,3.3608\n",
      "39284,3.3374\n",
      "39285,3.3201\n",
      "39286,3.3519\n",
      "39287,3.3202\n",
      "39288,3.3549\n",
      "39289,3.3576\n",
      "39290,3.3383\n",
      "39291,3.3145\n",
      "39292,3.3275\n",
      "39293,3.3205\n",
      "39294,3.3241\n",
      "39295,3.3188\n",
      "39296,3.3427\n",
      "39297,3.3851\n",
      "39298,3.3268\n",
      "39299,3.3835\n",
      "39300,3.3256\n",
      "(step: 39300, epoch: 0, block: 40243200), Train Loss: 3.2712, Val Loss: 3.2613\n",
      "Learning rate: 3.58904797e-05\n",
      "Scheduler step: 39300\n",
      "Checkpoint (step: 39300, epoch: 0, block: 40243200) Saved\n",
      "Step Documented\n",
      "15:14:56\n",
      "39301,3.3518\n",
      "39302,3.3457\n",
      "39303,3.3286\n",
      "39304,3.3215\n",
      "39305,3.3348\n",
      "39306,3.3198\n",
      "39307,3.3423\n",
      "39308,3.3559\n",
      "39309,3.3417\n",
      "39310,3.3336\n",
      "39311,3.3528\n",
      "39312,3.3591\n",
      "39313,3.3447\n",
      "39314,3.3282\n",
      "39315,3.3251\n",
      "39316,3.3493\n",
      "39317,3.3658\n",
      "39318,3.3617\n",
      "39319,3.3329\n",
      "39320,3.3464\n",
      "39321,3.3668\n",
      "39322,3.3421\n",
      "39323,3.3381\n",
      "39324,3.2938\n",
      "39325,3.3557\n",
      "39326,3.3364\n",
      "39327,3.3365\n",
      "39328,3.3523\n",
      "39329,3.3271\n",
      "39330,3.3497\n",
      "39331,3.3217\n",
      "39332,3.3217\n",
      "39333,3.3185\n",
      "39334,3.3330\n",
      "39335,3.3539\n",
      "39336,3.3383\n",
      "39337,3.3389\n",
      "39338,3.3593\n",
      "39339,3.3649\n",
      "39340,3.3238\n",
      "39341,3.3150\n",
      "39342,3.3240\n",
      "39343,3.3367\n",
      "39344,3.3565\n",
      "39345,3.3393\n",
      "39346,3.3338\n",
      "39347,3.2937\n",
      "39348,3.3282\n",
      "39349,3.3820\n",
      "39350,3.3424\n",
      "39351,3.3131\n",
      "39352,3.3631\n",
      "39353,3.3458\n",
      "39354,3.3630\n",
      "39355,3.3324\n",
      "39356,3.3186\n",
      "39357,3.3551\n",
      "39358,3.3335\n",
      "39359,3.3401\n",
      "39360,3.3292\n",
      "39361,3.3596\n",
      "39362,3.3286\n",
      "39363,3.3152\n",
      "39364,3.3249\n",
      "39365,3.3453\n",
      "39366,3.3434\n",
      "39367,3.3562\n",
      "39368,3.3628\n",
      "39369,3.3314\n",
      "39370,3.3105\n",
      "39371,3.3273\n",
      "39372,3.3265\n",
      "39373,3.3172\n",
      "39374,3.3438\n",
      "39375,3.3509\n",
      "39376,3.3081\n",
      "39377,3.3176\n",
      "39378,3.3259\n",
      "39379,3.3333\n",
      "39380,3.3075\n",
      "39381,3.3474\n",
      "39382,3.3505\n",
      "39383,3.3315\n",
      "39384,3.3436\n",
      "39385,3.3385\n",
      "39386,3.3460\n",
      "39387,3.3250\n",
      "39388,3.3839\n",
      "39389,3.3291\n",
      "39390,3.3429\n",
      "39391,3.3361\n",
      "39392,3.3461\n",
      "39393,3.3394\n",
      "39394,3.3334\n",
      "39395,3.3137\n",
      "39396,3.3206\n",
      "39397,3.3478\n",
      "39398,3.3441\n",
      "39399,3.3317\n",
      "39400,3.3509\n",
      "(step: 39400, epoch: 0, block: 40345600), Train Loss: 3.2581, Val Loss: 3.2790\n",
      "Learning rate: 3.56781604e-05\n",
      "Scheduler step: 39400\n",
      "Checkpoint (step: 39400, epoch: 0, block: 40345600) Saved\n",
      "Step Documented\n",
      "15:41:14\n",
      "39401,3.3423\n",
      "39402,3.3205\n",
      "39403,3.3437\n",
      "39404,3.3226\n",
      "39405,3.3439\n",
      "39406,3.3175\n",
      "39407,3.3597\n",
      "39408,3.3519\n",
      "39409,3.3408\n",
      "39410,3.3573\n",
      "39411,3.3207\n",
      "39412,3.3419\n",
      "39413,3.3309\n",
      "39414,3.3315\n",
      "39415,3.3524\n",
      "39416,3.3504\n",
      "39417,3.3503\n",
      "39418,3.3475\n",
      "39419,3.3652\n",
      "39420,3.3269\n",
      "39421,3.3413\n",
      "39422,3.3177\n",
      "39423,3.3277\n",
      "39424,3.3177\n",
      "39425,3.2847\n",
      "39426,3.3687\n",
      "39427,3.3321\n",
      "39428,3.3253\n",
      "39429,3.3333\n",
      "39430,3.3484\n",
      "39431,3.3349\n",
      "39432,3.3357\n",
      "39433,3.3103\n",
      "39434,3.3215\n",
      "39435,3.3290\n",
      "39436,3.3624\n",
      "39437,3.3480\n",
      "39438,3.3448\n",
      "39439,3.3307\n",
      "39440,3.3367\n",
      "39441,3.3205\n",
      "39442,3.3498\n",
      "39443,3.3274\n",
      "39444,3.3009\n",
      "39445,3.3208\n",
      "39446,3.3086\n",
      "39447,3.3377\n",
      "39448,3.3147\n",
      "39449,3.3434\n",
      "39450,3.3805\n",
      "39451,3.3301\n",
      "39452,3.3433\n",
      "39453,3.3630\n",
      "39454,3.3309\n",
      "39455,3.3513\n",
      "39456,3.3582\n",
      "39457,3.3239\n",
      "39458,3.3427\n",
      "39459,3.3572\n",
      "39460,3.3320\n",
      "39461,3.3130\n",
      "39462,3.3289\n",
      "39463,3.3277\n",
      "39464,3.3347\n",
      "39465,3.3338\n",
      "39466,3.3818\n",
      "39467,3.3412\n",
      "39468,3.3324\n",
      "39469,3.3298\n",
      "39470,3.3252\n",
      "39471,3.3223\n",
      "39472,3.3181\n",
      "39473,3.3062\n",
      "39474,3.3304\n",
      "39475,3.3379\n",
      "39476,3.3355\n",
      "39477,3.3715\n",
      "39478,3.3491\n",
      "39479,3.3454\n",
      "39480,3.3473\n",
      "39481,3.3420\n",
      "39482,3.3580\n",
      "39483,3.3494\n",
      "39484,3.3466\n",
      "39485,3.3320\n",
      "39486,3.3508\n",
      "39487,3.3243\n",
      "39488,3.3412\n",
      "39489,3.3278\n",
      "39490,3.3550\n",
      "39491,3.3340\n",
      "39492,3.3231\n",
      "39493,3.3346\n",
      "39494,3.3413\n",
      "39495,3.3149\n",
      "39496,3.3421\n",
      "39497,3.3511\n",
      "39498,3.3589\n",
      "39499,3.3339\n",
      "39500,3.3097\n",
      "(step: 39500, epoch: 0, block: 40448000), Train Loss: 3.2542, Val Loss: 3.2693\n",
      "Learning rate: 3.54663040e-05\n",
      "Scheduler step: 39500\n",
      "Checkpoint (step: 39500, epoch: 0, block: 40448000) Saved\n",
      "Step Documented\n",
      "16:07:33\n",
      "39501,3.3579\n",
      "39502,3.3469\n",
      "39503,3.3622\n",
      "39504,3.3490\n",
      "39505,3.3579\n",
      "39506,3.3549\n",
      "39507,3.3418\n",
      "39508,3.3746\n",
      "39509,3.3444\n",
      "39510,3.3612\n",
      "39511,3.3152\n",
      "39512,3.3501\n",
      "39513,3.3469\n",
      "39514,3.3374\n",
      "39515,3.3337\n",
      "39516,3.3816\n",
      "39517,3.3421\n",
      "39518,3.3346\n",
      "39519,3.3076\n",
      "39520,3.3304\n",
      "39521,3.3343\n",
      "39522,3.3512\n",
      "39523,3.3313\n",
      "39524,3.3501\n",
      "39525,3.3562\n",
      "39526,3.3242\n",
      "39527,3.3438\n",
      "39528,3.3025\n",
      "39529,3.3457\n",
      "39530,3.3291\n",
      "39531,3.3526\n",
      "39532,3.3445\n",
      "39533,3.3209\n",
      "39534,3.3445\n",
      "39535,3.3576\n",
      "39536,3.3236\n",
      "39537,3.3157\n",
      "39538,3.3265\n",
      "39539,3.3190\n",
      "39540,3.3439\n",
      "39541,3.3447\n",
      "39542,3.3716\n",
      "39543,3.3360\n",
      "39544,3.3441\n",
      "39545,3.3314\n",
      "39546,3.3527\n",
      "39547,3.3469\n",
      "39548,3.3620\n",
      "39549,3.3519\n",
      "39550,3.3477\n",
      "39551,3.3314\n",
      "39552,3.3542\n",
      "39553,3.3262\n",
      "39554,3.3656\n",
      "39555,3.3383\n",
      "39556,3.3267\n",
      "39557,3.3168\n",
      "39558,3.3289\n",
      "39559,3.3314\n",
      "39560,3.3242\n",
      "39561,3.3242\n",
      "39562,3.3093\n",
      "39563,3.3595\n",
      "39564,3.3347\n",
      "39565,3.3346\n",
      "39566,3.3398\n",
      "39567,3.3320\n",
      "39568,3.3514\n",
      "39569,3.3409\n",
      "39570,3.3364\n",
      "39571,3.3540\n",
      "39572,3.3319\n",
      "39573,3.3395\n",
      "39574,3.3391\n",
      "39575,3.3279\n",
      "39576,3.3572\n",
      "39577,3.3384\n",
      "39578,3.3406\n",
      "39579,3.3466\n",
      "39580,3.3291\n",
      "39581,3.3359\n",
      "39582,3.3169\n",
      "39583,3.3456\n",
      "39584,3.3752\n",
      "39585,3.3474\n",
      "39586,3.3362\n",
      "39587,3.3632\n",
      "39588,3.3232\n",
      "39589,3.3533\n",
      "39590,3.3298\n",
      "39591,3.3496\n",
      "39592,3.3323\n",
      "39593,3.3442\n",
      "39594,3.3302\n",
      "39595,3.3563\n",
      "39596,3.3254\n",
      "39597,3.3344\n",
      "39598,3.3494\n",
      "39599,3.3348\n",
      "39600,3.3366\n",
      "(step: 39600, epoch: 0, block: 40550400), Train Loss: 3.2685, Val Loss: 3.2894\n",
      "Learning rate: 3.52549158e-05\n",
      "Scheduler step: 39600\n",
      "Checkpoint (step: 39600, epoch: 0, block: 40550400) Saved\n",
      "Step Documented\n",
      "16:33:51\n",
      "39601,3.3355\n",
      "39602,3.3556\n",
      "39603,3.3330\n",
      "39604,3.3394\n",
      "39605,3.3507\n",
      "39606,3.3515\n",
      "39607,3.3336\n",
      "39608,3.3424\n",
      "39609,3.3201\n",
      "39610,3.3405\n",
      "39611,3.3518\n",
      "39612,3.3379\n",
      "39613,3.3559\n",
      "39614,3.3410\n",
      "39615,3.3259\n",
      "39616,3.3183\n",
      "39617,3.3428\n",
      "39618,3.3443\n",
      "39619,3.3259\n",
      "39620,3.3487\n",
      "39621,3.3368\n",
      "39622,3.3383\n",
      "39623,3.3574\n",
      "39624,3.3392\n",
      "39625,3.3417\n",
      "39626,3.3527\n",
      "39627,3.3555\n",
      "39628,3.3094\n",
      "39629,3.3775\n",
      "39630,3.3374\n",
      "39631,3.3091\n",
      "39632,3.3206\n",
      "39633,3.3231\n",
      "39634,3.3221\n",
      "39635,3.3333\n",
      "39636,3.3394\n",
      "39637,3.3356\n",
      "39638,3.3389\n",
      "39639,3.3701\n",
      "39640,3.3309\n",
      "39641,3.3242\n",
      "39642,3.3292\n",
      "39643,3.3419\n",
      "39644,3.3358\n",
      "39645,3.3431\n",
      "39646,3.3322\n",
      "39647,3.3410\n",
      "39648,3.3247\n",
      "39649,3.3478\n",
      "39650,3.3208\n",
      "39651,3.3218\n",
      "39652,3.2948\n",
      "39653,3.3132\n",
      "39654,3.3376\n",
      "39655,3.3262\n",
      "39656,3.3409\n",
      "39657,3.3665\n",
      "39658,3.3177\n",
      "39659,3.3661\n",
      "39660,3.3346\n",
      "39661,3.3262\n",
      "39662,3.3256\n",
      "39663,3.3543\n",
      "39664,3.3221\n",
      "39665,3.3555\n",
      "39666,3.3625\n",
      "39667,3.3461\n",
      "39668,3.3462\n",
      "39669,3.3255\n",
      "39670,3.3436\n",
      "39671,3.3480\n",
      "39672,3.3294\n",
      "39673,3.3425\n",
      "39674,3.3345\n",
      "39675,3.3388\n",
      "39676,3.3170\n",
      "39677,3.3264\n",
      "39678,3.3172\n",
      "39679,3.3146\n",
      "39680,3.3245\n",
      "39681,3.3189\n",
      "39682,3.3227\n",
      "39683,3.3237\n",
      "39684,3.3498\n",
      "39685,3.3664\n",
      "39686,3.3432\n",
      "39687,3.3551\n",
      "39688,3.3567\n",
      "39689,3.3581\n",
      "39690,3.3550\n",
      "39691,3.3410\n",
      "39692,3.3395\n",
      "39693,3.3319\n",
      "39694,3.3324\n",
      "39695,3.3520\n",
      "39696,3.3812\n",
      "39697,3.3197\n",
      "39698,3.3481\n",
      "39699,3.3514\n",
      "39700,3.3185\n",
      "(step: 39700, epoch: 0, block: 40652800), Train Loss: 3.2676, Val Loss: 3.2636\n",
      "Learning rate: 3.50440013e-05\n",
      "Scheduler step: 39700\n",
      "Checkpoint (step: 39700, epoch: 0, block: 40652800) Saved\n",
      "Step Documented\n",
      "17:00:11\n",
      "39701,3.3453\n",
      "39702,3.3158\n",
      "39703,3.3304\n",
      "39704,3.3289\n",
      "39705,3.3572\n",
      "39706,3.3259\n",
      "39707,3.3386\n",
      "39708,3.3329\n",
      "39709,3.3298\n",
      "39710,3.3325\n",
      "39711,3.3462\n",
      "39712,3.3547\n",
      "39713,3.3151\n",
      "39714,3.3297\n",
      "39715,3.3172\n",
      "39716,3.3396\n",
      "39717,3.3333\n",
      "39718,3.3200\n",
      "39719,3.3377\n",
      "39720,3.3186\n",
      "39721,3.3428\n",
      "39722,3.3299\n",
      "39723,3.3544\n",
      "39724,3.3468\n",
      "39725,3.3510\n",
      "39726,3.3302\n",
      "39727,3.3592\n",
      "39728,3.3079\n",
      "39729,3.3527\n",
      "39730,3.3326\n",
      "39731,3.3440\n",
      "39732,3.3445\n",
      "39733,3.3227\n",
      "39734,3.3514\n",
      "39735,3.3368\n",
      "39736,3.3702\n",
      "39737,3.3441\n",
      "39738,3.3557\n",
      "39739,3.3303\n",
      "39740,3.3351\n",
      "39741,3.3137\n",
      "39742,3.3641\n",
      "39743,3.3396\n",
      "39744,3.3438\n",
      "39745,3.3445\n",
      "39746,3.3309\n",
      "39747,3.3252\n",
      "39748,3.3382\n",
      "39749,3.3000\n",
      "39750,3.3142\n",
      "39751,3.3387\n",
      "39752,3.3484\n",
      "39753,3.3398\n",
      "39754,3.3142\n",
      "39755,3.3450\n",
      "39756,3.3427\n",
      "39757,3.3754\n",
      "39758,3.3235\n",
      "39759,3.3551\n",
      "39760,3.3853\n",
      "39761,3.3182\n",
      "39762,3.3330\n",
      "39763,3.3372\n",
      "39764,3.3432\n",
      "39765,3.3502\n",
      "39766,3.3054\n",
      "39767,3.3396\n",
      "39768,3.3135\n",
      "39769,3.3569\n",
      "39770,3.3216\n",
      "39771,3.3597\n",
      "39772,3.3327\n",
      "39773,3.3242\n",
      "39774,3.3215\n",
      "39775,3.3107\n",
      "39776,3.3501\n",
      "39777,3.3459\n",
      "39778,3.3327\n",
      "39779,3.3507\n",
      "39780,3.3385\n",
      "39781,3.3060\n",
      "39782,3.3219\n",
      "39783,3.3333\n",
      "39784,3.3400\n",
      "39785,3.3327\n",
      "39786,3.3544\n",
      "39787,3.3182\n",
      "39788,3.3266\n",
      "39789,3.3239\n",
      "39790,3.3377\n",
      "39791,3.3210\n",
      "39792,3.3318\n",
      "39793,3.3177\n",
      "39794,3.3499\n",
      "39795,3.3341\n",
      "39796,3.3201\n",
      "39797,3.3361\n",
      "39798,3.3704\n",
      "39799,3.3224\n",
      "39800,3.3484\n",
      "(step: 39800, epoch: 0, block: 40755200), Train Loss: 3.2301, Val Loss: 3.2980\n",
      "Learning rate: 3.48335657e-05\n",
      "Scheduler step: 39800\n",
      "Checkpoint (step: 39800, epoch: 0, block: 40755200) Saved\n",
      "Step Documented\n",
      "17:26:30\n",
      "39801,3.3225\n",
      "39802,3.3232\n",
      "39803,3.3340\n",
      "39804,3.3239\n",
      "39805,3.3614\n",
      "39806,3.3414\n",
      "39807,3.3214\n",
      "39808,3.3585\n",
      "39809,3.3564\n",
      "39810,3.3472\n",
      "39811,3.3560\n",
      "39812,3.3703\n",
      "39813,3.3559\n",
      "39814,3.3176\n",
      "39815,3.2952\n",
      "39816,3.3266\n",
      "39817,3.3342\n",
      "39818,3.3421\n",
      "39819,3.3353\n",
      "39820,3.3282\n",
      "39821,3.3146\n",
      "39822,3.3205\n",
      "39823,3.3412\n",
      "39824,3.2948\n",
      "39825,3.3467\n",
      "39826,3.3209\n",
      "39827,3.3192\n",
      "39828,3.3675\n",
      "39829,3.3325\n",
      "39830,3.3463\n",
      "39831,3.3264\n",
      "39832,3.3343\n",
      "39833,3.3572\n",
      "39834,3.3194\n",
      "39835,3.3412\n",
      "39836,3.3451\n",
      "39837,3.3662\n",
      "39838,3.3089\n",
      "39839,3.3156\n",
      "39840,3.3228\n",
      "39841,3.3121\n",
      "39842,3.3296\n",
      "39843,3.3294\n",
      "39844,3.3406\n",
      "39845,3.3425\n",
      "39846,3.3566\n",
      "39847,3.3144\n",
      "39848,3.3208\n",
      "39849,3.3471\n",
      "39850,3.3660\n",
      "39851,3.3565\n",
      "39852,3.3293\n",
      "39853,3.3498\n",
      "39854,3.3335\n",
      "39855,3.3294\n",
      "39856,3.3299\n",
      "39857,3.3526\n",
      "39858,3.3540\n",
      "39859,3.2979\n",
      "39860,3.3070\n",
      "39861,3.3653\n",
      "39862,3.3356\n",
      "39863,3.3399\n",
      "39864,3.3690\n",
      "39865,3.3585\n",
      "39866,3.3606\n",
      "39867,3.3312\n",
      "39868,3.3202\n",
      "39869,3.3548\n",
      "39870,3.3376\n",
      "39871,3.3134\n",
      "39872,3.3500\n",
      "39873,3.3318\n",
      "39874,3.3408\n",
      "39875,3.3538\n",
      "39876,3.3328\n",
      "39877,3.3645\n",
      "39878,3.3227\n",
      "39879,3.3300\n",
      "39880,3.3465\n",
      "39881,3.3535\n",
      "39882,3.3434\n",
      "39883,3.3142\n",
      "39884,3.3394\n",
      "39885,3.3491\n",
      "39886,3.3231\n",
      "39887,3.3290\n",
      "39888,3.3488\n",
      "39889,3.3374\n",
      "39890,3.3390\n",
      "39891,3.3582\n",
      "39892,3.3652\n",
      "39893,3.3023\n",
      "39894,3.3196\n",
      "39895,3.3285\n",
      "39896,3.3138\n",
      "39897,3.3569\n",
      "39898,3.3184\n",
      "39899,3.3290\n",
      "39900,3.3323\n",
      "(step: 39900, epoch: 0, block: 40857600), Train Loss: 3.2730, Val Loss: 3.2682\n",
      "Learning rate: 3.46236144e-05\n",
      "Scheduler step: 39900\n",
      "Checkpoint (step: 39900, epoch: 0, block: 40857600) Saved\n",
      "Step Documented\n",
      "17:52:50\n",
      "39901,3.3521\n",
      "39902,3.3490\n",
      "39903,3.3508\n",
      "39904,3.2967\n",
      "39905,3.3629\n",
      "39906,3.3067\n",
      "39907,3.3494\n",
      "39908,3.3831\n",
      "39909,3.3217\n",
      "39910,3.3377\n",
      "39911,3.3028\n",
      "39912,3.3198\n",
      "39913,3.3364\n",
      "39914,3.3364\n",
      "39915,3.3475\n",
      "39916,3.3557\n",
      "39917,3.3123\n",
      "39918,3.3393\n",
      "39919,3.3474\n",
      "39920,3.3499\n",
      "39921,3.3267\n",
      "39922,3.3499\n",
      "39923,3.3433\n",
      "39924,3.3379\n",
      "39925,3.3452\n",
      "39926,3.3402\n",
      "39927,3.3546\n",
      "39928,3.3321\n",
      "39929,3.3853\n",
      "39930,3.2954\n",
      "39931,3.3380\n",
      "39932,3.3368\n",
      "39933,3.3168\n",
      "39934,3.3192\n",
      "39935,3.3322\n",
      "39936,3.3406\n",
      "39937,3.3299\n",
      "39938,3.3353\n",
      "39939,3.3601\n",
      "39940,3.3337\n",
      "39941,3.3229\n",
      "39942,3.3143\n",
      "39943,3.3121\n",
      "39944,3.3352\n",
      "39945,3.3565\n",
      "39946,3.3282\n",
      "39947,3.3512\n",
      "39948,3.3479\n",
      "39949,3.3487\n",
      "39950,3.3099\n",
      "39951,3.3567\n",
      "39952,3.3158\n",
      "39953,3.3393\n",
      "39954,3.3378\n",
      "39955,3.3397\n",
      "39956,3.3328\n",
      "39957,3.3396\n",
      "39958,3.3549\n",
      "39959,3.3404\n",
      "39960,3.3402\n",
      "39961,3.3482\n",
      "39962,3.3582\n",
      "39963,3.3227\n",
      "39964,3.3803\n",
      "39965,3.3378\n",
      "39966,3.3210\n",
      "39967,3.3395\n",
      "39968,3.2859\n",
      "39969,3.3166\n",
      "39970,3.3579\n",
      "39971,3.3509\n",
      "39972,3.3455\n",
      "39973,3.3372\n",
      "39974,3.3446\n",
      "39975,3.3410\n",
      "39976,3.3425\n",
      "39977,3.3655\n",
      "39978,3.3507\n",
      "39979,3.3422\n",
      "39980,3.3400\n",
      "39981,3.3452\n",
      "39982,3.3362\n",
      "39983,3.3333\n",
      "39984,3.3363\n",
      "39985,3.3291\n",
      "39986,3.3392\n",
      "39987,3.3629\n",
      "39988,3.3266\n",
      "39989,3.3509\n",
      "39990,3.3588\n",
      "39991,3.3457\n",
      "39992,3.3715\n",
      "39993,3.3364\n",
      "39994,3.3315\n",
      "39995,3.3476\n",
      "39996,3.3438\n",
      "39997,3.3451\n",
      "39998,3.3259\n",
      "39999,3.3271\n",
      "40000,3.3471\n",
      "(step: 40000, epoch: 0, block: 40960000), Train Loss: 3.2499, Val Loss: 3.2854\n",
      "Learning rate: 3.44141526e-05\n",
      "Scheduler step: 40000\n",
      "Checkpoint (step: 40000, epoch: 0, block: 40960000) Saved\n",
      "Step Documented\n",
      "18:19:09\n",
      "40001,3.3523\n",
      "40002,3.3215\n",
      "40003,3.3292\n",
      "40004,3.3467\n",
      "40005,3.3004\n",
      "40006,3.3496\n",
      "40007,3.3261\n",
      "40008,3.3350\n",
      "40009,3.3489\n",
      "40010,3.3309\n",
      "40011,3.3522\n",
      "40012,3.3222\n",
      "40013,3.3502\n",
      "40014,3.3508\n",
      "40015,3.3469\n",
      "40016,3.3412\n",
      "40017,3.3358\n",
      "40018,3.3525\n",
      "40019,3.3277\n",
      "40020,3.3599\n",
      "40021,3.3557\n",
      "40022,3.3740\n",
      "40023,3.3258\n",
      "40024,3.3375\n",
      "40025,3.3276\n",
      "40026,3.3394\n",
      "40027,3.3506\n",
      "40028,3.3326\n",
      "40029,3.3179\n",
      "40030,3.3324\n",
      "40031,3.3425\n",
      "40032,3.3025\n",
      "40033,3.3911\n",
      "40034,3.3306\n",
      "40035,3.3445\n",
      "40036,3.3556\n",
      "40037,3.3324\n",
      "40038,3.3226\n",
      "40039,3.3489\n",
      "40040,3.3242\n",
      "40041,3.3377\n",
      "40042,3.3584\n",
      "40043,3.3170\n",
      "40044,3.3513\n",
      "40045,3.3619\n",
      "40046,3.3508\n",
      "40047,3.3567\n",
      "40048,3.3371\n",
      "40049,3.3181\n",
      "40050,3.3457\n",
      "40051,3.3344\n",
      "40052,3.2928\n",
      "40053,3.2965\n",
      "40054,3.3082\n",
      "40055,3.3122\n",
      "40056,3.3654\n",
      "40057,3.3281\n",
      "40058,3.3534\n",
      "40059,3.3579\n",
      "40060,3.3676\n",
      "40061,3.3211\n",
      "40062,3.3398\n",
      "40063,3.3571\n",
      "40064,3.3425\n",
      "40065,3.3392\n",
      "40066,3.3165\n",
      "40067,3.3168\n",
      "40068,3.3391\n",
      "40069,3.3602\n",
      "40070,3.3552\n",
      "40071,3.3362\n",
      "40072,3.3572\n",
      "40073,3.3683\n",
      "40074,3.3502\n",
      "40075,3.3279\n",
      "40076,3.3442\n",
      "40077,3.3652\n",
      "40078,3.3350\n",
      "40079,3.3225\n",
      "40080,3.3327\n",
      "40081,3.3470\n",
      "40082,3.3333\n",
      "40083,3.3103\n",
      "40084,3.3099\n",
      "40085,3.3601\n",
      "40086,3.3466\n",
      "40087,3.3421\n",
      "40088,3.3453\n",
      "40089,3.3220\n",
      "40090,3.3573\n",
      "40091,3.3389\n",
      "40092,3.3362\n",
      "40093,3.3513\n",
      "40094,3.3578\n",
      "40095,3.3473\n",
      "40096,3.3463\n",
      "40097,3.3427\n",
      "40098,3.3328\n",
      "40099,3.3456\n",
      "40100,3.3271\n",
      "(step: 40100, epoch: 0, block: 41062400), Train Loss: 3.2823, Val Loss: 3.2836\n",
      "Learning rate: 3.42051857e-05\n",
      "Scheduler step: 40100\n",
      "Checkpoint (step: 40100, epoch: 0, block: 41062400) Saved\n",
      "Step Documented\n",
      "18:45:30\n",
      "40101,3.3297\n",
      "40102,3.3299\n",
      "40103,3.3364\n",
      "40104,3.3255\n",
      "40105,3.3260\n",
      "40106,3.3821\n",
      "40107,3.3218\n",
      "40108,3.3229\n",
      "40109,3.3409\n",
      "40110,3.3591\n",
      "40111,3.3482\n",
      "40112,3.3293\n",
      "40113,3.3103\n",
      "40114,3.3323\n",
      "40115,3.3739\n",
      "40116,3.3237\n",
      "40117,3.3596\n",
      "40118,3.3569\n",
      "40119,3.3588\n",
      "40120,3.3504\n",
      "40121,3.3557\n",
      "40122,3.3304\n",
      "40123,3.3017\n",
      "40124,3.3250\n",
      "40125,3.3313\n",
      "40126,3.3715\n",
      "40127,3.3227\n",
      "40128,3.3452\n",
      "40129,3.3257\n",
      "40130,3.3340\n",
      "40131,3.3793\n",
      "40132,3.3551\n",
      "40133,3.3228\n",
      "40134,3.3489\n",
      "40135,3.3701\n",
      "40136,3.3514\n",
      "40137,3.3656\n",
      "40138,3.3416\n",
      "40139,3.3294\n",
      "40140,3.3175\n",
      "40141,3.3414\n",
      "40142,3.3369\n",
      "40143,3.3629\n",
      "40144,3.3514\n",
      "40145,3.3236\n",
      "40146,3.3349\n",
      "40147,3.3379\n",
      "40148,3.3445\n",
      "40149,3.3102\n",
      "40150,3.3583\n",
      "40151,3.2995\n",
      "40152,3.3320\n",
      "40153,3.3346\n",
      "40154,3.3564\n",
      "40155,3.3255\n",
      "40156,3.3211\n",
      "40157,3.3243\n",
      "40158,3.3308\n",
      "40159,3.3413\n",
      "40160,3.3310\n",
      "40161,3.3118\n",
      "40162,3.3423\n",
      "40163,3.3188\n",
      "40164,3.3442\n",
      "40165,3.3329\n",
      "40166,3.3519\n",
      "40167,3.3133\n",
      "40168,3.3365\n",
      "40169,3.3648\n",
      "40170,3.3289\n",
      "40171,3.3317\n",
      "40172,3.3203\n",
      "40173,3.3254\n",
      "40174,3.3449\n",
      "40175,3.3359\n",
      "40176,3.3625\n",
      "40177,3.3479\n",
      "40178,3.3321\n",
      "40179,3.3087\n",
      "40180,3.3335\n",
      "40181,3.3124\n",
      "40182,3.3243\n",
      "40183,3.3441\n",
      "40184,3.3193\n",
      "40185,3.3669\n",
      "40186,3.3314\n",
      "40187,3.3351\n",
      "40188,3.2951\n",
      "40189,3.3662\n",
      "40190,3.3367\n",
      "40191,3.3340\n",
      "40192,3.3380\n",
      "40193,3.3555\n",
      "40194,3.3554\n",
      "40195,3.3123\n",
      "40196,3.3595\n",
      "40197,3.3605\n",
      "40198,3.3387\n",
      "40199,3.3199\n",
      "40200,3.3473\n",
      "(step: 40200, epoch: 0, block: 41164800), Train Loss: 3.2408, Val Loss: 3.2775\n",
      "Learning rate: 3.39967189e-05\n",
      "Scheduler step: 40200\n",
      "Checkpoint (step: 40200, epoch: 0, block: 41164800) Saved\n",
      "Step Documented\n",
      "19:11:49\n",
      "40201,3.3536\n",
      "40202,3.2930\n",
      "40203,3.3197\n",
      "40204,3.3252\n",
      "40205,3.3291\n",
      "40206,3.3382\n",
      "40207,3.3415\n",
      "40208,3.3369\n",
      "40209,3.3352\n",
      "40210,3.3289\n",
      "40211,3.3669\n",
      "40212,3.3513\n",
      "40213,3.3173\n",
      "40214,3.3202\n",
      "40215,3.3404\n",
      "40216,3.3408\n",
      "40217,3.3321\n",
      "40218,3.3266\n",
      "40219,3.3334\n",
      "40220,3.3412\n",
      "40221,3.3398\n",
      "40222,3.3261\n",
      "40223,3.3300\n",
      "40224,3.3441\n",
      "40225,3.3301\n",
      "40226,3.3460\n",
      "40227,3.3402\n",
      "40228,3.3196\n",
      "40229,3.3220\n",
      "40230,3.3385\n",
      "40231,3.3586\n",
      "40232,3.3295\n",
      "40233,3.3641\n",
      "40234,3.3507\n",
      "40235,3.3408\n",
      "40236,3.3559\n",
      "40237,3.3609\n",
      "40238,3.3508\n",
      "40239,3.3518\n",
      "40240,3.3318\n",
      "40241,3.3337\n",
      "40242,3.3171\n",
      "40243,3.3406\n",
      "40244,3.3355\n",
      "40245,3.3163\n",
      "40246,3.3706\n",
      "40247,3.3426\n",
      "40248,3.3394\n",
      "40249,3.3400\n",
      "40250,3.3634\n",
      "40251,3.3400\n",
      "40252,3.3395\n",
      "40253,3.3433\n",
      "40254,3.3257\n",
      "40255,3.3549\n",
      "40256,3.3302\n",
      "40257,3.3491\n",
      "40258,3.3372\n",
      "40259,3.3193\n",
      "40260,3.3279\n",
      "40261,3.3318\n",
      "40262,3.3742\n",
      "40263,3.3788\n",
      "40264,3.3449\n",
      "40265,3.3348\n",
      "40266,3.3512\n",
      "40267,3.3374\n",
      "40268,3.3188\n",
      "40269,3.3635\n",
      "40270,3.3529\n",
      "40271,3.3224\n",
      "40272,3.3402\n",
      "40273,3.3194\n",
      "40274,3.3142\n",
      "40275,3.3564\n",
      "40276,3.3632\n",
      "40277,3.3701\n",
      "40278,3.3599\n",
      "40279,3.3533\n",
      "40280,3.3337\n",
      "40281,3.3092\n",
      "40282,3.3306\n",
      "40283,3.3374\n",
      "40284,3.3429\n",
      "40285,3.3110\n",
      "40286,3.3370\n",
      "40287,3.3461\n",
      "40288,3.3518\n",
      "40289,3.3504\n",
      "40290,3.3447\n",
      "40291,3.3287\n",
      "40292,3.3186\n",
      "40293,3.3270\n",
      "40294,3.3390\n",
      "40295,3.3502\n",
      "40296,3.3355\n",
      "40297,3.3592\n",
      "40298,3.3464\n",
      "40299,3.3616\n",
      "40300,3.3394\n",
      "(step: 40300, epoch: 0, block: 41267200), Train Loss: 3.2722, Val Loss: 3.2861\n",
      "Learning rate: 3.37887576e-05\n",
      "Scheduler step: 40300\n",
      "Checkpoint (step: 40300, epoch: 0, block: 41267200) Saved\n",
      "Step Documented\n",
      "19:38:09\n",
      "40301,3.3092\n",
      "40302,3.3091\n",
      "40303,3.3359\n",
      "40304,3.3375\n",
      "40305,3.3173\n",
      "40306,3.3367\n",
      "40307,3.3450\n",
      "40308,3.3574\n",
      "40309,3.3383\n",
      "40310,3.3157\n",
      "40311,3.3292\n",
      "40312,3.3639\n",
      "40313,3.3186\n",
      "40314,3.3372\n",
      "40315,3.3356\n",
      "40316,3.3543\n",
      "40317,3.3145\n",
      "40318,3.3495\n",
      "40319,3.3603\n",
      "40320,3.3396\n",
      "40321,3.3528\n",
      "40322,3.3015\n",
      "40323,3.3186\n",
      "40324,3.3293\n",
      "40325,3.3322\n",
      "40326,3.3505\n",
      "40327,3.3550\n",
      "40328,3.3534\n",
      "40329,3.3469\n",
      "40330,3.3300\n",
      "40331,3.3726\n",
      "40332,3.3228\n",
      "40333,3.3304\n",
      "40334,3.3419\n",
      "40335,3.3409\n",
      "40336,3.3426\n",
      "40337,3.3253\n",
      "40338,3.3516\n",
      "40339,3.3437\n",
      "40340,3.3272\n",
      "40341,3.3113\n",
      "40342,3.3589\n",
      "40343,3.3185\n",
      "40344,3.3518\n",
      "40345,3.3244\n",
      "40346,3.3403\n",
      "40347,3.3132\n",
      "40348,3.3420\n",
      "40349,3.3477\n",
      "40350,3.3281\n",
      "40351,3.3523\n",
      "40352,3.3591\n",
      "40353,3.3356\n",
      "40354,3.3482\n",
      "40355,3.3488\n",
      "40356,3.3267\n",
      "40357,3.3238\n",
      "40358,3.3449\n",
      "40359,3.3511\n",
      "40360,3.3409\n",
      "40361,3.3236\n",
      "40362,3.3306\n",
      "40363,3.3465\n",
      "40364,3.3184\n",
      "40365,3.3271\n",
      "40366,3.3431\n",
      "40367,3.3338\n",
      "40368,3.3507\n",
      "40369,3.3367\n",
      "40370,3.3312\n",
      "40371,3.3221\n",
      "40372,3.3508\n",
      "40373,3.3415\n",
      "40374,3.3466\n",
      "40375,3.3383\n",
      "40376,3.3401\n",
      "40377,3.3144\n",
      "40378,3.3297\n",
      "40379,3.3706\n",
      "40380,3.3347\n",
      "40381,3.3278\n",
      "40382,3.3501\n",
      "40383,3.3485\n",
      "40384,3.3171\n",
      "40385,3.3337\n",
      "40386,3.3454\n",
      "40387,3.3341\n",
      "40388,3.3205\n",
      "40389,3.3272\n",
      "40390,3.3120\n",
      "40391,3.3334\n",
      "40392,3.3452\n",
      "40393,3.3831\n",
      "40394,3.3085\n",
      "40395,3.3274\n",
      "40396,3.3406\n",
      "40397,3.3519\n",
      "40398,3.3506\n",
      "40399,3.3273\n",
      "40400,3.3202\n",
      "(step: 40400, epoch: 0, block: 41369600), Train Loss: 3.2343, Val Loss: 3.2633\n",
      "Learning rate: 3.35813069e-05\n",
      "Scheduler step: 40400\n",
      "Checkpoint (step: 40400, epoch: 0, block: 41369600) Saved\n",
      "Step Documented\n",
      "20:04:28\n",
      "40401,3.3306\n",
      "40402,3.3581\n",
      "40403,3.3275\n",
      "40404,3.3742\n",
      "40405,3.3677\n",
      "40406,3.3302\n",
      "40407,3.3430\n",
      "40408,3.3328\n",
      "40409,3.3407\n",
      "40410,3.3490\n",
      "40411,3.3180\n",
      "40412,3.3693\n",
      "40413,3.3634\n",
      "40414,3.3365\n",
      "40415,3.3492\n",
      "40416,3.3252\n",
      "40417,3.3473\n",
      "40418,3.3275\n",
      "40419,3.3272\n",
      "40420,3.3276\n",
      "40421,3.3712\n",
      "40422,3.2764\n",
      "40423,3.3253\n",
      "40424,3.3432\n",
      "40425,3.3368\n",
      "40426,3.3598\n",
      "40427,3.3550\n",
      "40428,3.3754\n",
      "40429,3.3261\n",
      "40430,3.3439\n",
      "40431,3.3390\n",
      "40432,3.3448\n",
      "40433,3.3428\n",
      "40434,3.3400\n",
      "40435,3.3270\n",
      "40436,3.3383\n",
      "40437,3.3399\n",
      "40438,3.3367\n",
      "40439,3.3220\n",
      "40440,3.3261\n",
      "40441,3.3787\n",
      "40442,3.3186\n",
      "40443,3.3382\n",
      "40444,3.3539\n",
      "40445,3.3294\n",
      "40446,3.3081\n",
      "40447,3.3306\n",
      "40448,3.3286\n",
      "40449,3.3374\n",
      "40450,3.3297\n",
      "40451,3.3092\n",
      "40452,3.3367\n",
      "40453,3.3040\n",
      "40454,3.3170\n",
      "40455,3.3197\n",
      "40456,3.3045\n",
      "40457,3.3251\n",
      "40458,3.3648\n",
      "40459,3.3353\n",
      "40460,3.3404\n",
      "40461,3.3119\n",
      "40462,3.3495\n",
      "40463,3.3256\n",
      "40464,3.3337\n",
      "40465,3.3523\n",
      "40466,3.3225\n",
      "40467,3.3210\n",
      "40468,3.3347\n",
      "40469,3.3355\n",
      "40470,3.3560\n",
      "40471,3.3634\n",
      "40472,3.3359\n",
      "40473,3.3228\n",
      "40474,3.3725\n",
      "40475,3.3407\n",
      "40476,3.3482\n",
      "40477,3.3348\n",
      "40478,3.3441\n",
      "40479,3.3320\n",
      "40480,3.3229\n",
      "40481,3.3404\n",
      "40482,3.3528\n",
      "40483,3.3413\n",
      "40484,3.3162\n",
      "40485,3.3486\n",
      "40486,3.3198\n",
      "40487,3.3535\n",
      "40488,3.3308\n",
      "40489,3.3505\n",
      "40490,3.3717\n",
      "40491,3.3528\n",
      "40492,3.3635\n",
      "40493,3.3426\n",
      "40494,3.3506\n",
      "40495,3.3234\n",
      "40496,3.3580\n",
      "40497,3.3150\n",
      "40498,3.3510\n",
      "40499,3.3372\n",
      "40500,3.3605\n",
      "(step: 40500, epoch: 0, block: 41472000), Train Loss: 3.2271, Val Loss: 3.2669\n",
      "Learning rate: 3.33743721e-05\n",
      "Scheduler step: 40500\n",
      "Checkpoint (step: 40500, epoch: 0, block: 41472000) Saved\n",
      "Step Documented\n",
      "20:30:47\n",
      "40501,3.3199\n",
      "40502,3.3269\n",
      "40503,3.3558\n",
      "40504,3.3183\n",
      "40505,3.3345\n",
      "40506,3.3272\n",
      "40507,3.3475\n",
      "40508,3.3454\n",
      "40509,3.3164\n",
      "40510,3.3022\n",
      "40511,3.3248\n",
      "40512,3.3395\n",
      "40513,3.3273\n",
      "40514,3.3610\n",
      "40515,3.3304\n",
      "40516,3.3048\n",
      "40517,3.3201\n",
      "40518,3.3703\n",
      "40519,3.3060\n",
      "40520,3.3582\n",
      "40521,3.3296\n",
      "40522,3.3395\n",
      "40523,3.3295\n",
      "40524,3.3521\n",
      "40525,3.3437\n",
      "40526,3.3218\n",
      "40527,3.3436\n",
      "40528,3.3213\n",
      "40529,3.3483\n",
      "40530,3.3587\n",
      "40531,3.3323\n",
      "40532,3.3636\n",
      "40533,3.3584\n",
      "40534,3.3416\n",
      "40535,3.3510\n",
      "40536,3.3503\n",
      "40537,3.3248\n",
      "40538,3.3372\n",
      "40539,3.3436\n",
      "40540,3.3352\n",
      "40541,3.3234\n",
      "40542,3.3392\n",
      "40543,3.3069\n",
      "40544,3.3375\n",
      "40545,3.3205\n",
      "40546,3.3351\n",
      "40547,3.3536\n",
      "40548,3.3472\n",
      "40549,3.3508\n",
      "40550,3.3563\n",
      "40551,3.3304\n",
      "40552,3.3402\n",
      "40553,3.3469\n",
      "40554,3.3092\n",
      "40555,3.3399\n",
      "40556,3.3584\n",
      "40557,3.3086\n",
      "40558,3.3481\n",
      "40559,3.3539\n",
      "40560,3.3431\n",
      "40561,3.3452\n",
      "40562,3.3359\n",
      "40563,3.3080\n",
      "40564,3.3363\n",
      "40565,3.3035\n",
      "40566,3.3265\n",
      "40567,3.3572\n",
      "40568,3.3549\n",
      "40569,3.3498\n",
      "40570,3.3285\n",
      "40571,3.3355\n",
      "40572,3.3344\n",
      "40573,3.3529\n",
      "40574,3.3623\n",
      "40575,3.3329\n",
      "40576,3.3371\n",
      "40577,3.3064\n",
      "40578,3.3436\n",
      "40579,3.3488\n",
      "40580,3.3069\n",
      "40581,3.3644\n",
      "40582,3.3327\n",
      "40583,3.3329\n",
      "40584,3.3420\n",
      "40585,3.3269\n",
      "40586,3.3776\n",
      "40587,3.3321\n",
      "40588,3.3394\n",
      "40589,3.3456\n",
      "40590,3.3453\n",
      "40591,3.3379\n",
      "40592,3.3061\n",
      "40593,3.3285\n",
      "40594,3.3396\n",
      "40595,3.3573\n",
      "40596,3.3409\n",
      "40597,3.3286\n",
      "40598,3.3425\n",
      "40599,3.3565\n",
      "40600,3.3650\n",
      "(step: 40600, epoch: 0, block: 41574400), Train Loss: 3.2800, Val Loss: 3.2790\n",
      "Learning rate: 3.31679584e-05\n",
      "Scheduler step: 40600\n",
      "Checkpoint (step: 40600, epoch: 0, block: 41574400) Saved\n",
      "Step Documented\n",
      "20:57:06\n",
      "40601,3.3505\n",
      "40602,3.3533\n",
      "40603,3.3391\n",
      "40604,3.3309\n",
      "40605,3.3234\n",
      "40606,3.3848\n",
      "40607,3.3194\n",
      "40608,3.3260\n",
      "40609,3.3172\n",
      "40610,3.3391\n",
      "40611,3.3161\n",
      "40612,3.3456\n",
      "40613,3.3366\n",
      "40614,3.3362\n",
      "40615,3.3432\n",
      "40616,3.3352\n",
      "40617,3.3296\n",
      "40618,3.3343\n",
      "40619,3.3686\n",
      "40620,3.3319\n",
      "40621,3.3365\n",
      "40622,3.3142\n",
      "40623,3.3423\n",
      "40624,3.3468\n",
      "40625,3.3190\n",
      "40626,3.3377\n",
      "40627,3.3352\n",
      "40628,3.3454\n",
      "40629,3.3464\n",
      "40630,3.3196\n",
      "40631,3.3200\n",
      "40632,3.3201\n",
      "40633,3.3452\n",
      "40634,3.3523\n",
      "40635,3.3221\n",
      "40636,3.3338\n",
      "40637,3.3185\n",
      "40638,3.3489\n",
      "40639,3.3312\n",
      "40640,3.3167\n",
      "40641,3.3142\n",
      "40642,3.3369\n",
      "40643,3.3278\n",
      "40644,3.3465\n",
      "40645,3.3307\n",
      "40646,3.3319\n",
      "40647,3.3379\n",
      "40648,3.3369\n",
      "40649,3.3165\n",
      "40650,3.3403\n",
      "40651,3.3577\n",
      "40652,3.3146\n",
      "40653,3.3435\n",
      "40654,3.3070\n",
      "40655,3.3296\n",
      "40656,3.3241\n",
      "40657,3.3333\n",
      "40658,3.3448\n",
      "40659,3.3580\n",
      "40660,3.3350\n",
      "40661,3.3126\n",
      "40662,3.3415\n",
      "40663,3.3167\n",
      "40664,3.3420\n",
      "40665,3.3525\n",
      "40666,3.3084\n",
      "40667,3.3484\n",
      "40668,3.3170\n",
      "40669,3.3382\n",
      "40670,3.3294\n",
      "40671,3.3474\n",
      "40672,3.3546\n",
      "40673,3.3611\n",
      "40674,3.3608\n",
      "40675,3.3374\n",
      "40676,3.3394\n",
      "40677,3.3395\n",
      "40678,3.3304\n",
      "40679,3.3394\n",
      "40680,3.3018\n",
      "40681,3.3244\n",
      "40682,3.3435\n",
      "40683,3.3180\n",
      "40684,3.2936\n",
      "40685,3.3109\n",
      "40686,3.3376\n",
      "40687,3.3289\n",
      "40688,3.3150\n",
      "40689,3.3360\n",
      "40690,3.3425\n",
      "40691,3.3518\n",
      "40692,3.3476\n",
      "40693,3.3345\n",
      "40694,3.3165\n",
      "40695,3.3256\n",
      "40696,3.3495\n",
      "40697,3.3176\n",
      "40698,3.3345\n",
      "40699,3.3470\n",
      "40700,3.3502\n",
      "(step: 40700, epoch: 0, block: 41676800), Train Loss: 3.2411, Val Loss: 3.2850\n",
      "Learning rate: 3.29620711e-05\n",
      "Scheduler step: 40700\n",
      "Checkpoint (step: 40700, epoch: 0, block: 41676800) Saved\n",
      "Step Documented\n",
      "21:23:24\n",
      "40701,3.3255\n",
      "40702,3.3237\n",
      "40703,3.3393\n",
      "40704,3.3206\n",
      "40705,3.3407\n",
      "40706,3.3341\n",
      "40707,3.3285\n",
      "40708,3.3474\n",
      "40709,3.3578\n",
      "40710,3.3304\n",
      "40711,3.3431\n",
      "40712,3.3414\n",
      "40713,3.3741\n",
      "40714,3.3146\n",
      "40715,3.3217\n",
      "40716,3.3257\n",
      "40717,3.3269\n",
      "40718,3.3196\n",
      "40719,3.3341\n",
      "40720,3.3502\n",
      "40721,3.3713\n",
      "40722,3.3227\n",
      "40723,3.3644\n",
      "40724,3.3299\n",
      "40725,3.3176\n",
      "40726,3.3632\n",
      "40727,3.3335\n",
      "40728,3.3297\n",
      "40729,3.3295\n",
      "40730,3.3242\n",
      "40731,3.3091\n",
      "40732,3.3349\n",
      "40733,3.3317\n",
      "40734,3.3300\n",
      "40735,3.3558\n",
      "40736,3.3092\n",
      "40737,3.3113\n",
      "40738,3.3171\n",
      "40739,3.3370\n",
      "40740,3.3401\n",
      "40741,3.3366\n",
      "40742,3.3389\n",
      "40743,3.3509\n",
      "40744,3.3461\n",
      "40745,3.3316\n",
      "40746,3.3404\n",
      "40747,3.3384\n",
      "40748,3.3457\n",
      "40749,3.3435\n",
      "40750,3.3338\n",
      "40751,3.3298\n",
      "40752,3.3257\n",
      "40753,3.3331\n",
      "40754,3.3241\n",
      "40755,3.3504\n",
      "40756,3.3212\n",
      "40757,3.3286\n",
      "40758,3.3402\n",
      "40759,3.3338\n",
      "40760,3.3108\n",
      "40761,3.3322\n",
      "40762,3.3277\n",
      "40763,3.3392\n",
      "40764,3.3142\n",
      "40765,3.3491\n",
      "40766,3.3442\n",
      "40767,3.3214\n",
      "40768,3.3312\n",
      "40769,3.3634\n",
      "40770,3.3221\n",
      "40771,3.3488\n",
      "40772,3.2994\n",
      "40773,3.3110\n",
      "40774,3.3361\n",
      "40775,3.3445\n",
      "40776,3.3335\n",
      "40777,3.3415\n",
      "40778,3.3345\n",
      "40779,3.3356\n",
      "40780,3.3082\n",
      "40781,3.3473\n",
      "40782,3.3087\n",
      "40783,3.3424\n",
      "40784,3.3502\n",
      "40785,3.3286\n",
      "40786,3.3485\n",
      "40787,3.3281\n",
      "40788,3.3265\n",
      "40789,3.3384\n",
      "40790,3.3114\n",
      "40791,3.3743\n",
      "40792,3.3248\n",
      "40793,3.3461\n",
      "40794,3.3254\n",
      "40795,3.3601\n",
      "40796,3.3454\n",
      "40797,3.3355\n",
      "40798,3.3317\n",
      "40799,3.3511\n",
      "40800,3.3606\n",
      "(step: 40800, epoch: 0, block: 41779200), Train Loss: 3.2499, Val Loss: 3.2850\n",
      "Learning rate: 3.27567153e-05\n",
      "Scheduler step: 40800\n",
      "Checkpoint (step: 40800, epoch: 0, block: 41779200) Saved\n",
      "Step Documented\n",
      "21:49:44\n",
      "40801,3.3444\n",
      "40802,3.3091\n",
      "40803,3.3454\n",
      "40804,3.3333\n",
      "40805,3.3230\n",
      "40806,3.3396\n",
      "40807,3.3217\n",
      "40808,3.3095\n",
      "40809,3.3245\n",
      "40810,3.3309\n",
      "40811,3.3356\n",
      "40812,3.3514\n",
      "40813,3.3259\n",
      "40814,3.3239\n",
      "40815,3.3519\n",
      "40816,3.3472\n",
      "40817,3.3348\n",
      "40818,3.3048\n",
      "40819,3.3334\n",
      "40820,3.3621\n",
      "40821,3.3431\n",
      "40822,3.3316\n",
      "40823,3.3444\n",
      "40824,3.3345\n",
      "40825,3.3329\n",
      "40826,3.3491\n",
      "40827,3.3488\n",
      "40828,3.3443\n",
      "40829,3.3207\n",
      "40830,3.3318\n",
      "40831,3.3308\n",
      "40832,3.3305\n",
      "40833,3.3290\n",
      "40834,3.2991\n",
      "40835,3.3500\n",
      "40836,3.3260\n",
      "40837,3.3226\n",
      "40838,3.3362\n",
      "40839,3.3118\n",
      "40840,3.3075\n",
      "40841,3.3252\n",
      "40842,3.3600\n",
      "40843,3.3005\n",
      "40844,3.3292\n",
      "40845,3.3202\n",
      "40846,3.3443\n",
      "40847,3.3148\n",
      "40848,3.3270\n",
      "40849,3.3187\n",
      "40850,3.3165\n",
      "40851,3.3131\n",
      "40852,3.3104\n",
      "40853,3.3524\n",
      "40854,3.3161\n",
      "40855,3.3449\n",
      "40856,3.3424\n",
      "40857,3.3560\n",
      "40858,3.3242\n",
      "40859,3.3451\n",
      "40860,3.3205\n",
      "40861,3.3057\n",
      "40862,3.3621\n",
      "40863,3.3312\n",
      "40864,3.3340\n",
      "40865,3.3125\n",
      "40866,3.3461\n",
      "40867,3.3472\n",
      "40868,3.3248\n",
      "40869,3.3198\n",
      "40870,3.3297\n",
      "40871,3.3244\n",
      "40872,3.3114\n",
      "40873,3.3319\n",
      "40874,3.3362\n",
      "40875,3.3559\n",
      "40876,3.3012\n",
      "40877,3.3436\n",
      "40878,3.3295\n",
      "40879,3.3723\n",
      "40880,3.3222\n",
      "40881,3.3210\n",
      "40882,3.3374\n",
      "40883,3.3090\n",
      "40884,3.3529\n",
      "40885,3.3393\n",
      "40886,3.3465\n",
      "40887,3.3185\n",
      "40888,3.3139\n",
      "40889,3.3524\n",
      "40890,3.3639\n",
      "40891,3.3302\n",
      "40892,3.3324\n",
      "40893,3.3223\n",
      "40894,3.3576\n",
      "40895,3.3468\n",
      "40896,3.3528\n",
      "40897,3.3300\n",
      "40898,3.3381\n",
      "40899,3.3303\n",
      "40900,3.3518\n",
      "(step: 40900, epoch: 0, block: 41881600), Train Loss: 3.2266, Val Loss: 3.2244\n",
      "Learning rate: 3.25518963e-05\n",
      "Scheduler step: 40900\n",
      "Checkpoint (step: 40900, epoch: 0, block: 41881600) Saved\n",
      "Step Documented\n",
      "22:16:03\n",
      "40901,3.2884\n",
      "40902,3.3486\n",
      "40903,3.3398\n",
      "40904,3.3540\n",
      "40905,3.3272\n",
      "40906,3.3542\n",
      "40907,3.3403\n",
      "40908,3.3463\n",
      "40909,3.3258\n",
      "40910,3.3428\n",
      "40911,3.3540\n",
      "40912,3.3261\n",
      "40913,3.3491\n",
      "40914,3.3257\n",
      "40915,3.3463\n",
      "40916,3.3608\n",
      "40917,3.3184\n",
      "40918,3.2932\n",
      "40919,3.3248\n",
      "40920,3.3277\n",
      "40921,3.3153\n",
      "40922,3.3455\n",
      "40923,3.3533\n",
      "40924,3.3288\n",
      "40925,3.3280\n",
      "40926,3.3284\n",
      "40927,3.3562\n",
      "40928,3.3481\n",
      "40929,3.3442\n",
      "40930,3.3495\n",
      "40931,3.3721\n",
      "40932,3.3362\n",
      "40933,3.3195\n",
      "40934,3.3387\n",
      "40935,3.3349\n",
      "40936,3.3691\n",
      "40937,3.3342\n",
      "40938,3.3238\n",
      "40939,3.3368\n",
      "40940,3.3247\n",
      "40941,3.3069\n",
      "40942,3.3354\n",
      "40943,3.3253\n",
      "40944,3.3424\n",
      "40945,3.3022\n",
      "40946,3.3275\n",
      "40947,3.3566\n",
      "40948,3.3144\n",
      "40949,3.3248\n",
      "40950,3.3693\n",
      "40951,3.3875\n",
      "40952,3.3269\n",
      "40953,3.3171\n",
      "40954,3.3369\n",
      "40955,3.2907\n",
      "40956,3.3427\n",
      "40957,3.3537\n",
      "40958,3.2941\n",
      "40959,3.3407\n",
      "40960,3.3534\n",
      "40961,3.3627\n",
      "40962,3.3383\n",
      "40963,3.3469\n",
      "40964,3.3247\n",
      "40965,3.3372\n",
      "40966,3.3408\n",
      "40967,3.3434\n",
      "40968,3.3218\n",
      "40969,3.3439\n",
      "40970,3.3410\n",
      "40971,3.3356\n",
      "40972,3.3491\n",
      "40973,3.3624\n",
      "40974,3.3349\n",
      "40975,3.3369\n",
      "40976,3.3100\n",
      "40977,3.3135\n",
      "40978,3.3188\n",
      "40979,3.3396\n",
      "40980,3.3089\n",
      "40981,3.3552\n",
      "40982,3.3576\n",
      "40983,3.3357\n",
      "40984,3.3304\n",
      "40985,3.3338\n",
      "40986,3.3427\n",
      "40987,3.3074\n",
      "40988,3.3230\n",
      "40989,3.3434\n",
      "40990,3.3340\n",
      "40991,3.3318\n",
      "40992,3.3350\n",
      "40993,3.3529\n",
      "40994,3.3229\n",
      "40995,3.3389\n",
      "40996,3.3580\n",
      "40997,3.3439\n",
      "40998,3.3564\n",
      "40999,3.3271\n",
      "41000,3.3560\n",
      "(step: 41000, epoch: 0, block: 41984000), Train Loss: 3.2242, Val Loss: 3.2646\n",
      "Learning rate: 3.23476191e-05\n",
      "Scheduler step: 41000\n",
      "Checkpoint (step: 41000, epoch: 0, block: 41984000) Saved\n",
      "Step Documented\n",
      "22:42:23\n",
      "41001,3.3511\n",
      "41002,3.3482\n",
      "41003,3.3413\n",
      "41004,3.3241\n",
      "41005,3.3486\n",
      "41006,3.3315\n",
      "41007,3.3228\n",
      "41008,3.3582\n",
      "41009,3.3353\n",
      "41010,3.3117\n",
      "41011,3.3385\n",
      "41012,3.3218\n",
      "41013,3.3422\n",
      "41014,3.3170\n",
      "41015,3.3336\n",
      "41016,3.3321\n",
      "41017,3.3396\n",
      "41018,3.3532\n",
      "41019,3.3279\n",
      "41020,3.3281\n",
      "41021,3.3366\n",
      "41022,3.3292\n",
      "41023,3.3122\n",
      "41024,3.3290\n",
      "41025,3.3143\n",
      "41026,3.3143\n",
      "41027,3.3102\n",
      "41028,3.3518\n",
      "41029,3.3385\n",
      "41030,3.3228\n",
      "41031,3.3319\n",
      "41032,3.3349\n",
      "41033,3.3433\n",
      "41034,3.3214\n",
      "41035,3.3260\n",
      "41036,3.3303\n",
      "41037,3.3150\n",
      "41038,3.3166\n",
      "41039,3.3298\n",
      "41040,3.3273\n",
      "41041,3.3594\n",
      "41042,3.3394\n",
      "41043,3.3236\n",
      "41044,3.3350\n",
      "41045,3.3304\n",
      "41046,3.3283\n",
      "41047,3.3115\n",
      "41048,3.3397\n",
      "41049,3.3642\n",
      "41050,3.3301\n",
      "41051,3.3057\n",
      "41052,3.3511\n",
      "41053,3.3140\n",
      "41054,3.3537\n",
      "41055,3.3318\n",
      "41056,3.3225\n",
      "41057,3.3358\n",
      "41058,3.3297\n",
      "41059,3.3232\n",
      "41060,3.3316\n",
      "41061,3.3444\n",
      "41062,3.3212\n",
      "41063,3.3525\n",
      "41064,3.3483\n",
      "41065,3.3469\n",
      "41066,3.3317\n",
      "41067,3.3225\n",
      "41068,3.3102\n",
      "41069,3.3244\n",
      "41070,3.3386\n",
      "41071,3.3493\n",
      "41072,3.3107\n",
      "41073,3.3432\n",
      "41074,3.3257\n",
      "41075,3.3369\n",
      "41076,3.3502\n",
      "41077,3.3256\n",
      "41078,3.3464\n",
      "41079,3.3205\n",
      "41080,3.3351\n",
      "41081,3.3280\n",
      "41082,3.2936\n",
      "41083,3.3663\n",
      "41084,3.3203\n",
      "41085,3.3288\n",
      "41086,3.3297\n",
      "41087,3.3226\n",
      "41088,3.3268\n",
      "41089,3.3473\n",
      "41090,3.3223\n",
      "41091,3.2935\n",
      "41092,3.3409\n",
      "41093,3.3246\n",
      "41094,3.3237\n",
      "41095,3.3539\n",
      "41096,3.3270\n",
      "41097,3.3051\n",
      "41098,3.3191\n",
      "41099,3.3473\n",
      "41100,3.3192\n",
      "(step: 41100, epoch: 0, block: 42086400), Train Loss: 3.2645, Val Loss: 3.2631\n",
      "Learning rate: 3.21438891e-05\n",
      "Scheduler step: 41100\n",
      "Checkpoint (step: 41100, epoch: 0, block: 42086400) Saved\n",
      "Step Documented\n",
      "23:08:43\n",
      "41101,3.3369\n",
      "41102,3.3563\n",
      "41103,3.3295\n",
      "41104,3.3122\n",
      "41105,3.3439\n",
      "41106,3.3624\n",
      "41107,3.3149\n",
      "41108,3.3512\n",
      "41109,3.3526\n",
      "41110,3.3343\n",
      "41111,3.3333\n",
      "41112,3.3098\n",
      "41113,3.3319\n",
      "41114,3.3412\n",
      "41115,3.3180\n",
      "41116,3.3452\n",
      "41117,3.3514\n",
      "41118,3.3251\n",
      "41119,3.3411\n",
      "41120,3.3163\n",
      "41121,3.3356\n",
      "41122,3.3470\n",
      "41123,3.3394\n",
      "41124,3.3267\n",
      "41125,3.3391\n",
      "41126,3.3446\n",
      "41127,3.3066\n",
      "41128,3.3617\n",
      "41129,3.3259\n",
      "41130,3.3157\n",
      "41131,3.3209\n",
      "41132,3.3121\n",
      "41133,3.3413\n",
      "41134,3.3740\n",
      "41135,3.3187\n",
      "41136,3.3262\n",
      "41137,3.3332\n",
      "41138,3.3316\n",
      "41139,3.3425\n",
      "41140,3.3199\n",
      "41141,3.3242\n",
      "41142,3.3344\n",
      "41143,3.3351\n",
      "41144,3.3298\n",
      "41145,3.3446\n",
      "41146,3.3163\n",
      "41147,3.3329\n",
      "41148,3.3618\n",
      "41149,3.3294\n",
      "41150,3.3716\n",
      "41151,3.3529\n",
      "41152,3.3510\n",
      "41153,3.3230\n",
      "41154,3.3040\n",
      "41155,3.3518\n",
      "41156,3.3411\n",
      "41157,3.3335\n",
      "41158,3.3487\n",
      "41159,3.3183\n",
      "41160,3.3220\n",
      "41161,3.3332\n",
      "41162,3.3219\n",
      "41163,3.3406\n",
      "41164,3.3521\n",
      "41165,3.3207\n",
      "41166,3.3469\n",
      "41167,3.3216\n",
      "41168,3.3363\n",
      "41169,3.3207\n",
      "41170,3.3146\n",
      "41171,3.3172\n",
      "41172,3.3255\n",
      "41173,3.3180\n",
      "41174,3.3201\n",
      "41175,3.3252\n",
      "41176,3.3184\n",
      "41177,3.3240\n",
      "41178,3.3417\n",
      "41179,3.3517\n",
      "41180,3.3248\n",
      "41181,3.3243\n",
      "41182,3.3520\n",
      "41183,3.3646\n",
      "41184,3.3281\n",
      "41185,3.3563\n",
      "41186,3.3385\n",
      "41187,3.3513\n",
      "41188,3.3427\n",
      "41189,3.3303\n",
      "41190,3.3052\n",
      "41191,3.3491\n",
      "41192,3.3271\n",
      "41193,3.3512\n",
      "41194,3.3500\n",
      "41195,3.3477\n",
      "41196,3.3539\n",
      "41197,3.3277\n",
      "41198,3.2960\n",
      "41199,3.3327\n",
      "41200,3.3359\n",
      "(step: 41200, epoch: 0, block: 42188800), Train Loss: 3.2788, Val Loss: 3.2284\n",
      "Learning rate: 3.19407112e-05\n",
      "Scheduler step: 41200\n",
      "Checkpoint (step: 41200, epoch: 0, block: 42188800) Saved\n",
      "Step Documented\n",
      "23:35:02\n",
      "41201,3.3535\n",
      "41202,3.3458\n",
      "41203,3.3213\n",
      "41204,3.3399\n",
      "41205,3.3722\n",
      "41206,3.3114\n",
      "41207,3.3179\n",
      "41208,3.3528\n",
      "41209,3.3529\n",
      "41210,3.3346\n",
      "41211,3.3075\n",
      "41212,3.3432\n",
      "41213,3.3534\n",
      "41214,3.3238\n",
      "41215,3.3303\n",
      "41216,3.3187\n",
      "41217,3.3175\n",
      "41218,3.3430\n",
      "41219,3.3098\n",
      "41220,3.3356\n",
      "41221,3.3366\n",
      "41222,3.3141\n",
      "41223,3.3448\n",
      "41224,3.3506\n",
      "41225,3.3583\n",
      "41226,3.3369\n",
      "41227,3.3223\n",
      "41228,3.3093\n",
      "41229,3.3476\n",
      "41230,3.3325\n",
      "41231,3.3450\n",
      "41232,3.3202\n",
      "41233,3.3271\n",
      "41234,3.3389\n",
      "41235,3.3431\n",
      "41236,3.3121\n",
      "41237,3.2901\n",
      "41238,3.3436\n",
      "41239,3.3315\n",
      "41240,3.3282\n",
      "41241,3.3220\n",
      "41242,3.3216\n",
      "41243,3.3097\n",
      "41244,3.3372\n",
      "41245,3.3438\n",
      "41246,3.3030\n",
      "41247,3.3139\n",
      "41248,3.3412\n",
      "41249,3.3401\n",
      "41250,3.3229\n",
      "41251,3.3310\n",
      "41252,3.3313\n",
      "41253,3.3315\n",
      "41254,3.3359\n",
      "41255,3.3220\n",
      "41256,3.3426\n",
      "41257,3.3185\n",
      "41258,3.3325\n",
      "41259,3.3147\n",
      "41260,3.3593\n",
      "41261,3.3351\n",
      "41262,3.3430\n",
      "41263,3.3661\n",
      "41264,3.3512\n",
      "41265,3.3130\n",
      "41266,3.3282\n",
      "41267,3.3433\n",
      "41268,3.3431\n",
      "41269,3.3420\n",
      "41270,3.3124\n",
      "41271,3.3371\n",
      "41272,3.3233\n",
      "41273,3.3508\n",
      "41274,3.3581\n",
      "41275,3.3234\n",
      "41276,3.3274\n",
      "41277,3.3511\n",
      "41278,3.3476\n",
      "41279,3.3242\n",
      "41280,3.3166\n",
      "41281,3.3539\n",
      "41282,3.3171\n",
      "41283,3.3372\n",
      "41284,3.3483\n",
      "41285,3.3434\n",
      "41286,3.3360\n",
      "41287,3.3229\n",
      "41288,3.3404\n",
      "41289,3.3443\n",
      "41290,3.3146\n",
      "41291,3.3549\n",
      "41292,3.3170\n",
      "41293,3.3591\n",
      "41294,3.3250\n",
      "41295,3.3488\n",
      "41296,3.3192\n",
      "41297,3.3322\n",
      "41298,3.2963\n",
      "41299,3.3252\n",
      "41300,3.3391\n",
      "(step: 41300, epoch: 0, block: 42291200), Train Loss: 3.2410, Val Loss: 3.2762\n",
      "Learning rate: 3.17380907e-05\n",
      "Scheduler step: 41300\n",
      "Checkpoint (step: 41300, epoch: 0, block: 42291200) Saved\n",
      "Step Documented\n",
      "00:01:21\n",
      "41301,3.3229\n",
      "41302,3.3033\n",
      "41303,3.3187\n",
      "41304,3.3435\n",
      "41305,3.3070\n",
      "41306,3.3101\n",
      "41307,3.3117\n",
      "41308,3.3345\n",
      "41309,3.3275\n",
      "41310,3.3463\n",
      "41311,3.3226\n",
      "41312,3.3073\n",
      "41313,3.3170\n",
      "41314,3.3456\n",
      "41315,3.3266\n",
      "41316,3.3272\n",
      "41317,3.3137\n",
      "41318,3.3262\n",
      "41319,3.3784\n",
      "41320,3.3339\n",
      "41321,3.3384\n",
      "41322,3.3283\n",
      "41323,3.3508\n",
      "41324,3.3360\n",
      "41325,3.3344\n",
      "41326,3.3477\n",
      "41327,3.3413\n",
      "41328,3.3289\n",
      "41329,3.3306\n",
      "41330,3.3509\n",
      "41331,3.3530\n",
      "41332,3.3368\n",
      "41333,3.3548\n",
      "41334,3.3353\n",
      "41335,3.3103\n",
      "41336,3.3093\n",
      "41337,3.3275\n",
      "41338,3.3283\n",
      "41339,3.3452\n",
      "41340,3.3464\n",
      "41341,3.3578\n",
      "41342,3.3676\n",
      "41343,3.3202\n",
      "41344,3.3471\n",
      "41345,3.3350\n",
      "41346,3.3416\n",
      "41347,3.3323\n",
      "41348,3.3118\n",
      "41349,3.3575\n",
      "41350,3.3464\n",
      "41351,3.3483\n",
      "41352,3.2995\n",
      "41353,3.3061\n",
      "41354,3.3457\n",
      "41355,3.3362\n",
      "41356,3.3185\n",
      "41357,3.3626\n",
      "41358,3.3412\n",
      "41359,3.3409\n",
      "41360,3.3497\n",
      "41361,3.3058\n",
      "41362,3.3149\n",
      "41363,3.3666\n",
      "41364,3.3429\n",
      "41365,3.3267\n",
      "41366,3.3147\n",
      "41367,3.3470\n",
      "41368,3.3450\n",
      "41369,3.3148\n",
      "41370,3.3522\n",
      "41371,3.3434\n",
      "41372,3.3472\n",
      "41373,3.3069\n",
      "41374,3.3662\n",
      "41375,3.3243\n",
      "41376,3.3347\n",
      "41377,3.3248\n",
      "41378,3.3374\n",
      "41379,3.3141\n",
      "41380,3.3099\n",
      "41381,3.3409\n",
      "41382,3.3496\n",
      "41383,3.3475\n",
      "41384,3.3158\n",
      "41385,3.3422\n",
      "41386,3.3403\n",
      "41387,3.3197\n",
      "41388,3.3342\n",
      "41389,3.3403\n",
      "41390,3.3163\n",
      "41391,3.3361\n",
      "41392,3.3294\n",
      "41393,3.3062\n",
      "41394,3.3097\n",
      "41395,3.3522\n",
      "41396,3.3203\n",
      "41397,3.3221\n",
      "41398,3.3176\n",
      "41399,3.3418\n",
      "41400,3.3111\n",
      "(step: 41400, epoch: 0, block: 42393600), Train Loss: 3.2784, Val Loss: 3.2872\n",
      "Learning rate: 3.15360327e-05\n",
      "Scheduler step: 41400\n",
      "Checkpoint (step: 41400, epoch: 0, block: 42393600) Saved\n",
      "Step Documented\n",
      "00:27:39\n",
      "41401,3.3290\n",
      "41402,3.3346\n",
      "41403,3.3322\n",
      "41404,3.3145\n",
      "41405,3.3359\n",
      "41406,3.3257\n",
      "41407,3.3053\n",
      "41408,3.3786\n",
      "41409,3.3073\n",
      "41410,3.3553\n",
      "41411,3.3326\n",
      "41412,3.3072\n",
      "41413,3.3081\n",
      "41414,3.3169\n",
      "41415,3.3319\n",
      "41416,3.3145\n",
      "41417,3.3092\n",
      "41418,3.3662\n",
      "41419,3.3290\n",
      "41420,3.2899\n",
      "41421,3.3495\n",
      "41422,3.3302\n",
      "41423,3.3319\n",
      "41424,3.3149\n",
      "41425,3.3322\n",
      "41426,3.3199\n",
      "41427,3.3095\n",
      "41428,3.3238\n",
      "41429,3.3011\n",
      "41430,3.3032\n",
      "41431,3.3575\n",
      "41432,3.3395\n",
      "41433,3.3582\n",
      "41434,3.3480\n",
      "41435,3.3303\n",
      "41436,3.3436\n",
      "41437,3.3454\n",
      "41438,3.3238\n",
      "41439,3.3228\n",
      "41440,3.3289\n",
      "41441,3.3572\n",
      "41442,3.3435\n",
      "41443,3.3488\n",
      "41444,3.3368\n",
      "41445,3.3090\n",
      "41446,3.3342\n",
      "41447,3.3196\n",
      "41448,3.3284\n",
      "41449,3.3247\n",
      "41450,3.3414\n",
      "41451,3.3377\n",
      "41452,3.3549\n",
      "41453,3.3507\n",
      "41454,3.3534\n",
      "41455,3.3408\n",
      "41456,3.3127\n",
      "41457,3.3530\n",
      "41458,3.3139\n",
      "41459,3.3402\n",
      "41460,3.3499\n",
      "41461,3.3423\n",
      "41462,3.3249\n",
      "41463,3.3370\n",
      "41464,3.3293\n",
      "41465,3.2993\n",
      "41466,3.3347\n",
      "41467,3.3139\n",
      "41468,3.3228\n",
      "41469,3.3450\n",
      "41470,3.3305\n",
      "41471,3.3151\n",
      "41472,3.3325\n",
      "41473,3.3439\n",
      "41474,3.3427\n",
      "41475,3.3247\n",
      "41476,3.3154\n",
      "41477,3.3248\n",
      "41478,3.3438\n",
      "41479,3.3232\n",
      "41480,3.3183\n",
      "41481,3.3458\n",
      "41482,3.3123\n",
      "41483,3.3624\n",
      "41484,3.3455\n",
      "41485,3.3435\n",
      "41486,3.3484\n",
      "41487,3.3424\n",
      "41488,3.3132\n",
      "41489,3.3354\n",
      "41490,3.3224\n",
      "41491,3.3197\n",
      "41492,3.3421\n",
      "41493,3.3753\n",
      "41494,3.3556\n",
      "41495,3.3331\n",
      "41496,3.3280\n",
      "41497,3.3368\n",
      "41498,3.3493\n",
      "41499,3.3371\n",
      "41500,3.3349\n",
      "(step: 41500, epoch: 0, block: 42496000), Train Loss: 3.2586, Val Loss: 3.2894\n",
      "Learning rate: 3.13345423e-05\n",
      "Scheduler step: 41500\n",
      "Checkpoint (step: 41500, epoch: 0, block: 42496000) Saved\n",
      "Step Documented\n",
      "00:53:57\n",
      "41501,3.3613\n",
      "41502,3.3297\n",
      "41503,3.3283\n",
      "41504,3.3353\n",
      "41505,3.3260\n",
      "41506,3.3471\n",
      "41507,3.3362\n",
      "41508,3.3626\n",
      "41509,3.3253\n",
      "41510,3.3515\n",
      "41511,3.3468\n",
      "41512,3.3657\n",
      "41513,3.3620\n",
      "41514,3.3355\n",
      "41515,3.3344\n",
      "41516,3.3297\n",
      "41517,3.3395\n",
      "41518,3.3445\n",
      "41519,3.3383\n",
      "41520,3.3026\n",
      "41521,3.3370\n",
      "41522,3.3245\n",
      "41523,3.3090\n",
      "41524,3.3160\n",
      "41525,3.3244\n",
      "41526,3.3384\n",
      "41527,3.3429\n",
      "41528,3.3461\n",
      "41529,3.3383\n",
      "41530,3.3066\n",
      "41531,3.3320\n",
      "41532,3.3335\n",
      "41533,3.3346\n",
      "41534,3.3265\n",
      "41535,3.3219\n",
      "41536,3.3555\n",
      "41537,3.3434\n",
      "41538,3.3373\n",
      "41539,3.3258\n",
      "41540,3.3421\n",
      "41541,3.3336\n",
      "41542,3.3569\n",
      "41543,3.3429\n",
      "41544,3.3260\n",
      "41545,3.3621\n",
      "41546,3.3180\n",
      "41547,3.3319\n",
      "41548,3.3393\n",
      "41549,3.3219\n",
      "41550,3.3183\n",
      "41551,3.3200\n",
      "41552,3.3190\n",
      "41553,3.3281\n",
      "41554,3.3434\n",
      "41555,3.3149\n",
      "41556,3.3392\n",
      "41557,3.3537\n",
      "41558,3.3522\n",
      "41559,3.3315\n",
      "41560,3.3515\n",
      "41561,3.3330\n",
      "41562,3.3395\n",
      "41563,3.3058\n",
      "41564,3.3491\n",
      "41565,3.3440\n",
      "41566,3.3225\n",
      "41567,3.3515\n",
      "41568,3.3367\n",
      "41569,3.3324\n",
      "41570,3.3357\n",
      "41571,3.3084\n",
      "41572,3.3517\n",
      "41573,3.3327\n",
      "41574,3.3338\n",
      "41575,3.3311\n",
      "41576,3.3267\n",
      "41577,3.3233\n",
      "41578,3.3297\n",
      "41579,3.3557\n",
      "41580,3.3303\n",
      "41581,3.3598\n",
      "41582,3.3520\n",
      "41583,3.3218\n",
      "41584,3.3399\n",
      "41585,3.3138\n",
      "41586,3.3309\n",
      "41587,3.3441\n",
      "41588,3.3494\n",
      "41589,3.3300\n",
      "41590,3.3441\n",
      "41591,3.3135\n",
      "41592,3.3009\n",
      "41593,3.3322\n",
      "41594,3.3282\n",
      "41595,3.3742\n",
      "41596,3.3334\n",
      "41597,3.3503\n",
      "41598,3.3252\n",
      "41599,3.3244\n",
      "41600,3.3493\n",
      "(step: 41600, epoch: 0, block: 42598400), Train Loss: 3.2661, Val Loss: 3.2436\n",
      "Learning rate: 3.11336245e-05\n",
      "Scheduler step: 41600\n",
      "Checkpoint (step: 41600, epoch: 0, block: 42598400) Saved\n",
      "Step Documented\n",
      "01:20:14\n",
      "41601,3.3242\n",
      "41602,3.2935\n",
      "41603,3.3284\n",
      "41604,3.2968\n",
      "41605,3.3348\n",
      "41606,3.3105\n",
      "41607,3.3287\n",
      "41608,3.3661\n",
      "41609,3.3164\n",
      "41610,3.3255\n",
      "41611,3.3274\n",
      "41612,3.3223\n",
      "41613,3.3201\n",
      "41614,3.3481\n",
      "41615,3.2970\n",
      "41616,3.3358\n",
      "41617,3.3872\n",
      "41618,3.3192\n",
      "41619,3.3474\n",
      "41620,3.3609\n",
      "41621,3.3167\n",
      "41622,3.3244\n",
      "41623,3.3291\n",
      "41624,3.3186\n",
      "41625,3.3215\n",
      "41626,3.3303\n",
      "41627,3.3517\n",
      "41628,3.3014\n",
      "41629,3.3200\n",
      "41630,3.3593\n",
      "41631,3.3306\n",
      "41632,3.3180\n",
      "41633,3.3501\n",
      "41634,3.3314\n",
      "41635,3.3148\n",
      "41636,3.3207\n",
      "41637,3.3200\n",
      "41638,3.3193\n",
      "41639,3.3399\n",
      "41640,3.3491\n",
      "41641,3.3106\n",
      "41642,3.3191\n",
      "41643,3.3347\n",
      "41644,3.3317\n",
      "41645,3.3592\n",
      "41646,3.3112\n",
      "41647,3.3052\n",
      "41648,3.3345\n",
      "41649,3.3774\n",
      "41650,3.3373\n",
      "41651,3.3303\n",
      "41652,3.3588\n",
      "41653,3.3417\n",
      "41654,3.3194\n",
      "41655,3.3178\n",
      "41656,3.3213\n",
      "41657,3.3286\n",
      "41658,3.3194\n",
      "41659,3.3235\n",
      "41660,3.3209\n",
      "41661,3.3488\n",
      "41662,3.3334\n",
      "41663,3.3256\n",
      "41664,3.3419\n",
      "41665,3.3271\n",
      "41666,3.3199\n",
      "41667,3.3511\n",
      "41668,3.3511\n",
      "41669,3.3257\n",
      "41670,3.2896\n",
      "41671,3.3076\n",
      "41672,3.3119\n",
      "41673,3.3086\n",
      "41674,3.3513\n",
      "41675,3.3182\n",
      "41676,3.3381\n",
      "41677,3.3264\n",
      "41678,3.3223\n",
      "41679,3.3008\n",
      "41680,3.3104\n",
      "41681,3.3567\n",
      "41682,3.3026\n",
      "41683,3.3386\n",
      "41684,3.3398\n",
      "41685,3.3417\n",
      "41686,3.3442\n",
      "41687,3.3488\n",
      "41688,3.3499\n",
      "41689,3.3201\n",
      "41690,3.3010\n",
      "41691,3.3140\n",
      "41692,3.3263\n",
      "41693,3.3436\n",
      "41694,3.3253\n",
      "41695,3.3437\n",
      "41696,3.3692\n",
      "41697,3.3530\n",
      "41698,3.3581\n",
      "41699,3.3050\n",
      "41700,3.3432\n",
      "(step: 41700, epoch: 0, block: 42700800), Train Loss: 3.2103, Val Loss: 3.3014\n",
      "Learning rate: 3.09332845e-05\n",
      "Scheduler step: 41700\n",
      "Checkpoint (step: 41700, epoch: 0, block: 42700800) Saved\n",
      "Step Documented\n",
      "01:46:32\n",
      "41701,3.3221\n",
      "41702,3.3090\n",
      "41703,3.3340\n",
      "41704,3.3614\n",
      "41705,3.3177\n",
      "41706,3.3326\n",
      "41707,3.3435\n",
      "41708,3.3285\n",
      "41709,3.3271\n",
      "41710,3.3478\n",
      "41711,3.3477\n",
      "41712,3.3588\n",
      "41713,3.3091\n",
      "41714,3.2940\n",
      "41715,3.3267\n",
      "41716,3.3274\n",
      "41717,3.3341\n",
      "41718,3.3457\n",
      "41719,3.3189\n",
      "41720,3.3439\n",
      "41721,3.3263\n",
      "41722,3.3103\n",
      "41723,3.3466\n",
      "41724,3.3244\n",
      "41725,3.3487\n",
      "41726,3.3387\n",
      "41727,3.3334\n",
      "41728,3.3337\n",
      "41729,3.3291\n",
      "41730,3.3266\n",
      "41731,3.3430\n",
      "41732,3.2968\n",
      "41733,3.2992\n",
      "41734,3.3077\n",
      "41735,3.3328\n",
      "41736,3.3255\n",
      "41737,3.3389\n",
      "41738,3.3249\n",
      "41739,3.3499\n",
      "41740,3.3634\n",
      "41741,3.3267\n",
      "41742,3.3195\n",
      "41743,3.3540\n",
      "41744,3.3598\n",
      "41745,3.3214\n",
      "41746,3.3266\n",
      "41747,3.3400\n",
      "41748,3.3321\n",
      "41749,3.3073\n",
      "41750,3.3482\n",
      "41751,3.3249\n",
      "41752,3.3439\n",
      "41753,3.3468\n",
      "41754,3.3432\n",
      "41755,3.3118\n",
      "41756,3.3270\n",
      "41757,3.3372\n",
      "41758,3.3124\n",
      "41759,3.3228\n",
      "41760,3.3427\n",
      "41761,3.3448\n",
      "41762,3.3397\n",
      "41763,3.3120\n",
      "41764,3.3268\n",
      "41765,3.3407\n",
      "41766,3.3157\n",
      "41767,3.3218\n",
      "41768,3.3413\n",
      "41769,3.3179\n",
      "41770,3.3397\n",
      "41771,3.3336\n",
      "41772,3.3350\n",
      "41773,3.3387\n",
      "41774,3.3224\n",
      "41775,3.3321\n",
      "41776,3.3379\n",
      "41777,3.3498\n",
      "41778,3.3578\n",
      "41779,3.3137\n",
      "41780,3.3463\n",
      "41781,3.3150\n",
      "41782,3.3412\n",
      "41783,3.3300\n",
      "41784,3.3483\n",
      "41785,3.3332\n",
      "41786,3.3451\n",
      "41787,3.3268\n",
      "41788,3.3124\n",
      "41789,3.2912\n",
      "41790,3.3292\n",
      "41791,3.3519\n",
      "41792,3.3359\n",
      "41793,3.3498\n",
      "41794,3.3220\n",
      "41795,3.3251\n",
      "41796,3.3326\n",
      "41797,3.3589\n",
      "41798,3.3577\n",
      "41799,3.3544\n",
      "41800,3.3211\n",
      "(step: 41800, epoch: 0, block: 42803200), Train Loss: 3.2799, Val Loss: 3.2571\n",
      "Learning rate: 3.07335273e-05\n",
      "Scheduler step: 41800\n",
      "Checkpoint (step: 41800, epoch: 0, block: 42803200) Saved\n",
      "Step Documented\n",
      "02:12:51\n",
      "41801,3.2967\n",
      "41802,3.3291\n",
      "41803,3.3334\n",
      "41804,3.3529\n",
      "41805,3.3485\n",
      "41806,3.3322\n",
      "41807,3.3263\n",
      "41808,3.3241\n",
      "41809,3.3343\n",
      "41810,3.3238\n",
      "41811,3.3342\n",
      "41812,3.3448\n",
      "41813,3.3069\n",
      "41814,3.3651\n",
      "41815,3.3845\n",
      "41816,3.3628\n",
      "41817,3.3407\n",
      "41818,3.3413\n",
      "41819,3.3208\n",
      "41820,3.3009\n",
      "41821,3.3300\n",
      "41822,3.3374\n",
      "41823,3.3315\n",
      "41824,3.3317\n",
      "41825,3.3397\n",
      "41826,3.3474\n",
      "41827,3.3323\n",
      "41828,3.2975\n",
      "41829,3.3299\n",
      "41830,3.3322\n",
      "41831,3.3277\n",
      "41832,3.3302\n",
      "41833,3.3030\n",
      "41834,3.3567\n",
      "41835,3.3497\n",
      "41836,3.3414\n",
      "41837,3.2985\n",
      "41838,3.3176\n",
      "41839,3.3609\n",
      "41840,3.3085\n",
      "41841,3.3237\n",
      "41842,3.3248\n",
      "41843,3.3481\n",
      "41844,3.3465\n",
      "41845,3.3361\n",
      "41846,3.3369\n",
      "41847,3.3349\n",
      "41848,3.3193\n",
      "41849,3.3246\n",
      "41850,3.3509\n",
      "41851,3.3302\n",
      "41852,3.3511\n",
      "41853,3.3298\n",
      "41854,3.3228\n",
      "41855,3.3445\n",
      "41856,3.3274\n",
      "41857,3.3321\n",
      "41858,3.3021\n",
      "41859,3.3191\n",
      "41860,3.3464\n",
      "41861,3.3250\n",
      "41862,3.3607\n",
      "41863,3.3345\n",
      "41864,3.3384\n",
      "41865,3.3479\n",
      "41866,3.3472\n",
      "41867,3.3293\n",
      "41868,3.3221\n",
      "41869,3.3281\n",
      "41870,3.3121\n",
      "41871,3.2884\n",
      "41872,3.3397\n",
      "41873,3.3282\n",
      "41874,3.3420\n",
      "41875,3.3434\n",
      "41876,3.3424\n",
      "41877,3.3273\n",
      "41878,3.2990\n",
      "41879,3.3488\n",
      "41880,3.3419\n",
      "41881,3.3524\n",
      "41882,3.3672\n",
      "41883,3.3334\n",
      "41884,3.3058\n",
      "41885,3.3199\n",
      "41886,3.3364\n",
      "41887,3.3450\n",
      "41888,3.3435\n",
      "41889,3.3401\n",
      "41890,3.3203\n",
      "41891,3.3480\n",
      "41892,3.3206\n",
      "41893,3.3297\n",
      "41894,3.3159\n",
      "41895,3.3475\n",
      "41896,3.3216\n",
      "41897,3.3382\n",
      "41898,3.3337\n",
      "41899,3.3392\n",
      "41900,3.3494\n",
      "(step: 41900, epoch: 0, block: 42905600), Train Loss: 3.2558, Val Loss: 3.2877\n",
      "Learning rate: 3.05343579e-05\n",
      "Scheduler step: 41900\n",
      "Checkpoint (step: 41900, epoch: 0, block: 42905600) Saved\n",
      "Step Documented\n",
      "02:39:08\n",
      "41901,3.3212\n",
      "41902,3.3427\n",
      "41903,3.3262\n",
      "41904,3.3325\n",
      "41905,3.3461\n",
      "41906,3.3505\n",
      "41907,3.3262\n",
      "41908,3.3340\n",
      "41909,3.3345\n",
      "41910,3.3387\n",
      "41911,3.3546\n",
      "41912,3.3240\n",
      "41913,3.3332\n",
      "41914,3.3229\n",
      "41915,3.3377\n",
      "41916,3.3392\n",
      "41917,3.3219\n",
      "41918,3.3542\n",
      "41919,3.3479\n",
      "41920,3.3507\n",
      "41921,3.3227\n",
      "41922,3.3246\n",
      "41923,3.3512\n",
      "41924,3.3415\n",
      "41925,3.3162\n",
      "41926,3.3269\n",
      "41927,3.3365\n",
      "41928,3.3433\n",
      "41929,3.3360\n",
      "41930,3.3259\n",
      "41931,3.3349\n",
      "41932,3.3322\n",
      "41933,3.3300\n",
      "41934,3.3225\n",
      "41935,3.3359\n",
      "41936,3.3199\n",
      "41937,3.3324\n",
      "41938,3.3606\n",
      "41939,3.3075\n",
      "41940,3.3373\n",
      "41941,3.3188\n",
      "41942,3.3291\n",
      "41943,3.3288\n",
      "41944,3.3318\n",
      "41945,3.3249\n",
      "41946,3.3239\n",
      "41947,3.3255\n",
      "41948,3.3318\n",
      "41949,3.3481\n",
      "41950,3.3129\n",
      "41951,3.3494\n",
      "41952,3.3350\n",
      "41953,3.3158\n",
      "41954,3.3333\n",
      "41955,3.3149\n",
      "41956,3.3037\n",
      "41957,3.3543\n",
      "41958,3.3479\n",
      "41959,3.3608\n",
      "41960,3.3198\n",
      "41961,3.3198\n",
      "41962,3.3278\n",
      "41963,3.3280\n",
      "41964,3.3286\n",
      "41965,3.3257\n",
      "41966,3.3366\n",
      "41967,3.3256\n",
      "41968,3.3106\n",
      "41969,3.3280\n",
      "41970,3.3119\n",
      "41971,3.3459\n",
      "41972,3.3181\n",
      "41973,3.3157\n",
      "41974,3.3056\n",
      "41975,3.3127\n",
      "41976,3.3525\n",
      "41977,3.3362\n",
      "41978,3.3150\n",
      "41979,3.3290\n",
      "41980,3.3423\n",
      "41981,3.3334\n",
      "41982,3.3531\n",
      "41983,3.3560\n",
      "41984,3.3389\n",
      "41985,3.3426\n",
      "41986,3.3489\n",
      "41987,3.3699\n",
      "41988,3.3262\n",
      "41989,3.3250\n",
      "41990,3.3301\n",
      "41991,3.3398\n",
      "41992,3.3215\n",
      "41993,3.3187\n",
      "41994,3.3574\n",
      "41995,3.3195\n",
      "41996,3.3164\n",
      "41997,3.3324\n",
      "41998,3.3604\n",
      "41999,3.3442\n",
      "42000,3.3330\n",
      "(step: 42000, epoch: 0, block: 43008000), Train Loss: 3.2335, Val Loss: 3.2758\n",
      "Learning rate: 3.03357814e-05\n",
      "Scheduler step: 42000\n",
      "Checkpoint (step: 42000, epoch: 0, block: 43008000) Saved\n",
      "Step Documented\n",
      "03:05:27\n",
      "42001,3.3350\n",
      "42002,3.3282\n",
      "42003,3.3382\n",
      "42004,3.3163\n",
      "42005,3.3497\n",
      "42006,3.3134\n",
      "42007,3.3438\n",
      "42008,3.3422\n",
      "42009,3.3315\n",
      "42010,3.3447\n",
      "42011,3.3555\n",
      "42012,3.3241\n",
      "42013,3.3517\n",
      "42014,3.3320\n",
      "42015,3.3357\n",
      "42016,3.3495\n",
      "42017,3.3462\n",
      "42018,3.3038\n",
      "42019,3.3458\n",
      "42020,3.3245\n",
      "42021,3.3469\n",
      "42022,3.3343\n",
      "42023,3.3556\n",
      "42024,3.3541\n",
      "42025,3.3363\n",
      "42026,3.3360\n",
      "42027,3.3245\n",
      "42028,3.3348\n",
      "42029,3.3335\n",
      "42030,3.3271\n",
      "42031,3.3346\n",
      "42032,3.3175\n",
      "42033,3.3309\n",
      "42034,3.3519\n",
      "42035,3.3029\n",
      "42036,3.3528\n",
      "42037,3.3220\n",
      "42038,3.3455\n",
      "42039,3.3232\n",
      "42040,3.2972\n",
      "42041,3.3368\n",
      "42042,3.3410\n",
      "42043,3.2996\n",
      "42044,3.3166\n",
      "42045,3.3208\n",
      "42046,3.3172\n",
      "42047,3.3344\n",
      "42048,3.3403\n",
      "42049,3.3382\n",
      "42050,3.3265\n",
      "42051,3.2948\n",
      "42052,3.3236\n",
      "42053,3.3269\n",
      "42054,3.3358\n",
      "42055,3.3321\n",
      "42056,3.3319\n",
      "42057,3.3403\n",
      "42058,3.3445\n",
      "42059,3.3110\n",
      "42060,3.3208\n",
      "42061,3.3586\n",
      "42062,3.3224\n",
      "42063,3.3261\n",
      "42064,3.3294\n",
      "42065,3.3275\n",
      "42066,3.3529\n",
      "42067,3.3139\n",
      "42068,3.3449\n",
      "42069,3.3205\n",
      "42070,3.3142\n",
      "42071,3.3090\n",
      "42072,3.3107\n",
      "42073,3.3308\n",
      "42074,3.3494\n",
      "42075,3.3455\n",
      "42076,3.3389\n",
      "42077,3.3137\n",
      "42078,3.3218\n",
      "42079,3.3044\n",
      "42080,3.3418\n",
      "42081,3.3426\n",
      "42082,3.3439\n",
      "42083,3.3083\n",
      "42084,3.3163\n",
      "42085,3.3607\n",
      "42086,3.3417\n",
      "42087,3.3320\n",
      "42088,3.3437\n",
      "42089,3.3355\n",
      "42090,3.3527\n",
      "42091,3.3197\n",
      "42092,3.3427\n",
      "42093,3.3269\n",
      "42094,3.3139\n",
      "42095,3.3590\n",
      "42096,3.3132\n",
      "42097,3.3123\n",
      "42098,3.3383\n",
      "42099,3.3262\n",
      "42100,3.2924\n",
      "(step: 42100, epoch: 0, block: 43110400), Train Loss: 3.2608, Val Loss: 3.2504\n",
      "Learning rate: 3.01378029e-05\n",
      "Scheduler step: 42100\n",
      "Checkpoint (step: 42100, epoch: 0, block: 43110400) Saved\n",
      "Step Documented\n",
      "03:31:45\n",
      "42101,3.3159\n",
      "42102,3.3323\n",
      "42103,3.3228\n",
      "42104,3.3352\n",
      "42105,3.3424\n",
      "42106,3.3475\n",
      "42107,3.3125\n",
      "42108,3.3267\n",
      "42109,3.3271\n",
      "42110,3.3382\n",
      "42111,3.3089\n",
      "42112,3.3105\n",
      "42113,3.3164\n",
      "42114,3.3199\n",
      "42115,3.3395\n",
      "42116,3.3142\n",
      "42117,3.3459\n",
      "42118,3.3277\n",
      "42119,3.3437\n",
      "42120,3.3141\n",
      "42121,3.3150\n",
      "42122,3.3435\n",
      "42123,3.3365\n",
      "42124,3.3283\n",
      "42125,3.3267\n",
      "42126,3.3055\n",
      "42127,3.3234\n",
      "42128,3.3200\n",
      "42129,3.3414\n",
      "42130,3.3282\n",
      "42131,3.3233\n",
      "42132,3.3192\n",
      "42133,3.3486\n",
      "42134,3.3156\n",
      "42135,3.3052\n",
      "42136,3.3091\n",
      "42137,3.3473\n",
      "42138,3.3087\n",
      "42139,3.3218\n",
      "42140,3.3497\n",
      "42141,3.3361\n",
      "42142,3.3466\n",
      "42143,3.2938\n",
      "42144,3.3404\n",
      "42145,3.3267\n",
      "42146,3.3215\n",
      "42147,3.3326\n",
      "42148,3.3270\n",
      "42149,3.3336\n",
      "42150,3.3414\n",
      "42151,3.3071\n",
      "42152,3.3277\n",
      "42153,3.3295\n",
      "42154,3.3204\n",
      "42155,3.3273\n",
      "42156,3.3329\n",
      "42157,3.3454\n",
      "42158,3.3350\n",
      "42159,3.3291\n",
      "42160,3.3260\n",
      "42161,3.3406\n",
      "42162,3.3500\n",
      "42163,3.3449\n",
      "42164,3.3251\n",
      "42165,3.3133\n",
      "42166,3.3278\n",
      "42167,3.3182\n",
      "42168,3.3612\n",
      "42169,3.3275\n",
      "42170,3.3223\n",
      "42171,3.3338\n",
      "42172,3.3576\n",
      "42173,3.3094\n",
      "42174,3.3422\n",
      "42175,3.3222\n",
      "42176,3.3367\n",
      "42177,3.3418\n",
      "42178,3.2930\n",
      "42179,3.3289\n",
      "42180,3.3663\n",
      "42181,3.3491\n",
      "42182,3.3190\n",
      "42183,3.3307\n",
      "42184,3.3384\n",
      "42185,3.3254\n",
      "42186,3.3534\n",
      "42187,3.3203\n",
      "42188,3.3348\n",
      "42189,3.3129\n",
      "42190,3.3041\n",
      "42191,3.3116\n",
      "42192,3.3277\n",
      "42193,3.3476\n",
      "42194,3.3427\n",
      "42195,3.3550\n",
      "42196,3.3406\n",
      "42197,3.3184\n",
      "42198,3.3341\n",
      "42199,3.3395\n",
      "42200,3.3390\n",
      "(step: 42200, epoch: 0, block: 43212800), Train Loss: 3.2406, Val Loss: 3.2777\n",
      "Learning rate: 2.99404272e-05\n",
      "Scheduler step: 42200\n",
      "Checkpoint (step: 42200, epoch: 0, block: 43212800) Saved\n",
      "Step Documented\n",
      "03:58:04\n",
      "42201,3.3451\n",
      "42202,3.3514\n",
      "42203,3.3276\n",
      "42204,3.3452\n",
      "42205,3.3517\n",
      "42206,3.3249\n",
      "42207,3.3391\n",
      "42208,3.3411\n",
      "42209,3.3218\n",
      "42210,3.3489\n",
      "42211,3.3313\n",
      "42212,3.3083\n",
      "42213,3.3233\n",
      "42214,3.2999\n",
      "42215,3.3077\n",
      "42216,3.3340\n",
      "42217,3.3452\n",
      "42218,3.3294\n",
      "42219,3.3339\n",
      "42220,3.3026\n",
      "42221,3.3245\n",
      "42222,3.3424\n",
      "42223,3.3292\n",
      "42224,3.3305\n",
      "42225,3.3350\n",
      "42226,3.3279\n",
      "42227,3.3422\n",
      "42228,3.3255\n",
      "42229,3.3348\n",
      "42230,3.3314\n",
      "42231,3.3397\n",
      "42232,3.3062\n",
      "42233,3.3303\n",
      "42234,3.3109\n",
      "42235,3.3020\n",
      "42236,3.3453\n",
      "42237,3.3291\n",
      "42238,3.3102\n",
      "42239,3.3355\n",
      "42240,3.3371\n",
      "42241,3.3219\n",
      "42242,3.2879\n",
      "42243,3.3237\n",
      "42244,3.3062\n",
      "42245,3.3082\n",
      "42246,3.3404\n",
      "42247,3.3269\n",
      "42248,3.3084\n",
      "42249,3.3106\n",
      "42250,3.3251\n",
      "42251,3.3068\n",
      "42252,3.3504\n",
      "42253,3.3356\n",
      "42254,3.3309\n",
      "42255,3.3522\n",
      "42256,3.3577\n",
      "42257,3.3367\n",
      "42258,3.3224\n",
      "42259,3.3036\n",
      "42260,3.3389\n",
      "42261,3.3537\n",
      "42262,3.3114\n",
      "42263,3.3388\n",
      "42264,3.3447\n",
      "42265,3.3160\n",
      "42266,3.3475\n",
      "42267,3.3125\n",
      "42268,3.3144\n",
      "42269,3.3471\n",
      "42270,3.3270\n",
      "42271,3.3153\n",
      "42272,3.3330\n",
      "42273,3.2960\n",
      "42274,3.3220\n",
      "42275,3.3153\n",
      "42276,3.3227\n",
      "42277,3.3204\n",
      "42278,3.3432\n",
      "42279,3.3264\n",
      "42280,3.3417\n",
      "42281,3.3226\n",
      "42282,3.3236\n",
      "42283,3.3416\n",
      "42284,3.3065\n",
      "42285,3.3213\n",
      "42286,3.3326\n",
      "42287,3.3045\n",
      "42288,3.3594\n",
      "42289,3.3358\n",
      "42290,3.3577\n",
      "42291,3.3158\n",
      "42292,3.3498\n",
      "42293,3.3162\n",
      "42294,3.3343\n",
      "42295,3.3217\n",
      "42296,3.3588\n",
      "42297,3.3038\n",
      "42298,3.3178\n",
      "42299,3.3519\n",
      "42300,3.3331\n",
      "(step: 42300, epoch: 0, block: 43315200), Train Loss: 3.2284, Val Loss: 3.3035\n",
      "Learning rate: 2.97436594e-05\n",
      "Scheduler step: 42300\n",
      "Checkpoint (step: 42300, epoch: 0, block: 43315200) Saved\n",
      "Step Documented\n",
      "04:24:22\n",
      "42301,3.3237\n",
      "42302,3.3208\n",
      "42303,3.3282\n",
      "42304,3.3092\n",
      "42305,3.3445\n",
      "42306,3.3228\n",
      "42307,3.3321\n",
      "42308,3.3361\n",
      "42309,3.3229\n",
      "42310,3.3550\n",
      "42311,3.3342\n",
      "42312,3.3647\n",
      "42313,3.3014\n",
      "42314,3.3369\n",
      "42315,3.3436\n",
      "42316,3.3591\n",
      "42317,3.3289\n",
      "42318,3.3295\n",
      "42319,3.3312\n",
      "42320,3.3267\n",
      "42321,3.3190\n",
      "42322,3.3182\n",
      "42323,3.3529\n",
      "42324,3.3591\n",
      "42325,3.3293\n",
      "42326,3.3261\n",
      "42327,3.3495\n",
      "42328,3.3138\n",
      "42329,3.3525\n",
      "42330,3.2889\n",
      "42331,3.3346\n",
      "42332,3.3381\n",
      "42333,3.3275\n",
      "42334,3.3299\n",
      "42335,3.3353\n",
      "42336,3.3092\n",
      "42337,3.3270\n",
      "42338,3.3285\n",
      "42339,3.3353\n",
      "42340,3.3321\n",
      "42341,3.3574\n",
      "42342,3.3517\n",
      "42343,3.3498\n",
      "42344,3.3078\n",
      "42345,3.3285\n",
      "42346,3.3372\n",
      "42347,3.3194\n",
      "42348,3.3221\n",
      "42349,3.3528\n",
      "42350,3.2984\n",
      "42351,3.3357\n",
      "42352,3.3313\n",
      "42353,3.3554\n",
      "42354,3.3415\n",
      "42355,3.3481\n",
      "42356,3.3510\n",
      "42357,3.3396\n",
      "42358,3.3150\n",
      "42359,3.3186\n",
      "42360,3.3689\n",
      "42361,3.3551\n",
      "42362,3.3252\n",
      "42363,3.3244\n",
      "42364,3.3097\n",
      "42365,3.3412\n",
      "42366,3.3475\n",
      "42367,3.3292\n",
      "42368,3.3003\n",
      "42369,3.3230\n",
      "42370,3.3326\n",
      "42371,3.3573\n",
      "42372,3.3425\n",
      "42373,3.3000\n",
      "42374,3.3394\n",
      "42375,3.3454\n",
      "42376,3.3424\n",
      "42377,3.3252\n",
      "42378,3.3653\n",
      "42379,3.3785\n",
      "42380,3.2992\n",
      "42381,3.3198\n",
      "42382,3.3197\n",
      "42383,3.3381\n",
      "42384,3.3530\n",
      "42385,3.3327\n",
      "42386,3.3133\n",
      "42387,3.3046\n",
      "42388,3.3141\n",
      "42389,3.3468\n",
      "42390,3.3263\n",
      "42391,3.3313\n",
      "42392,3.3430\n",
      "42393,3.3317\n",
      "42394,3.3389\n",
      "42395,3.3218\n",
      "42396,3.3104\n",
      "42397,3.3309\n",
      "42398,3.3351\n",
      "42399,3.3321\n",
      "42400,3.3297\n",
      "(step: 42400, epoch: 0, block: 43417600), Train Loss: 3.2720, Val Loss: 3.2804\n",
      "Learning rate: 2.95475045e-05\n",
      "Scheduler step: 42400\n",
      "Checkpoint (step: 42400, epoch: 0, block: 43417600) Saved\n",
      "Step Documented\n",
      "04:50:40\n",
      "42401,3.3523\n",
      "42402,3.3440\n",
      "42403,3.3596\n",
      "42404,3.3238\n",
      "42405,3.3206\n",
      "42406,3.3555\n",
      "42407,3.3422\n",
      "42408,3.3591\n",
      "42409,3.3107\n",
      "42410,3.3205\n",
      "42411,3.3442\n",
      "42412,3.3577\n",
      "42413,3.3103\n",
      "42414,3.3493\n",
      "42415,3.3377\n",
      "42416,3.3207\n",
      "42417,3.3122\n",
      "42418,3.3267\n",
      "42419,3.3046\n",
      "42420,3.3237\n",
      "42421,3.3332\n",
      "42422,3.3131\n",
      "42423,3.3316\n",
      "42424,3.3073\n",
      "42425,3.3266\n",
      "42426,3.3263\n",
      "42427,3.3595\n",
      "42428,3.3396\n",
      "42429,3.3519\n",
      "42430,3.3494\n",
      "42431,3.3180\n",
      "42432,3.3201\n",
      "42433,3.3166\n",
      "42434,3.3095\n",
      "42435,3.3255\n",
      "42436,3.3455\n",
      "42437,3.3411\n",
      "42438,3.3353\n",
      "42439,3.3439\n",
      "42440,3.3329\n",
      "42441,3.3194\n",
      "42442,3.3538\n",
      "42443,3.3488\n",
      "42444,3.3616\n",
      "42445,3.3413\n",
      "42446,3.3345\n",
      "42447,3.3322\n",
      "42448,3.3290\n",
      "42449,3.3161\n",
      "42450,3.3063\n",
      "42451,3.3254\n",
      "42452,3.3343\n",
      "42453,3.3254\n",
      "42454,3.3504\n",
      "42455,3.3270\n",
      "42456,3.3062\n",
      "42457,3.3276\n",
      "42458,3.3604\n",
      "42459,3.3345\n",
      "42460,3.3363\n",
      "42461,3.3290\n",
      "42462,3.3351\n",
      "42463,3.3028\n",
      "42464,3.3287\n",
      "42465,3.3212\n",
      "42466,3.3093\n",
      "42467,3.3372\n",
      "42468,3.3183\n",
      "42469,3.3195\n",
      "42470,3.3319\n",
      "42471,3.3109\n",
      "42472,3.3206\n",
      "42473,3.3434\n",
      "42474,3.3249\n",
      "42475,3.3165\n",
      "42476,3.3309\n",
      "42477,3.3156\n",
      "42478,3.3316\n",
      "42479,3.3084\n",
      "42480,3.3486\n",
      "42481,3.3288\n",
      "42482,3.3250\n",
      "42483,3.3140\n",
      "42484,3.3146\n",
      "42485,3.3622\n",
      "42486,3.3390\n",
      "42487,3.3360\n",
      "42488,3.3431\n",
      "42489,3.3244\n",
      "42490,3.3130\n",
      "42491,3.3426\n",
      "42492,3.3265\n",
      "42493,3.3171\n",
      "42494,3.3464\n",
      "42495,3.3153\n",
      "42496,3.3282\n",
      "42497,3.3373\n",
      "42498,3.3111\n",
      "42499,3.3360\n",
      "42500,3.3625\n",
      "(step: 42500, epoch: 0, block: 43520000), Train Loss: 3.2470, Val Loss: 3.2411\n",
      "Learning rate: 2.93519674e-05\n",
      "Scheduler step: 42500\n",
      "Checkpoint (step: 42500, epoch: 0, block: 43520000) Saved\n",
      "Step Documented\n",
      "05:16:59\n",
      "42501,3.3588\n",
      "42502,3.3356\n",
      "42503,3.3608\n",
      "42504,3.3389\n",
      "42505,3.3427\n",
      "42506,3.3371\n",
      "42507,3.3442\n",
      "42508,3.3301\n",
      "42509,3.3224\n",
      "42510,3.3172\n",
      "42511,3.3376\n",
      "42512,3.3320\n",
      "42513,3.3428\n",
      "42514,3.3504\n",
      "42515,3.3566\n",
      "42516,3.3465\n",
      "42517,3.3597\n",
      "42518,3.3289\n",
      "42519,3.3121\n",
      "42520,3.3377\n",
      "42521,3.3444\n",
      "42522,3.3576\n",
      "42523,3.3297\n",
      "42524,3.3603\n",
      "42525,3.3610\n",
      "42526,3.3174\n",
      "42527,3.3106\n",
      "42528,3.3099\n",
      "42529,3.3265\n",
      "42530,3.3094\n",
      "42531,3.3316\n",
      "42532,3.3246\n",
      "42533,3.3229\n",
      "42534,3.3142\n",
      "42535,3.3389\n",
      "42536,3.3468\n",
      "42537,3.3301\n",
      "42538,3.3325\n",
      "42539,3.3143\n",
      "42540,3.3107\n",
      "42541,3.3325\n",
      "42542,3.3204\n",
      "42543,3.3371\n",
      "42544,3.3311\n",
      "42545,3.3199\n",
      "42546,3.3170\n",
      "42547,3.3420\n",
      "42548,3.3001\n",
      "42549,3.3242\n",
      "42550,3.3413\n",
      "42551,3.3231\n",
      "42552,3.3427\n",
      "42553,3.3265\n",
      "42554,3.3307\n",
      "42555,3.3649\n",
      "42556,3.3505\n",
      "42557,3.3392\n",
      "42558,3.3156\n",
      "42559,3.3403\n",
      "42560,3.3136\n",
      "42561,3.3198\n",
      "42562,3.3493\n",
      "42563,3.3261\n",
      "42564,3.2952\n",
      "42565,3.3089\n",
      "42566,3.3289\n",
      "42567,3.3337\n",
      "42568,3.3227\n",
      "42569,3.3161\n",
      "42570,3.3431\n",
      "42571,3.3075\n",
      "42572,3.3226\n",
      "42573,3.3094\n",
      "42574,3.3573\n",
      "42575,3.3531\n",
      "42576,3.3319\n",
      "42577,3.3289\n",
      "42578,3.3338\n",
      "42579,3.3412\n",
      "42580,3.3343\n",
      "42581,3.3185\n",
      "42582,3.3146\n",
      "42583,3.3164\n",
      "42584,3.2938\n",
      "42585,3.3331\n",
      "42586,3.3095\n",
      "42587,3.3371\n",
      "42588,3.3109\n",
      "42589,3.3172\n",
      "42590,3.3206\n",
      "42591,3.3396\n",
      "42592,3.3774\n",
      "42593,3.3345\n",
      "42594,3.3313\n",
      "42595,3.3234\n",
      "42596,3.3365\n",
      "42597,3.3280\n",
      "42598,3.3454\n",
      "42599,3.3629\n",
      "42600,3.3405\n",
      "(step: 42600, epoch: 0, block: 43622400), Train Loss: 3.2336, Val Loss: 3.2787\n",
      "Learning rate: 2.91570531e-05\n",
      "Scheduler step: 42600\n",
      "Checkpoint (step: 42600, epoch: 0, block: 43622400) Saved\n",
      "Step Documented\n",
      "05:43:16\n",
      "42601,3.3535\n",
      "42602,3.3486\n",
      "42603,3.3417\n",
      "42604,3.3294\n",
      "42605,3.3350\n",
      "42606,3.3345\n",
      "42607,3.3549\n",
      "42608,3.3305\n",
      "42609,3.3320\n",
      "42610,3.3663\n",
      "42611,3.3683\n",
      "42612,3.3260\n",
      "42613,3.3056\n",
      "42614,3.3275\n",
      "42615,3.3417\n",
      "42616,3.3340\n",
      "42617,3.3306\n",
      "42618,3.3329\n",
      "42619,3.3383\n",
      "42620,3.3677\n",
      "42621,3.3180\n",
      "42622,3.3513\n",
      "42623,3.3329\n",
      "42624,3.3410\n",
      "42625,3.2951\n",
      "42626,3.3451\n",
      "42627,3.2886\n",
      "42628,3.3223\n",
      "42629,3.3302\n",
      "42630,3.3351\n",
      "42631,3.3199\n",
      "42632,3.3223\n",
      "42633,3.3287\n",
      "42634,3.3192\n",
      "42635,3.3214\n",
      "42636,3.3055\n",
      "42637,3.3186\n",
      "42638,3.3256\n",
      "42639,3.3199\n",
      "42640,3.3337\n",
      "42641,3.3399\n",
      "42642,3.3458\n",
      "42643,3.3018\n",
      "42644,3.3164\n",
      "42645,3.3335\n",
      "42646,3.3332\n",
      "42647,3.3625\n",
      "42648,3.3251\n",
      "42649,3.3383\n",
      "42650,3.3282\n",
      "42651,3.3447\n",
      "42652,3.3609\n",
      "42653,3.3239\n",
      "42654,3.3382\n",
      "42655,3.3354\n",
      "42656,3.3420\n",
      "42657,3.3190\n",
      "42658,3.3422\n",
      "42659,3.3036\n",
      "42660,3.3264\n",
      "42661,3.3490\n",
      "42662,3.3114\n",
      "42663,3.3095\n",
      "42664,3.2924\n",
      "42665,3.3215\n",
      "42666,3.3388\n",
      "42667,3.3180\n",
      "42668,3.3143\n",
      "42669,3.2858\n",
      "42670,3.3262\n",
      "42671,3.3377\n",
      "42672,3.3246\n",
      "42673,3.3513\n",
      "42674,3.3753\n",
      "42675,3.3701\n",
      "42676,3.3197\n",
      "42677,3.3382\n",
      "42678,3.3393\n",
      "42679,3.3254\n",
      "42680,3.3349\n",
      "42681,3.3416\n",
      "42682,3.3467\n",
      "42683,3.3037\n",
      "42684,3.3328\n",
      "42685,3.3415\n",
      "42686,3.3385\n",
      "42687,3.3493\n",
      "42688,3.3399\n",
      "42689,3.3254\n",
      "42690,3.3495\n",
      "42691,3.3466\n",
      "42692,3.3056\n",
      "42693,3.3384\n",
      "42694,3.3397\n",
      "42695,3.3164\n",
      "42696,3.3433\n",
      "42697,3.3190\n",
      "42698,3.3438\n",
      "42699,3.3243\n",
      "42700,3.3372\n",
      "(step: 42700, epoch: 0, block: 43724800), Train Loss: 3.2367, Val Loss: 3.2515\n",
      "Learning rate: 2.89627665e-05\n",
      "Scheduler step: 42700\n",
      "Checkpoint (step: 42700, epoch: 0, block: 43724800) Saved\n",
      "Step Documented\n",
      "06:09:36\n",
      "42701,3.3208\n",
      "42702,3.3389\n",
      "42703,3.3225\n",
      "42704,3.3270\n",
      "42705,3.2977\n",
      "42706,3.3474\n",
      "42707,3.3216\n",
      "42708,3.3239\n",
      "42709,3.3475\n",
      "42710,3.3373\n",
      "42711,3.3572\n",
      "42712,3.3443\n",
      "42713,3.3020\n",
      "42714,3.3175\n",
      "42715,3.3255\n",
      "42716,3.3494\n",
      "42717,3.3386\n",
      "42718,3.2881\n",
      "42719,3.3239\n",
      "42720,3.3580\n",
      "42721,3.3527\n",
      "42722,3.3262\n",
      "42723,3.3390\n",
      "42724,3.3447\n",
      "42725,3.3259\n",
      "42726,3.3411\n",
      "42727,3.3092\n",
      "42728,3.3117\n",
      "42729,3.3096\n",
      "42730,3.3251\n",
      "42731,3.3261\n",
      "42732,3.3444\n",
      "42733,3.3129\n",
      "42734,3.3521\n",
      "42735,3.3350\n",
      "42736,3.3092\n",
      "42737,3.3134\n",
      "42738,3.3333\n",
      "42739,3.3444\n",
      "42740,3.3369\n",
      "42741,3.3328\n",
      "42742,3.3467\n",
      "42743,3.3584\n",
      "42744,3.3458\n",
      "42745,3.3367\n",
      "42746,3.3264\n",
      "42747,3.3343\n",
      "42748,3.3517\n",
      "42749,3.3346\n",
      "42750,3.3544\n",
      "42751,3.3426\n",
      "42752,3.3477\n",
      "42753,3.3410\n",
      "42754,3.3191\n",
      "42755,3.3397\n",
      "42756,3.3392\n",
      "42757,3.3038\n",
      "42758,3.3075\n",
      "42759,3.3293\n",
      "42760,3.2932\n",
      "42761,3.3363\n",
      "42762,3.3244\n",
      "42763,3.3242\n",
      "42764,3.3358\n",
      "42765,3.3434\n",
      "42766,3.3101\n",
      "42767,3.3340\n",
      "42768,3.3307\n",
      "42769,3.3430\n",
      "42770,3.3217\n",
      "42771,3.3341\n",
      "42772,3.3349\n",
      "42773,3.3356\n",
      "42774,3.2842\n",
      "42775,3.3142\n",
      "42776,3.3344\n",
      "42777,3.3383\n",
      "42778,3.3070\n",
      "42779,3.3498\n",
      "42780,3.3168\n",
      "42781,3.3327\n",
      "42782,3.3255\n",
      "42783,3.3308\n",
      "42784,3.3468\n",
      "42785,3.3445\n",
      "42786,3.3520\n",
      "42787,3.3610\n",
      "42788,3.3266\n",
      "42789,3.3562\n",
      "42790,3.3154\n",
      "42791,3.3121\n",
      "42792,3.3599\n",
      "42793,3.3169\n",
      "42794,3.3282\n",
      "42795,3.3096\n",
      "42796,3.3201\n",
      "42797,3.3161\n",
      "42798,3.2935\n",
      "42799,3.3487\n",
      "42800,3.3372\n",
      "(step: 42800, epoch: 0, block: 43827200), Train Loss: 3.2380, Val Loss: 3.2800\n",
      "Learning rate: 2.87691124e-05\n",
      "Scheduler step: 42800\n",
      "Checkpoint (step: 42800, epoch: 0, block: 43827200) Saved\n",
      "Step Documented\n",
      "06:35:53\n",
      "42801,3.3039\n",
      "42802,3.3353\n",
      "42803,3.3310\n",
      "42804,3.3483\n",
      "42805,3.3312\n",
      "42806,3.3263\n",
      "42807,3.2935\n",
      "42808,3.3335\n",
      "42809,3.3220\n",
      "42810,3.3174\n",
      "42811,3.3119\n",
      "42812,3.3433\n",
      "42813,3.3572\n",
      "42814,3.3375\n",
      "42815,3.3289\n",
      "42816,3.3312\n",
      "42817,3.3330\n",
      "42818,3.3293\n",
      "42819,3.3247\n",
      "42820,3.3171\n",
      "42821,3.3260\n",
      "42822,3.3069\n",
      "42823,3.3699\n",
      "42824,3.3652\n",
      "42825,3.3287\n",
      "42826,3.3675\n",
      "42827,3.3279\n",
      "42828,3.3407\n",
      "42829,3.3412\n",
      "42830,3.3428\n",
      "42831,3.3363\n",
      "42832,3.3259\n",
      "42833,3.2973\n",
      "42834,3.2979\n",
      "42835,3.3340\n",
      "42836,3.3374\n",
      "42837,3.3420\n",
      "42838,3.3454\n",
      "42839,3.3052\n",
      "42840,3.3510\n",
      "42841,3.3401\n",
      "42842,3.3524\n",
      "42843,3.3201\n",
      "42844,3.3261\n",
      "42845,3.3078\n",
      "42846,3.3640\n",
      "42847,3.3354\n",
      "42848,3.3583\n",
      "42849,3.3134\n",
      "42850,3.3278\n",
      "42851,3.3109\n",
      "42852,3.3344\n",
      "42853,3.3567\n",
      "42854,3.3130\n",
      "42855,3.3319\n",
      "42856,3.3156\n",
      "42857,3.3662\n",
      "42858,3.3016\n",
      "42859,3.3452\n",
      "42860,3.3363\n",
      "42861,3.3421\n",
      "42862,3.3083\n",
      "42863,3.3659\n",
      "42864,3.3404\n",
      "42865,3.3329\n",
      "42866,3.3369\n",
      "42867,3.3241\n",
      "42868,3.3422\n",
      "42869,3.3087\n",
      "42870,3.3217\n",
      "42871,3.3709\n",
      "42872,3.3348\n",
      "42873,3.3240\n",
      "42874,3.3362\n",
      "42875,3.3105\n",
      "42876,3.2998\n",
      "42877,3.3196\n",
      "42878,3.3119\n",
      "42879,3.3295\n",
      "42880,3.3495\n",
      "42881,3.3299\n",
      "42882,3.3204\n",
      "42883,3.3238\n",
      "42884,3.3441\n",
      "42885,3.3457\n",
      "42886,3.3063\n",
      "42887,3.3382\n",
      "42888,3.3421\n",
      "42889,3.3273\n",
      "42890,3.3292\n",
      "42891,3.3274\n",
      "42892,3.3636\n",
      "42893,3.3597\n",
      "42894,3.3302\n",
      "42895,3.3347\n",
      "42896,3.3150\n",
      "42897,3.3159\n",
      "42898,3.2956\n",
      "42899,3.3539\n",
      "42900,3.3565\n",
      "(step: 42900, epoch: 0, block: 43929600), Train Loss: 3.2906, Val Loss: 3.2570\n",
      "Learning rate: 2.85760958e-05\n",
      "Scheduler step: 42900\n",
      "Checkpoint (step: 42900, epoch: 0, block: 43929600) Saved\n",
      "Step Documented\n",
      "07:02:12\n",
      "42901,3.3203\n",
      "42902,3.3421\n",
      "42903,3.3083\n",
      "42904,3.3266\n",
      "42905,3.3333\n",
      "42906,3.3091\n",
      "42907,3.3254\n",
      "42908,3.3393\n",
      "42909,3.3364\n",
      "42910,3.3216\n",
      "42911,3.3250\n",
      "42912,3.3187\n",
      "42913,3.3381\n",
      "42914,3.3346\n",
      "42915,3.2998\n",
      "42916,3.3420\n",
      "42917,3.3558\n",
      "42918,3.3306\n",
      "42919,3.3344\n",
      "42920,3.3064\n",
      "42921,3.3254\n",
      "42922,3.3406\n",
      "42923,3.3214\n",
      "42924,3.3298\n",
      "42925,3.3223\n",
      "42926,3.3344\n",
      "42927,3.3442\n",
      "42928,3.3335\n",
      "42929,3.3257\n",
      "42930,3.2904\n",
      "42931,3.3264\n",
      "42932,3.3279\n",
      "42933,3.2984\n",
      "42934,3.3274\n",
      "42935,3.3536\n",
      "42936,3.3390\n",
      "42937,3.3082\n",
      "42938,3.3240\n",
      "42939,3.3270\n",
      "42940,3.3299\n",
      "42941,3.3077\n",
      "42942,3.3258\n",
      "42943,3.3369\n",
      "42944,3.3385\n",
      "42945,3.3079\n",
      "42946,3.3258\n",
      "42947,3.3215\n",
      "42948,3.3333\n",
      "42949,3.3307\n",
      "42950,3.3583\n",
      "42951,3.3569\n",
      "42952,3.3632\n",
      "42953,3.3345\n",
      "42954,3.3336\n",
      "42955,3.3122\n",
      "42956,3.3180\n",
      "42957,3.3302\n",
      "42958,3.3391\n",
      "42959,3.2848\n",
      "42960,3.3487\n",
      "42961,3.3235\n",
      "42962,3.3559\n",
      "42963,3.3367\n",
      "42964,3.3590\n",
      "42965,3.3277\n",
      "42966,3.3143\n",
      "42967,3.3538\n",
      "42968,3.3316\n",
      "42969,3.3393\n",
      "42970,3.2996\n",
      "42971,3.3267\n",
      "42972,3.3343\n",
      "42973,3.3348\n",
      "42974,3.3487\n",
      "42975,3.3025\n",
      "42976,3.3356\n",
      "42977,3.3287\n",
      "42978,3.3199\n",
      "42979,3.3139\n",
      "42980,3.3425\n",
      "42981,3.3546\n",
      "42982,3.3364\n",
      "42983,3.3614\n",
      "42984,3.3435\n",
      "42985,3.3524\n",
      "42986,3.3046\n",
      "42987,3.3271\n",
      "42988,3.3187\n",
      "42989,3.3679\n",
      "42990,3.3116\n",
      "42991,3.3465\n",
      "42992,3.3161\n",
      "42993,3.3551\n",
      "42994,3.3250\n",
      "42995,3.3324\n",
      "42996,3.3216\n",
      "42997,3.3220\n",
      "42998,3.3025\n",
      "42999,3.3144\n",
      "43000,3.3490\n",
      "(step: 43000, epoch: 0, block: 44032000), Train Loss: 3.2797, Val Loss: 3.2679\n",
      "Learning rate: 2.83837216e-05\n",
      "Scheduler step: 43000\n",
      "Checkpoint (step: 43000, epoch: 0, block: 44032000) Saved\n",
      "Step Documented\n",
      "07:28:30\n",
      "43001,3.3383\n",
      "43002,3.3131\n",
      "43003,3.3122\n",
      "43004,3.3373\n",
      "43005,3.3386\n",
      "43006,3.3005\n",
      "43007,3.3604\n",
      "43008,3.3240\n",
      "43009,3.3448\n",
      "43010,3.3267\n",
      "43011,3.3611\n",
      "43012,3.2906\n",
      "43013,3.3249\n",
      "43014,3.3438\n",
      "43015,3.3169\n",
      "43016,3.3391\n",
      "43017,3.3119\n",
      "43018,3.3150\n",
      "43019,3.3230\n",
      "43020,3.3252\n",
      "43021,3.3426\n",
      "43022,3.3318\n",
      "43023,3.3210\n",
      "43024,3.3250\n",
      "43025,3.3309\n",
      "43026,3.3330\n",
      "43027,3.3411\n",
      "43028,3.3489\n",
      "43029,3.3423\n",
      "43030,3.3284\n",
      "43031,3.3286\n",
      "43032,3.3565\n",
      "43033,3.3086\n",
      "43034,3.3388\n",
      "43035,3.3465\n",
      "43036,3.3338\n",
      "43037,3.3284\n",
      "43038,3.3239\n",
      "43039,3.3318\n",
      "43040,3.3388\n",
      "43041,3.3558\n",
      "43042,3.3128\n",
      "43043,3.3346\n",
      "43044,3.3613\n",
      "43045,3.3485\n",
      "43046,3.3420\n",
      "43047,3.3341\n",
      "43048,3.3176\n",
      "43049,3.3546\n",
      "43050,3.3626\n",
      "43051,3.3436\n",
      "43052,3.3213\n",
      "43053,3.3548\n",
      "43054,3.3357\n",
      "43055,3.3129\n",
      "43056,3.3436\n",
      "43057,3.3284\n",
      "43058,3.3265\n",
      "43059,3.3094\n",
      "43060,3.3019\n",
      "43061,3.3247\n",
      "43062,3.3284\n",
      "43063,3.3353\n",
      "43064,3.3233\n",
      "43065,3.3126\n",
      "43066,3.3835\n",
      "43067,3.3158\n",
      "43068,3.3239\n",
      "43069,3.3364\n",
      "43070,3.3205\n",
      "43071,3.3272\n",
      "43072,3.3285\n",
      "43073,3.3439\n",
      "43074,3.3359\n",
      "43075,3.3219\n",
      "43076,3.3296\n",
      "43077,3.3325\n",
      "43078,3.3597\n",
      "43079,3.3262\n",
      "43080,3.3569\n",
      "43081,3.3267\n",
      "43082,3.3088\n",
      "43083,3.3173\n",
      "43084,3.3335\n",
      "43085,3.2983\n",
      "43086,3.3375\n",
      "43087,3.3449\n",
      "43088,3.3431\n",
      "43089,3.3531\n",
      "43090,3.3437\n",
      "43091,3.3138\n",
      "43092,3.3303\n",
      "43093,3.3351\n",
      "43094,3.3060\n",
      "43095,3.3416\n",
      "43096,3.3296\n",
      "43097,3.3313\n",
      "43098,3.3229\n",
      "43099,3.3063\n",
      "43100,3.3335\n",
      "(step: 43100, epoch: 0, block: 44134400), Train Loss: 3.2465, Val Loss: 3.2566\n",
      "Learning rate: 2.81919947e-05\n",
      "Scheduler step: 43100\n",
      "Checkpoint (step: 43100, epoch: 0, block: 44134400) Saved\n",
      "Step Documented\n",
      "07:54:48\n",
      "43101,3.3108\n",
      "43102,3.3133\n",
      "43103,3.3441\n",
      "43104,3.3191\n",
      "43105,3.3541\n",
      "43106,3.3507\n",
      "43107,3.3203\n",
      "43108,3.3051\n",
      "43109,3.3613\n",
      "43110,3.3258\n",
      "43111,3.3216\n",
      "43112,3.3546\n",
      "43113,3.3171\n",
      "43114,3.3385\n",
      "43115,3.3210\n",
      "43116,3.3281\n",
      "43117,3.3162\n",
      "43118,3.3453\n",
      "43119,3.3432\n",
      "43120,3.3339\n",
      "43121,3.3108\n",
      "43122,3.3208\n",
      "43123,3.3397\n",
      "43124,3.3541\n",
      "43125,3.3265\n",
      "43126,3.3516\n",
      "43127,3.3390\n",
      "43128,3.3410\n",
      "43129,3.3455\n",
      "43130,3.3211\n",
      "43131,3.3330\n",
      "43132,3.3398\n",
      "43133,3.3538\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m train_start_event\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 34\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[1;32m     36\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m m(x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "train_data, train_loader, train_sampler = get_dataloaders(\n",
    "    train_dataset_path, minibatch_size, block_size, train_map_paths[start_epoch], start_block\n",
    ")\n",
    "print(f\"{time.localtime().tm_hour:02}:{time.localtime().tm_min:02}:{time.localtime().tm_sec:02}\")\n",
    "losses = estimate_loss(optim_step)    # estimate a base loss before training session\n",
    "print(f\"(step: {optim_step}, epoch: {start_epoch}, block: {start_block}), Train Loss: {losses['train']:.4f}, Val Loss: {losses['val']:.4f}\")\n",
    "print(f\"{time.localtime().tm_hour:02}:{time.localtime().tm_min:02}:{time.localtime().tm_sec:02}\")\n",
    "\n",
    "if save:\n",
    "    # if never trained log the pre optim loss in a new csv\n",
    "    if optim_step == 0 or save_model_number != load_model_number:\n",
    "        print(f\"Initializing new data collection file at: {LOG_FILE}\")\n",
    "        with open(LOG_FILE, 'a') as f:\n",
    "            f.write(f\"step,epoch,block,train_loss,val_loss\\n{optim_step:05d},{start_epoch:03d},{start_block:05d},{losses['train']:.4f},{losses['val']:.4f}\\n\")\n",
    "\n",
    "# Dictionary to hold total accumulated time and count\n",
    "times_tracker = defaultdict(lambda: {'time': 0.0, 'count': 0})\n",
    "\n",
    "train_start_event = torch.cuda.Event(enable_timing=True)\n",
    "train_end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    if epoch != start_epoch:\n",
    "        start_block = 0\n",
    "        train_data, train_loader, train_sampler = get_dataloaders(\n",
    "            train_dataset_path, minibatch_size, block_size, train_map_paths[epoch], start_block\n",
    "        )\n",
    "        \n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    #running_loss = 0\n",
    "    train_start_event.record()\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, loss = m(x, y)\n",
    "            loss /= accumulation_steps\n",
    "        #running_loss += loss\n",
    "        loss.backward()\n",
    "\n",
    "        block += minibatch_size\n",
    "        \n",
    "        if (batch_idx+1) % accumulation_steps == 0:\n",
    "            \"\"\"with torch.no_grad():\n",
    "                all_grads = torch.cat([p.grad.view(-1) for p in m.parameters() if p.grad is not None])\n",
    "                \n",
    "                # Measure the 'Signal' (Average magnitude of the gradient)\n",
    "                grad_norm = torch.linalg.norm(all_grads).item()\n",
    "                \n",
    "                # Measure the 'Noise' (Standard deviation of gradient values)\n",
    "                grad_std = torch.std(all_grads).item()\n",
    "                \n",
    "                # Noise-to-Signal Ratio\n",
    "                ns_ratio = grad_std / (grad_norm + 1e-8)\n",
    "                \n",
    "                print(f\"Step {optim_step} | Grad Norm: {grad_norm:.4f} | Noise Ratio: {ns_ratio:.6f}\")\"\"\"\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim_step += 1\n",
    "            #print(f\"{optim_step},{running_loss:.4f}\")\n",
    "            #running_loss = 0\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            if (optim_step - start_optim_step) % save_iters == 0:\n",
    "                train_end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "                # elapsed_time returns milliseconds, so divide by 1000.0\n",
    "                elapsed_time_sec = train_start_event.elapsed_time(train_end_event) / 1000.0\n",
    "                times_tracker['train']['time'] += elapsed_time_sec\n",
    "                times_tracker['train']['count'] += save_iters\n",
    "                \n",
    "                tic = time.perf_counter()\n",
    "                losses = estimate_loss(optim_step)\n",
    "                torch.cuda.synchronize()\n",
    "                times_tracker['estimate']['time'] += (time.perf_counter() - tic)\n",
    "                times_tracker['estimate']['count'] += 1\n",
    "                \n",
    "                print(f\"(step: {optim_step}, epoch: {epoch}, block: {block}), Train Loss: {losses['train']:.4f}, Val Loss: {losses['val']:.4f}\")\n",
    "                print(f\"Learning rate: {optim.param_groups[0]['lr']:.8e}\")\n",
    "                print(f\"Scheduler step: {scheduler.last_epoch}\")\n",
    "                tic = time.perf_counter()\n",
    "                \n",
    "                if save:\n",
    "                    os.makedirs(save_path, exist_ok=True)\n",
    "                    save_checkpoint(optim_step, epoch, block, m, optim, scheduler, f\"{save_path}/model{save_model_number}_step{optim_step}.pt\")\n",
    "            \n",
    "                    # write a new line in our data csv\n",
    "                    with open(LOG_FILE, 'a') as f:\n",
    "                        f.write(f\"{optim_step:05d},{epoch:03d},{block:05d},{losses['train']:.4f},{losses['val']:.4f}\\n\")\n",
    "                    print(f\"Step Documented\")\n",
    "                    \n",
    "                times_tracker['save']['time'] += (time.perf_counter() - tic)\n",
    "                times_tracker['save']['count'] += 1\n",
    "                    \n",
    "        \n",
    "                print(f\"{time.localtime().tm_hour:02}:{time.localtime().tm_min:02}:{time.localtime().tm_sec:02}\")\n",
    "                train_start_event.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68b45da-28f8-4901-8acc-1aabdf9175da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time train: 66.443hr\n",
      "Total time estim: 0.435hr\n",
      "Total time Check: 0.278hr\n",
      "Average time per Optimizer step: 15.634sec\n",
      "Average time 100 Optimizr steps: 26.056min\n",
      "Average time per Estimate  Loss: 10.241sec\n",
      "Average time per Chckpoint Save: 6.548sec\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time train: {times_tracker['train']['time'] /60/60:.3f}hr\")\n",
    "print(f\"Total time estim: {times_tracker['estimate']['time'] /60/60:.3f}hr\")\n",
    "print(f\"Total time Check: {times_tracker['save']['time'] /60/60:.3f}hr\")\n",
    "print(f\"Average time per Optimizer step: {times_tracker['train']['time'] / times_tracker['train']['count']:.3f}sec\")\n",
    "print(f\"Average time {save_iters} Optimizr steps: {times_tracker['train']['time'] / (times_tracker['train']['count'] / save_iters)/60:.3f}min\")\n",
    "print(f\"Average time per Estimate  Loss: {times_tracker['estimate']['time'] / times_tracker['estimate']['count']:.3f}sec\")\n",
    "print(f\"Average time per Chckpoint Save: {times_tracker['save']['time'] / times_tracker['save']['count']:.3f}sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a17b4-33fa-4a74-b06a-6cb201b38337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
