{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c902b045-7f16-4da0-8502-b6d4b7496358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "import gc\n",
    "from gptmodel import GPTLanguageModel\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# everything below here NEEDS to be identical to load an extistng model\n",
    "n_embed = 1536\n",
    "n_head = 24\n",
    "n_layer = 32\n",
    "dropout = 0.2\n",
    "vocab_size = 30000\n",
    "\n",
    "model_id = \"01\"\n",
    "model_step = \"43100\"\n",
    "load_path = f\"checkpoints/model{model_id}/model{model_id}_step{model_step}.pt\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer/tokenizer-01.json\")\n",
    "eot_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baea7e61-abb1-4fc5-9700-51a8c3eec30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfuly from checkpoints/model01/model01_step43100.pt\n"
     ]
    }
   ],
   "source": [
    "m = GPTLanguageModel(vocab_size, n_embed, n_head, n_layer, dropout).to(device)\n",
    "checkpoint = torch.load(load_path, map_location='cpu', weights_only=False)\n",
    "m.load_state_dict(checkpoint['model'])\n",
    "print(f\"Model loaded successfuly from {load_path}\")\n",
    "del checkpoint\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e6347d-a28f-47dc-a5e0-40d46b15418e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style: Calculated\n",
      "ELI5: How does a computer actually work? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do? What does it do?\n",
      "\n",
      "Style: Normal\n",
      "ELI5: How does a computer actually work? And how do you make it work in practice? What is it that you do to make it work?\n",
      "\n",
      "NIKOLA ELIYA: I do what I do to make it work. I do what I do to make it work in practice. I try to make it work for everybody. And, when it doesn’t work for me, I try to make it work for everybody. I try to make it work for everyone.\n",
      "\n",
      "So, for example, I try to make the computer work for me, and I try to make it work for people. And, then I try to make it work for myself.\n",
      "\n",
      "So, one of the things that I try to do is, if I make a computer, I try to make it work for people who can’t make it work for themselves. And, so, if I can’t make it work for myself, I try to make it work for myself.\n",
      "\n",
      "And, in fact, you see that for me in this kind of work\n",
      "\n",
      "Style: Chaotic\n",
      "ELI5: How does a computer actually work? What is its purpose?\n",
      "\n",
      "ELIZA MAYBEL I. DYNAMES: And its applications are limited largely to a few functions. At most, the computing device I describe today will give an Internet signal from one server to the next, from its owner to all possible computers and, say for security. This will also allow you to use a very simple computing function. This function will have applications in a computer for things like, that kind of encryption, and so on. All that happens is, once you've seen a computer for instance, you start getting a sort of computational representation, with the input (like a telephone) in these two dimensional images. But because I describe this as the computer doing this computation, you actually have input and output being transformed in different ways in realtime and then it's output is turned directly into outputs on the CPU. There's just not yet. So that you want you need to actually control one chip or one machine. When\n",
      "\n"
     ]
    }
   ],
   "source": [
    "styles = [(0.4, 50, 0.5), (0.8, 50, 0.9), (1.5, 50, 1.0)]\n",
    "names = [\"Calculated\", \"Normal\", \"Chaotic\"]\n",
    "for i, (temp, topk, topp) in enumerate(styles):\n",
    "    ids = tokenizer.encode(\"ELI5: How does a computer actually work?\", add_special_tokens=False).ids\n",
    "    context = torch.tensor([ids], dtype=torch.long, device=device)\n",
    "    out = m.generate(context, max_new_tokens=200, temp=temp, top_k=topk, top_p=topp, eot_id=eot_id)\n",
    "\n",
    "    print(f\"Style: {names[i]}\")\n",
    "    print(tokenizer.decode(out[0].tolist(), skip_special_tokens=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fbbb4d-aacc-4f9b-88d0-a783d71864de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfuly from checkpoints/model01/model01_step22700.pt\n",
      "Step 22700:\n",
      "\n",
      "PROMPT: The cat sat on the mat. The cat sat on the\n",
      "The cat sat on the mat. The cat sat on the others.\n",
      "\n",
      "The cats danced a little around the pout, gushing something down.\n",
      "\n",
      "And then the cat started on the other side. The cats danced a little through the pout, but eventually the cat got to sleep.\n",
      "\n",
      "The cats danced a little beside each other and seemed to enjoy their laughter. The cat was not happy, just thrilled.\n",
      "\n",
      "The cats even had a jolly moments.\n",
      "\n",
      "The cats were really happy\n",
      "\n",
      "For a short while the cat and the cats danced over each other.\n",
      "\n",
      "The cats danced a little through the pout, gushing something down.\n",
      "\n",
      "The cats did not sleep that night, but that's all their problems ever were.\n",
      "\n",
      "For the next year and a half the cats lived in a big house in front of a kitchen where a beautiful Jelly seemed to let out a warm chilling message: \"yeah i ain't no.\" Jack crouched in a large wheelchair himself and he rose at the pout.\n",
      "\n",
      "As the cats ate it all night\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "steps = [\"22700\"]\n",
    "prompts = [\"The cat sat on the mat. The cat sat on the\"] #\"Once upon a time,\", \"The capital of Illinois is\", \n",
    "\n",
    "m = GPTLanguageModel(vocab_size, n_embed, n_head, n_layer, dropout).to(device)\n",
    "\n",
    "for step in steps:\n",
    "    path = f\"{load_path}{step}.pt\"\n",
    "    checkpoint = torch.load(path, map_location='cpu', weights_only=False)\n",
    "    m.load_state_dict(checkpoint['model'])\n",
    "    print(f\"Model loaded successfuly from {path}\")\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Step {step}:\")\n",
    "    print()\n",
    "\n",
    "    for prompt in prompts:\n",
    "        ids = tokenizer.encode(prompt, add_special_tokens=False).ids\n",
    "        context = torch.tensor([ids], dtype=torch.long, device=device)\n",
    "        out = m.generate(context, max_new_tokens=200, eot_id=eot_id)\n",
    "\n",
    "        print(f\"PROMPT: {prompt}\")\n",
    "        print(tokenizer.decode(out[0].tolist(), skip_special_tokens=False))\n",
    "        print()\n",
    "\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576b3e61-4fc5-42aa-95dc-f59370adc61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Illinois is actually a shift in direction for the increasingly powerful bank, which has been noticeably more aggressive in its issuance of new bonds, and has moved ever closer to its charter.\"\n",
      "\n",
      "Agriculture tends to preserve its independence\n",
      "\n",
      "Oxford economists said other banks looked to the United States for help. \"We tend to think of the United States as the 28th largest market,\" Dr. David Sutt of Bayard-Rockwell College in Connecticut said of the United States. \"They are effectively telling us all the important things about the economy of the country as a whole.\n",
      "\n",
      "\"The longer this policy goes on, the worse it will be for U.S. banks. If they have to resort to bank-broking, that's another potential problem.\"\n",
      "\n",
      "Earlier this month, the Federal Government admonished the banks that hold Chicago Liquor (CRE) stock to include Chicago Stakeholders Direct (LDS) on a notice of serious market instability stemming from the collapse of Chase Manhattan\n"
     ]
    }
   ],
   "source": [
    "# Once upon a time, | The capital of Illinois is | The cat sat on the mat. The cat sat on the\n",
    "prompt = tokenizer.encode(\"The capital of Illinois is\", add_special_tokens=False).ids\n",
    "context = torch.tensor([prompt], dtype=torch.long, device=device)\n",
    "print(tokenizer.decode(m.generate(context, max_new_tokens=200, eot_id=eot_id)[0].tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77ab98d-11b4-4f3c-986b-8cd7f15dd79d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mdecode([\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.decode([0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776b2b2a-8f44-4f82-9914-f164b39de199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be71729-cec3-4ba9-8131-59b1e02b1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    prompt = input(\"Prompt:\\n\")\n",
    "    context = torch.tensor([tokenizer.encode(prompt, add_special_tokens=False).ids], dtype=torch.long, device=device)\n",
    "    generated_chars = tokenizer.decode(m.generate(context, max_new_tokens=200)[0].tolist())\n",
    "    print(f\"Completed:\\n{generated_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a17b4-33fa-4a74-b06a-6cb201b38337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
