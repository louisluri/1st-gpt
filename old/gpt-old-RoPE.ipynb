{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c902b045-7f16-4da0-8502-b6d4b7496358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import arange, cat\n",
    "from torch.nn import functional as F\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "HF_DATASET = \"openwebtext\"\n",
    "model_number = \"02\"\n",
    "CHECKPOINT_PATH = f'checkpoints/model{model_number}.pt'\n",
    "LOG_FILE = f'train_data/model{model_number}_data.csv'\n",
    "tokenizer_file = 'tokenizer/tokenizer-01.json'\n",
    "RESUME_ITER = 0     # Default starting step\n",
    "save = False   # save model and data?\n",
    "\n",
    "# evrything here can be changed each training session to optimize learning\n",
    "minibatch_size = 16   # effective batch size is minibatch_size * accumulation_steps\n",
    "accumulation_steps = 64\n",
    "block_size = 128\n",
    "learning_rate = 5e-3\n",
    "eval_iters = 100\n",
    "estimate_loss_iters = 100\n",
    "\n",
    "# everything below here NEEDS to stay the same to load an extistng model\n",
    "n_embed = 2048\n",
    "n_head = 16\n",
    "n_layer = 32\n",
    "dropout = 0.2\n",
    "max_seq_len = 1024       # tril size\n",
    "base = float(10000.0)    # RoPE pos encoding param\n",
    "head_size = n_embed // n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99c5dcc-ac07-484f-84ae-2845738013b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432d380a5d2848928f5ab22bb05930f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d32c08a92bd47a783ed183ca4754ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93743c87e70b46bdb39940a7a0223677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOAD DATASET\n",
    "dataset = load_dataset(HF_DATASET)\n",
    "\n",
    "dataset = dataset['train']\n",
    "# Use a seed so your split is the same every time you run the code\n",
    "seed_value = 42\n",
    "\n",
    "# Split the dataset: 90% for training and 10% for validation\n",
    "dataset = dataset.train_test_split(\n",
    "    test_size=0.10,  # Use 10% of the data for the validation set\n",
    "    seed=seed_value\n",
    ")\n",
    "\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee8b625-b3af-4875-a8c4-1c87661e01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TOKENIZER\n",
    "tokenizer = Tokenizer.from_file(tokenizer_file)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b49785b-716c-4ec3-9fdc-0daa187789b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROPE\n",
    "\"\"\"Create the RoPE rotation amounts vector\"\"\"\n",
    "# --- a) Generate Inverse Frequencies (theta_i) ---\n",
    "# torch.arange(0, D, 2) gets [0, 2, 4, ..., (D - 2)]\n",
    "# We calculate 1 / (base^(2i/D))\n",
    "inv_freq = 1.0 / (base ** (arange(0, head_size, 2).float() / head_size))\n",
    "inv_freq = inv_freq.to(device)\n",
    "# inv_freq shape: [D/2]\n",
    "\n",
    "def get_RoPE(positions):\n",
    "    \"\"\"Create the RoPE pos embedding table\"\"\"        \n",
    "    # --- c) Calculate Rotation Factors (m * theta_i) ---\n",
    "    # Outer product: [L] x [D/2] -> [L, D/2]\n",
    "    # \"i,j->ij\" means every element of i is multiplied by every element of j\n",
    "    freqs = torch.outer(positions.float(), inv_freq)\n",
    "\n",
    "    top = torch.stack((freqs.cos(), -freqs.sin()), dim=-1)\n",
    "    bottom = torch.stack((freqs.sin(), freqs.cos()), dim=-1)\n",
    "    \n",
    "    RoPE = torch.stack((top, bottom), dim=-2)\n",
    "    return RoPE.float().unsqueeze(0).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a97305-f9a5-494f-811a-7e696e514e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE GET BATCH\n",
    "def get_random_chunk(split, batch_size):\n",
    "    \"\"\"Fetches random documents, tokenizes them, and concatenates\n",
    "    them into one large, flat list of tokens. also returns one large flat list of positions\"\"\"\n",
    "    \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    \n",
    "    encoded_texts = tokenizer.encode_batch([data[i]['text'] for i in ix.tolist()])\n",
    "    \n",
    "    # Concatenate all lists into one flat list\n",
    "    all_tokens = []\n",
    "    #all_pos = []\n",
    "    for encoding in encoded_texts:\n",
    "        all_tokens.extend(encoding.ids)\n",
    "        #all_pos.extend(range(len(encoding.ids)))\n",
    "        \n",
    "    # Convert to a tensor\n",
    "    return torch.tensor(all_tokens, dtype=torch.long)\n",
    "    \n",
    "def get_batch(split, batch_size):\n",
    "    data = get_random_chunk(split, batch_size)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    #p = torch.stack([pos[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c41ebf-ab1f-4360-a90a-378cd4477100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE ESTIMATE LOSS FUNCTION\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(estimate_loss_iters)\n",
    "        for k in range(estimate_loss_iters):\n",
    "            X, Y = get_batch(split, minibatch_size)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b6d43c-7f03-4f35-9ed7-b2f070fb26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITION\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of attention in paralel\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qkv_projection = nn.Linear(n_embed, n_embed*3, bias=False)\n",
    "        #self.register_buffer('tril', torch.tril(torch.ones(max_seq_len, max_seq_len)))\n",
    "        self.proj = nn.Linear(head_size * n_head, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_projection(x)\n",
    "        q, k, v = qkv.split(n_embed, dim=2)  # (B, T, C)\n",
    "        q = q.view(B, T, n_head, C // n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
    "        k = k.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "\n",
    "        RoPE = get_RoPE(torch.arange(T, device=x.device))\n",
    "\n",
    "        # Reshape Q and K for RoPE (Split the last dim into 128 * 2)\n",
    "        # New Shape: [B, T, d_half, 2] -> [B, T, 128, 2]\n",
    "        k_reshaped = k.view(B, n_head, T, -1, 2)\n",
    "        q_reshaped = q.view(B, n_head, T, -1, 2)\n",
    "\n",
    "        # Apply Rotation via Batched Matrix Multiplication\n",
    "        # [B, T, 128, 2] @ [B, T, 128, 2, 2] -> [B, T, 128, 2]\n",
    "        # NOTE: PyTorch broadcasts the RoPE_slice if it only has shape [T, d_half, 2, 2]\n",
    "        k_rotated = torch.einsum('bnthc, bnthcd -> bnthd', k_reshaped, RoPE)\n",
    "        q_rotated = torch.einsum('bnthc, bnthcd -> bnthd', q_reshaped, RoPE)\n",
    "\n",
    "        # Flatten back to the original head dimension\n",
    "        # New Shape: [B, T, d_k] -> [B, T, 256]\n",
    "        k = k_rotated.contiguous().view(B, n_head, T, -1)\n",
    "        q = q_rotated.contiguous().view(B, n_head, T, -1)\n",
    "        \n",
    "        # sdpa\n",
    "        #wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        #wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        #wei = F.softmax(wei, dim=-1)\n",
    "        #wei = self.dropout(wei)\n",
    "        #out = wei @ v\n",
    "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0.0, is_causal=True)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, n_embed)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed Forward Block\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed), \n",
    "            nn.GELU(), \n",
    "            nn.Linear(4 * n_embed, n_embed), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Decoder Block\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention()\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.RMSNorm(n_embed)\n",
    "        self.ln2 = nn.RMSNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.blocks = nn.ModuleList([Block() for _ in range(n_layer)])\n",
    "        self.ln_f = nn.RMSNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        \n",
    "        x = self.token_embedding_table(index) # (B,T,C)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get positions tensor\n",
    "                T = index.shape[-1]\n",
    "                pos = torch.unsqueeze(torch.arange(T, dtype=torch.long, device=index.device), dim=0)\n",
    "                # get the logits for each char in the index\n",
    "                logits, loss = self(index)\n",
    "                # only look at the logits for each last letter\n",
    "                logits = logits[:, -1, :]\n",
    "                # turn each batch's last-letter logits to a normalized probability\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # choose one outcome randomly based on the previously defines probabilities\n",
    "                index_next = torch.multinomial(probs, num_samples=1) #(B, 1)\n",
    "                #concatenate the letter choice for each batch onto the end of the existing char list\n",
    "                index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
    "            self.train()\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e8dafb2-dba2-4135-af1d-42dd4b94c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE/LOAD CHECKPOINT FUNCTIONS\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    \"\"\"loads a previous checkpoint from the checkpoint path specified above, \n",
    "    returns the most recent optimizer step that model and optimizer were saved on\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=torch.device('cuda'), weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['iteration']\n",
    "\n",
    "def save_checkpoint(step, model, optimizer, path):\n",
    "    \"\"\"saves the current model and optimizer state to the checkpoint path specified at the top\n",
    "    Prints to confirm completion\"\"\"\n",
    "    checkpoint = {\n",
    "        'iteration': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    # Save the dictionary to a file. overwrites old checkpoint\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint {step} Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fbbb4d-aacc-4f9b-88d0-a783d71864de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1,734,049,072\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE AND LOAD MODEL AND OPTIMIZER\n",
    "\n",
    "# define the model\n",
    "m = GPTLanguageModel(vocab_size).to(device)\n",
    "m = torch.compile(m)\n",
    "\n",
    "# define a PyTorch optimizer\n",
    "optim = bnb.optim.PagedAdamW8bit(m.parameters(), lr=learning_rate)\n",
    "\n",
    "if save:\n",
    "    # check for an existing checkpoint and load if necessary\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(f\"Loading checkpoint from {CHECKPOINT_PATH}\")\n",
    "        RESUME_ITER = load_checkpoint(m, optim, CHECKPOINT_PATH)\n",
    "        print(f\"Loaded succesfuly from step: {RESUME_ITER}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {CHECKPOINT_PATH}.\")\n",
    "        print(f\"New model will be training from step: {RESUME_ITER}\")\n",
    "\n",
    "print(f\"Total parameters: {sum(p.numel() for p in m.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04d910fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Many total steps do you want to train to?\n",
    "max_iters = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c8bfb3-c345-4806-84fb-792608455ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:08:25\n",
      "11:08:25\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(accumulation_steps):\n\u001b[0;32m---> 24\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[1;32m     26\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#p = torch.stack([pos[i:i+block_size] for i in ix])\u001b[39;00m\n\u001b[1;32m     26\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mblock_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[0;32m---> 27\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "print(f\"{time.localtime().tm_hour:02}:{time.localtime().tm_min:02}:{time.localtime().tm_sec:02}\")\n",
    "losses = estimate_loss()    # estimate a base loss before training session\n",
    "print(f\"Step: {RESUME_ITER:04d}, Train Loss: {losses['train']:.3f}, Val Loss: {losses['val']:.3f}\")\n",
    "print(f\"{time.localtime().tm_hour:02}:{time.localtime().tm_min:02}:{time.localtime().tm_sec:02}\")\n",
    "\n",
    "if save:\n",
    "    # if never trained log the pre optim loss in a new csv\n",
    "    if RESUME_ITER == 0:\n",
    "        print(f\"Initializing new data collection file at: {LOG_FILE}\")\n",
    "        with open(LOG_FILE, 'a') as f:\n",
    "            f.write(f\"step,train_loss,val_loss\\n{RESUME_ITER:05d},{losses['train']:.3f},{losses['val']:.3f}\\n\")\n",
    "\n",
    "# Dictionary to hold total accumulated time and count\n",
    "times_tracker = defaultdict(lambda: {'time': 0.0, 'count': 0})\n",
    "\n",
    "train_start_event = torch.cuda.Event(enable_timing=True)\n",
    "train_end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "train_start_event.record()\n",
    "for iter in range(RESUME_ITER + 1, max_iters + 1):\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    for step in range(accumulation_steps):\n",
    "        xb, yb = get_batch('train', minibatch_size)\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, loss = m(xb, yb)\n",
    "            loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if (iter - RESUME_ITER) % eval_iters == 0:\n",
    "        train_end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        # elapsed_time returns milliseconds, so divide by 1000.0\n",
    "        elapsed_time_sec = train_start_event.elapsed_time(train_end_event) / 1000.0\n",
    "        times_tracker['train']['time'] += elapsed_time_sec\n",
    "        times_tracker['train']['count'] += eval_iters\n",
    "        \n",
    "        tic = time.perf_counter()\n",
    "        losses = estimate_loss()\n",
    "        torch.cuda.synchronize()\n",
    "        times_tracker['estimate']['time'] += (time.perf_counter() - tic)\n",
    "        times_tracker['estimate']['count'] += 1\n",
    "        \n",
    "        print(f\"Step: {iter:04d}, Train Loss: {losses['train']:.3f}, Val Loss: {losses['val']:.3f}\")\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "        if save:\n",
    "            save_checkpoint(iter, m, optim, CHECKPOINT_PATH)\n",
    "    \n",
    "            # write a new line in our data csv\n",
    "            with open(LOG_FILE, 'a') as f:\n",
    "                f.write(f\"{iter:05d},{losses['train']:.3f},{losses['val']:.3f}\\n\")\n",
    "            print(f\"Step Documented\")\n",
    "            \n",
    "        times_tracker['save']['time'] += (time.perf_counter() - tic)\n",
    "        times_tracker['save']['count'] += 1\n",
    "            \n",
    "\n",
    "        print(f\"{time.localtime().tm_hour:02}:{time.localtime().tm_min:02}:{time.localtime().tm_sec:02}\")\n",
    "        train_start_event.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68b45da-28f8-4901-8acc-1aabdf9175da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time train: 0.083hr\n",
      "Total time estim: 0.000hr\n",
      "Total time Check: 0.000hr\n",
      "Average time per Optimizer step: 0.298sec\n",
      "Average time 100 Optimizr steps: 0.496min\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage time per Optimizer step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes_tracker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtimes_tracker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Optimizr steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes_tracker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(times_tracker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39meval_iters)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage time per Estimate  Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtimes_tracker\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mestimate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mtimes_tracker\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mestimate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage time per Chckpoint Save: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes_tracker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtimes_tracker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "print(f\"Total time train: {times_tracker['train']['time'] /60/60:.3f}hr\")\n",
    "print(f\"Total time estim: {times_tracker['estimate']['time'] /60/60:.3f}hr\")\n",
    "print(f\"Total time Check: {times_tracker['save']['time'] /60/60:.3f}hr\")\n",
    "print(f\"Average time per Optimizer step: {times_tracker['train']['time'] / times_tracker['train']['count']:.3f}sec\")\n",
    "print(f\"Average time {eval_iters} Optimizr steps: {times_tracker['train']['time'] / (times_tracker['train']['count'] / eval_iters)/60:.3f}min\")\n",
    "print(f\"Average time per Estimate  Loss: {times_tracker['estimate']['time'] / times_tracker['estimate']['count']:.3f}sec\")\n",
    "print(f\"Average time per Chckpoint Save: {times_tracker['save']['time'] / times_tracker['save']['count']:.3f}sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b3e61-4fc5-42aa-95dc-f59370adc61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#prompt = tokenizer.encode(input(\"Prompt:\\n\"), add_special_tokens=False)\n",
    "prompt = torch.randint(450, 451, (1,)).item()\n",
    "context = torch.tensor([[prompt]], dtype=torch.long, device=device)\n",
    "generated_chars = tokenizer.decode(m.generate(context, max_new_tokens=200)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e77ab98d-11b4-4f3c-986b-8cd7f15dd79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be71729-cec3-4ba9-8131-59b1e02b1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    prompt = input(\"Prompt:\\n\")\n",
    "    context = torch.tensor([tokenizer.encode(prompt, add_special_tokens=False).ids], dtype=torch.long, device=device)\n",
    "    generated_chars = tokenizer.decode(m.generate(context, max_new_tokens=200)[0].tolist())\n",
    "    print(f\"Completed:\\n{generated_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a17b4-33fa-4a74-b06a-6cb201b38337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
