{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c902b045-7f16-4da0-8502-b6d4b7496358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import arange, cat\n",
    "from torch.nn import functional as F\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "import os\n",
    "import unicodedata\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "HF_DATASET = \"openwebtext\"\n",
    "model_number = \"02\"\n",
    "CHECKPOINT_PATH = f'checkpoints/model{model_number}.pt'\n",
    "LOG_FILE = f'train_data/model{model_number}_data.csv'\n",
    "tokenizer_file = 'tokenizer/tokenizer-01.json'\n",
    "RESUME_ITER = 0     # Default starting step\n",
    "save = True         # save model and data?\n",
    "\n",
    "# evrything here can be changed each training session to optimize learning\n",
    "minibatch_size = 64   # effective batch size is minibatch_size * accumulation_steps\n",
    "accumulation_steps = 16\n",
    "block_size = 128\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 100\n",
    "estimate_loss_iters = 100\n",
    "\n",
    "# everything below here NEEDS to stay the same to load an extistng model\n",
    "n_embed = 512\n",
    "n_head = 8\n",
    "n_layer = 12\n",
    "dropout = 0.2\n",
    "max_seq_len = 1024       # tril size\n",
    "base = float(10000.0)    # RoPE pos encoding param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c99c5dcc-ac07-484f-84ae-2845738013b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48881ba7edf84272833d3aec990c25b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bbaeb2e5c4444ebb8a5d94a6154dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184dc13003594532b7540674df3159fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOAD DATASET\n",
    "dataset = load_dataset(HF_DATASET)\n",
    "\n",
    "dataset = dataset['train']\n",
    "# Use a seed so your split is the same every time you run the code\n",
    "seed_value = 42\n",
    "\n",
    "# Split the dataset: 90% for training and 10% for validation\n",
    "dataset = dataset.train_test_split(\n",
    "    test_size=0.10,  # Use 10% of the data for the validation set\n",
    "    seed=seed_value\n",
    ")\n",
    "\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ee8b625-b3af-4875-a8c4-1c87661e01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TOKENIZER\n",
    "tokenizer = Tokenizer.from_file(tokenizer_file)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b49785b-716c-4ec3-9fdc-0daa187789b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ROPE\n",
    "\"\"\"Create the RoPE rotation amounts vector\"\"\"\n",
    "# --- a) Generate Inverse Frequencies (theta_i) ---\n",
    "# torch.arange(0, D, 2) gets [0, 2, 4, ..., (D - 2)]\n",
    "# We calculate 1 / (base^(2i/D))\n",
    "inv_freq = 1.0 / (base ** (arange(0, n_embed, 2).float() / n_embed))\n",
    "inv_freq = inv_freq.to(device)\n",
    "# inv_freq shape: [D/2]\n",
    "\n",
    "def get_RoPE(positions):\n",
    "    \"\"\"Create the RoPE pos embedding table\"\"\"        \n",
    "    # --- c) Calculate Rotation Factors (m * theta_i) ---\n",
    "    # Outer product: [L] x [D/2] -> [L, D/2]\n",
    "    # \"i,j->ij\" means every element of i is multiplied by every element of j\n",
    "    freqs = torch.einsum(\"bi,j->bij\", positions.float(), inv_freq)\n",
    "\n",
    "    top = torch.stack((freqs.cos(), -freqs.sin()), dim=-1)\n",
    "    bottom = torch.stack((freqs.sin(), freqs.cos()), dim=-1)\n",
    "    \n",
    "    RoPE = torch.stack((top, bottom), dim=-2)\n",
    "    return RoPE.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55a97305-f9a5-494f-811a-7e696e514e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE GET BATCH\n",
    "def get_random_chunk(split, batch_size):\n",
    "    \"\"\"Fetches random documents, tokenizes them, and concatenates\n",
    "    them into one large, flat list of tokens. also returns one large flat list of positions\"\"\"\n",
    "    \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    \n",
    "    # Tokenize all texts. The post-processor will automatically\n",
    "    #    add your \"<|end_of_text|>\" token to each one.\n",
    "    encoded_texts = tokenizer.encode_batch([data[i]['text'] for i in ix.tolist()])\n",
    "    \n",
    "    # Concatenate all lists into one flat list\n",
    "    all_tokens = []\n",
    "    all_pos = []\n",
    "    for encoding in encoded_texts:\n",
    "        all_tokens.extend(encoding.ids)\n",
    "        all_pos.extend(range(len(encoding.ids)))\n",
    "        \n",
    "    # Convert to a tensor\n",
    "    return torch.tensor(all_tokens, dtype=torch.long), torch.tensor(all_pos, dtype=torch.long)\n",
    "    \n",
    "def get_batch(split, batch_size):\n",
    "    data, pos = get_random_chunk(split, batch_size)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    p = torch.stack([pos[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y, p = x.to(device), y.to(device), p.to(device)\n",
    "    return x, y, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76c41ebf-ab1f-4360-a90a-378cd4477100",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE ESTIMATE LOSS FUNCTION\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(estimate_loss_iters)\n",
    "        for k in range(estimate_loss_iters):\n",
    "            X, Y, P = get_batch(split, minibatch_size)\n",
    "            logits, loss = m(X, P, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0b6d43c-7f03-4f35-9ed7-b2f070fb26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITION\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of Scaled Dot Product Attention\"\"\"\n",
    "    def __init__(self, head_size, n_embed):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_seq_len, max_seq_len)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, RoPE_slice):\n",
    "        #input of size (batch, time-step, channels)\n",
    "        #output of size (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # Reshape Q and K for RoPE (Split the last dim into 128 * 2)\n",
    "        # New Shape: [B, T, d_half, 2] -> [B, T, 128, 2]\n",
    "        k_reshaped = k.view(B, T, -1, 2) \n",
    "        q_reshaped = q.view(B, T, -1, 2)\n",
    "\n",
    "        # Apply Rotation via Batched Matrix Multiplication\n",
    "        # [B, T, 128, 2] @ [B, T, 128, 2, 2] -> [B, T, 128, 2]\n",
    "        # NOTE: PyTorch broadcasts the RoPE_slice if it only has shape [T, d_half, 2, 2]\n",
    "        k_rotated = torch.einsum('bthc, bthcd -> bthd', k_reshaped, RoPE_slice)\n",
    "        q_rotated = torch.einsum('bthc, bthcd -> bthd', q_reshaped, RoPE_slice)\n",
    "\n",
    "        # Flatten back to the original head dimension\n",
    "        # New Shape: [B, T, d_k] -> [B, T, 256]\n",
    "        k = k_rotated.view(B, T, -1)\n",
    "        q = q_rotated.view(B, T, -1)\n",
    "\n",
    "        # compute the attention scores ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        #perform the weighted aggregation of the values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of attention in paralel\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embed) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, positions):\n",
    "        # Shape: [B, T, d_half, 2, 2]\n",
    "        RoPE_emb = get_RoPE(positions)\n",
    "\n",
    "        # Use torch.chunk to split the tensor into 8 pieces along dimension 2 (d_half)\n",
    "        # The result is a tuple of 8 smaller tensors\n",
    "        rope_head_chunks = torch.chunk(RoPE_emb, n_head, dim=-3)\n",
    "\n",
    "        # To get a list, convert the tuple:\n",
    "        rope_head_list = list(rope_head_chunks)\n",
    "        \n",
    "        out = torch.cat([h(x, rope_head_list[head_id]) for head_id, h in enumerate(self.heads)], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed Forward Block\"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed), \n",
    "            nn.GELU(), \n",
    "            nn.Linear(4 * n_embed, n_embed), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Decoder Block\"\"\"\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x_and_positions):\n",
    "        x, positions = x_and_positions\n",
    "        y = self.sa(x, positions)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x, positions\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, index, positions, targets=None):\n",
    "        B, T = index.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        x, pos = self.blocks((tok_emb, positions)) # (B,T,C)\n",
    "        \n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get positions tensor\n",
    "                T = index.shape[-1]\n",
    "                pos = torch.unsqueeze(torch.arange(T, dtype=torch.long, device=index.device), dim=0)\n",
    "                # get the logits for each char in the index\n",
    "                logits, loss = self(index, pos)\n",
    "                # only look at the logits for each last letter\n",
    "                logits = logits[:, -1, :]\n",
    "                # turn each batch's last-letter logits to a normalized probability\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # choose one outcome randomly based on the previously defines probabilities\n",
    "                index_next = torch.multinomial(probs, num_samples=1) #(B, 1)\n",
    "                #concatenate the letter choice for each batch onto the end of the existing char list\n",
    "                index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
    "            self.train()\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e8dafb2-dba2-4135-af1d-42dd4b94c119",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SAVE/LOAD CHECKPOINT FUNCTIONS\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    \"\"\"loads a previous checkpoint from the checkpoint path specified above, \n",
    "    returns the most recent optimizer step that model and optimizer were saved on\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=torch.device('cuda'), weights_only=False)\n",
    "    \n",
    "    # Load the learned knowledge\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load the learned history/momentum\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Return the iteration number to start the loop from\n",
    "    return checkpoint['iteration']\n",
    "\n",
    "def save_checkpoint(step, model, optimizer, path):\n",
    "    \"\"\"saves the current model and optimizer state to the checkpoint path specified at the top\n",
    "    Prints to confirm completion\"\"\"\n",
    "    checkpoint = {\n",
    "        'iteration': step,\n",
    "        # 1. Save the model's learned knowledge\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        # 2. Save the optimizer's learned history/momentum\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    # Save the dictionary to a single file. This overwrites the old checkpoint.\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint {step} Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23fbbb4d-aacc-4f9b-88d0-a783d71864de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from checkpoints/model02.pt\n",
      "Loaded succesfuly from step: 20100\n",
      "Total parameters: 68,561,200\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE AND LOAD MODEL AND OPTIMIZER\n",
    "\n",
    "# define the model\n",
    "m = GPTLanguageModel(vocab_size)\n",
    "m.to(device)\n",
    "\n",
    "# define a PyTorch optimizer\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "if save:\n",
    "    # check for an existing checkpoint and load if necessary\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(f\"Loading checkpoint from {CHECKPOINT_PATH}\")\n",
    "        RESUME_ITER = load_checkpoint(m, optim, CHECKPOINT_PATH)\n",
    "        print(f\"Loaded succesfuly from step: {RESUME_ITER}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {CHECKPOINT_PATH}.\")\n",
    "        print(f\"New model will be training from step: {RESUME_ITER}\")\n",
    "\n",
    "print(f\"Total parameters: {sum(p.numel() for p in m.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04d910fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Many total steps do you want to train to?\n",
    "max_iters = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c8bfb3-c345-4806-84fb-792608455ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:04:44\n",
      "10:04:44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m m(xb, pb, yb)\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28miter\u001b[39m \u001b[38;5;241m-\u001b[39m RESUME_ITER) \u001b[38;5;241m%\u001b[39m eval_iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda_env/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda_env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda_env/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "print(f\"{time.localtime().tm_hour:02}:{time.localtime().tm_min:02}:{time.localtime().tm_sec:02}\")\n",
    "#losses = estimate_loss()    # estimate a base loss before training session\n",
    "#print(f\"Step: {RESUME_ITER:04d}, Train Loss: {losses['train']:.3f}, Val Loss: {losses['val']:.3f}\")\n",
    "print(f\"{time.localtime().tm_hour:02}:{time.localtime().tm_min:02}:{time.localtime().tm_sec:02}\")\n",
    "\n",
    "if save:\n",
    "    # if never trained log the pre optim loss in a new csv\n",
    "    if RESUME_ITER == 0:\n",
    "        print(f\"Initializing new data collection file at: {LOG_FILE}\")\n",
    "        with open(LOG_FILE, 'a') as f:\n",
    "            f.write(f\"step,train_loss,val_loss\\n{RESUME_ITER:05d},{losses['train']:.3f},{losses['val']:.3f}\\n\")\n",
    "\n",
    "# Dictionary to hold total accumulated time and count\n",
    "times_tracker = defaultdict(lambda: {'time': 0.0, 'count': 0})\n",
    "\n",
    "train_start_event = torch.cuda.Event(enable_timing=True)\n",
    "train_end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "train_start_event.record()\n",
    "for iter in range(RESUME_ITER + 1, max_iters + 1):\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    for step in range(accumulation_steps):\n",
    "        xb, yb, pb = get_batch('train', minibatch_size)\n",
    "        logits, loss = m(xb, pb, yb)\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if (iter - RESUME_ITER) % eval_iters == 0:\n",
    "        train_end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        # elapsed_time returns milliseconds, so divide by 1000.0\n",
    "        elapsed_time_sec = train_start_event.elapsed_time(train_end_event) / 1000.0\n",
    "        times_tracker['train']['time'] += elapsed_time_sec\n",
    "        times_tracker['train']['count'] += eval_iters\n",
    "        \n",
    "        tic = time.perf_counter()\n",
    "        losses = estimate_loss()\n",
    "        torch.cuda.synchronize()\n",
    "        times_tracker['estimate']['time'] += (time.perf_counter() - tic)\n",
    "        times_tracker['estimate']['count'] += 1\n",
    "        \n",
    "        print(f\"Step: {iter:04d}, Train Loss: {losses['train']:.3f}, Val Loss: {losses['val']:.3f}\")\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "        if save:\n",
    "            save_checkpoint(iter, m, optim, CHECKPOINT_PATH)\n",
    "    \n",
    "            # write a new line in our data csv\n",
    "            with open(LOG_FILE, 'a') as f:\n",
    "                f.write(f\"{iter:05d},{losses['train']:.3f},{losses['val']:.3f}\\n\")\n",
    "            print(f\"Step Documented\")\n",
    "            \n",
    "        times_tracker['save']['time'] += (time.perf_counter() - tic)\n",
    "        times_tracker['save']['count'] += 1\n",
    "            \n",
    "\n",
    "        print(f\"{time.localtime().tm_hour:02}:{time.localtime().tm_min:02}:{time.localtime().tm_sec:02}\")\n",
    "        train_start_event.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68b45da-28f8-4901-8acc-1aabdf9175da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time train: 5.216hr\n",
      "Total time estim: 0.314hr\n",
      "Total time Check: 0.011hr\n",
      "Average time per Optimizer step: 5.216sec\n",
      "Average time 100 Optimizr steps: 8.693min\n",
      "Average time per Estimate  Loss: 31.373sec\n",
      "Average time per Chckpoint Save: 1.085sec\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time train: {times_tracker['train']['time'] /60/60:.3f}hr\")\n",
    "print(f\"Total time estim: {times_tracker['estimate']['time'] /60/60:.3f}hr\")\n",
    "print(f\"Total time Check: {times_tracker['save']['time'] /60/60:.3f}hr\")\n",
    "print(f\"Average time per Optimizer step: {times_tracker['train']['time'] / times_tracker['train']['count']:.3f}sec\")\n",
    "print(f\"Average time {eval_iters} Optimizr steps: {times_tracker['train']['time'] / (times_tracker['train']['count'] / eval_iters)/60:.3f}min\")\n",
    "print(f\"Average time per Estimate  Loss: {times_tracker['estimate']['time'] / times_tracker['estimate']['count']:.3f}sec\")\n",
    "print(f\"Average time per Chckpoint Save: {times_tracker['save']['time'] / times_tracker['save']['count']:.3f}sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "576b3e61-4fc5-42aa-95dc-f59370adc61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U.S. Senate and Senate rejected the chamber's plan and eliminated minimum wage runs, the Post adds. General Assembly President and Blackwell (R) pronounced it a landmark decision.\n",
      "\n",
      "\"I think [the)\" panel responses stated it was \"unconstitutional,\" the Post wrote during a hearing in federal court in January. Although the critics still didn't consider it a step up, the effort had taken a number of studies -- and narrowed down language regarding serious measures to regulate the Atlantic, \"temporary inspections of American banks\" -- as well as those at the U.S. Capitol in Washington where gun control measures are being sought.\n",
      "\n",
      "\"Don't take liberty very seriously,\" Oppenheim argued, putting Congress' \"comparable\" fiscal responsibility in his mouth.\n",
      "\n",
      "\"Andispensing force and child-supporting groups are getting very serious resistance to enumerated veterans' demands, especially which now urge the League of Conservation Voters to abandon its mandate for policymaking, \"the bill would effectively eliminate the\n"
     ]
    }
   ],
   "source": [
    "#prompt = tokenizer.encode(input(\"Prompt:\\n\"), add_special_tokens=False)\n",
    "prompt = torch.randint(450, 451, (1,)).item()\n",
    "context = torch.tensor([[prompt]], dtype=torch.long, device=device)\n",
    "generated_chars = tokenizer.decode(m.generate(context, max_new_tokens=200)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e77ab98d-11b4-4f3c-986b-8cd7f15dd79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be71729-cec3-4ba9-8131-59b1e02b1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    prompt = input(\"Prompt:\\n\")\n",
    "    context = torch.tensor([tokenizer.encode(prompt, add_special_tokens=False).ids], dtype=torch.long, device=device)\n",
    "    generated_chars = tokenizer.decode(m.generate(context, max_new_tokens=200)[0].tolist())\n",
    "    print(f\"Completed:\\n{generated_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a17b4-33fa-4a74-b06a-6cb201b38337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
